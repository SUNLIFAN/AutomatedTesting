Tensor atan2 ( const Tensor & self , const Tensor & other ) { Tensor result ; auto iter = TensorIterator::binary_float_op ( result , self , other ) ; atan2_stub ( iter . device_type ( ) , iter ) ; return iter . output ( ) ; } 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( const auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) 
std::shared_ptr<Source> findSourceInArchiveFromQualifier ( caffe2::serialize::PyTorchStreamReader & reader , const std::string & export_prefix , const std::string & qualifier ) 
loaded_sources_ . insert ( qualifier ) ; std::shared_ptr<Source> src = source_loader_ ( qualifier ) ; 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return address ; } 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in MPI process group" ) ; } 
TORCH_CHECK ( tensor . numel ( ) == 0 || data . data_ptr ( ) , "tolist() shouldn't be called on a tensor with unallocated storage" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } 
c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } 
size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_with_handles_ [ pc ] . instruction ; 
VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) { return impl ( ) -> image ( imageSizes ) ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; AT_CUDA_CHECK ( cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } 
shdrList_ = ( Elf64_Shdr * ) ( fileData + ehdr_ -> e_shoff ) ; auto strtabSecNo = ehdr_ -> e_shstrndx ; MULTIPY_CHECK ( strtabSecNo >= 0 && strtabSecNo < numSections_ , "e_shstrndx out of range" ) ; 
const auto operand = operands [ i ] ; const auto labels = op_labels [ i ] ; const auto ndims = operand . dim ( ) ; int64_t nlabels = static_cast<int64_t> ( labels . size ( ) ) ; bool has_ellipsis = false ; for ( const auto & label : labels ) { 
MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 || size < 0 ) { TORCH_CHECK ( false , "Failed to get the world_size / rank" ) ; } } } 
List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( src -> exception_ptr ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
static Tensor norm ( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype ) { if ( self . is_sparse ( ) ) { return at::native_norm ( self , p , dim , keepdim , opt_dtype ) ; } else { Tensor result ; return at::native::norm_out ( result , self , p , dim , keepdim , opt_dtype ) ; } } 
std::end ( nextParameterIValues ) ) ; } } for ( auto n : nodesToDestroy ) { n -> destroy ( ) ; } return parameterIValues ; } 
{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , "at::Tensor scalar type %s is not supported on java side" , c10::toString ( scalarType ) ) ; } const auto & tensorShape = tensor . sizes ( ) ; 
TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( "Unexpected number of bound info entries in cacheAccesses in the fuser." ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad || info . kind == kMutate ; bool hasWrites = info . kind == kStore || info . kind == kMutate ; 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
if ( str != "None" ) { default_string = parse_string_literal ( str ) ; } 
Tensor tensor ; if ( tensor_ . is_quantized ( ) ) { tensor = tensor_ . dequantize ( ) . to ( kCPU , kDouble ) . contiguous ( ) ; } else { tensor = tensor_ . to ( kCPU , kDouble ) . contiguous ( ) ; } 
static Tensor norm ( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype ) { if ( self . is_sparse ( ) ) { return at::native_norm ( self , p , dim , keepdim , opt_dtype ) ; } else { Tensor result ; return at::native::norm_out ( result , self , p , dim , keepdim , opt_dtype ) ; } } 
if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << "Got exception: " << errMsg ; TORCH_CHECK ( false , errMsg ) ; } } 
makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , "" ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForInterface(): unsupported gloo device" ) ; } return device ; } 
if ( comms ) { for ( const auto i : c10::irange ( ndevices ) ) { int dummy_var ; if ( C10_CUDA_ERROR_HANDLED ( cudaGetDevice ( & dummy_var ) ) != cudaSuccess ) { 
unsigned previous = y - 1 ; for ( size_t x = 1 ; x <= n ; ++ x ) { const auto old_row = row [ x ] ; row [ x ] = std::min ( previous + ( word1 [ y - 1 ] == word2 [ x - 1 ] 0 u : 1 u ) , std::min ( row [ x - 1 ] , row [ x ] ) + 1 ) ; 
while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + "_" + c10::to_string ( uid ++ ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; 
if ( response == detail::CheckResponseType::NOT_READY ) { return false ; } throw std::runtime_error ( "ready or not_ready response expected" ) ; } void TCPStore::wait ( const std::vector<std::string> & keys ) { 
int64_t dim ) { auto self_ = moveBatchDimToFront ( self , self_bdim ) ; auto rank = rankWithoutBatchDim ( self , self_bdim ) ; dim = maybe_wrap_dim ( dim , rank + 1 ) ; if ( self_bdim ) { dim += 1 ; } return { self_ . unsqueeze ( dim ) , valIfNonempty ( self_bdim , 0 ) } ; } TORCH_LIBRARY_IMPL ( aten , FT_BATCHED_KEY , m ) { 
if ( future_ ) { future_ -> setError ( std::make_exception_ptr ( Future::FutureError ( ss . str ( ) ) ) ) ; } else if ( is_jit_exception ) { throw JITException ( ss . str ( ) ) ; } 
unsigned previous = y - 1 ; for ( size_t x = 1 ; x <= n ; ++ x ) { const auto old_row = row [ x ] ; row [ x ] = std::min ( previous + ( word1 [ y - 1 ] == word2 [ x - 1 ] 0 u : 1 u ) , std::min ( row [ x - 1 ] , row [ x ] ) + 1 ) ; 
Tensor weight_ ; if ( weight && weight -> defined ( ) ) { auto self_ = self ; auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; 
MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 || size < 0 ) { TORCH_CHECK ( false , "Failed to get the world_size / rank" ) ; } } } 
return { self . repeat ( sizes ) , nullopt } ; } VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return { self_ . repeat ( sizes_with_bdim ) , 0 } ; } 
std::map < ParallelType , bool > ParallelTypeBitmap::getMap ( ) const { std::map < ParallelType , bool > map ; for ( const auto & pt_offset : pt_to_offset_ ) { map . emplace ( pt_offset . first , bitset_ [ pt_offset . second ] ) ; } return map ; } 
if ( lhs . is_mkldnn ( ) || rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . is_nested ( ) || rhs . is_nested ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
void index_put__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
Tensor atan2 ( const Tensor & self , const Tensor & other ) { Tensor result ; auto iter = TensorIterator::binary_float_op ( result , self , other ) ; atan2_stub ( iter . device_type ( ) , iter ) ; return iter . output ( ) ; } 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
void apply_eig ( const Tensor & self , bool eigenvectors , Tensor & vals_ , Tensor & vecs_ , int * info_ptr ) { if T_BUILD_WITH_LAPACK ( ) TORCH_CHECK ( false , "Calling torch.eig on a CPU tensor requires compiling " , "PyTorch with LAPACK. Please use PyTorch built with LAPACK support." ) ; } 
if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } 
if ( min && max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { maximum_stub ( device_type ( ) , * this ) ; } else if ( max ) { minimum_stub ( device_type ( ) , * this ) ; } } 
if ( is_up_to_date ( ) ) { return ; } apply_updates ( ) ; regenerate_from_base ( ) ; } void FunctionalTensorWrapper::regenerate_from_base ( ) { 
if ( beta != scalar_t ( 1 ) && beta != scalar_t ( 0 ) ) scal<scalar_t> ( m , beta , y , incy ) ; for ( int64_t j = 0 ; j < n ; j ++ ) { scalar_t * column_ = a + lda * j ; scalar_t z = alpha * x [ j * incx ] ; for ( int64_t i = 0 ; i < m ; i ++ ) { if ( j == 0 && beta == scalar_t ( 0 ) ) { y [ i * incy ] = scalar_t ( 0 ) ; } y [ i * incy ] += z * column_ [ i ] ; } } 
static VmapDimVector range ( int64_t start , int64_t stop ) { TORCH_INTERNAL_ASSERT ( stop >= start ) ; VmapDimVector dims ; dims . reserve ( stop - start ) ; 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
void set_data ( Tensor & self , const Tensor & new_data ) { AT_ERROR ( "set_data is not implemented for Tensor" ) ; } 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; TORCH_INTERNAL_ASSERT_DEBUG_ONLY ( reduce_tensor . scalar_type ( ) == at::ScalarType::Half , "Expected reduced tensor to be fp16 in FP16CompressHook, but got type " , reduce_tensor . scalar_type ( ) ) ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; 
Tensor d = at::full_like ( sizes , D ) ; sizes = at::cat ( { sizes , d } , 1 ) . to ( kCPU ) ; return at::_nested_from_padded ( t , sizes , false ) ; } 
const auto operand = operands [ i ] ; const auto labels = op_labels [ i ] ; const auto ndims = operand . dim ( ) ; int64_t nlabels = static_cast<int64_t> ( labels . size ( ) ) ; bool has_ellipsis = false ; for ( const auto & label : labels ) { 
void VImage::addImageMemoryBarrierToGeneral ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_GENERAL ) ; } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; break ; } else if ( err != cudaSuccess ) { return err ; 
inline Variable valueToTensor ( c10::TensorOptions options , PyObject * value , const at::Device & device ) { if ( THPVariable_Check ( value ) ) { return THPVariable_Unpack ( value ) ; } 
HANDLE_TH_ERRORS TORCH_CHECK ( PyTuple_Size ( args ) == 2 , "_new_with_file takes exactly two arguments" ) ; int fd = PyObject_AsFileDescriptor ( PyTuple_GetItem ( args , 0 ) ) ; THPUtils_assert ( fd != -1 , "_new_with_file couldn't retrieve a file " "descriptor from given object" ) ; 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
static void printDebugValue ( Value * V ) { if ( auto F = dyn_cast<Function> ( V ) ) { std::cerr << "[FUNC] " << _demangle ( _name ( F ) ) ; } else if ( isa<Constant> ( V ) ) { std::cerr << "[CONST] " << * V ; } else if ( isa<Instruction> ( V ) ) { 
{ AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( 
auto bdims = batched -> bdims ( ) ; return bdims . back ( ) . level ( ) >= level ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { 
MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 || size < 0 ) { TORCH_CHECK ( false , "Failed to get the world_size / rank" ) ; } } } 
const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
TORCH_INTERNAL_ASSERT ( false , "unrecognized message type " , message . type ( ) ) ; } return true ; } 
static inline Tensor unary_op_impl_with_complex_to_float ( const Tensor & self , OutImpl & out_impl ) { if ( self . is_complex ( ) ) { const auto float_type = c10::toRealValueType ( self . scalar_type ( ) ) ; Tensor result = at::empty_like ( self , self . options ( ) . dtype ( float_type ) ) ; return out_impl ( result , self ) ; } 
void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device && static_cast<size_t> ( device ) < device_allocator . size ( ) , "Allocator not initialized for device " , device , ": did you call init?" ) ; 
isOpSupportedInMobile ( op ) , toString ( op ) , " is not supported in mobile module." ) ; code_ -> instructions_with_handles_ . emplace_back ( Instruction ( op , X , N ) , dbg_handle ) ; } bool Function::append_operator ( 
 void print_init_message ( const char * message ) { write ( 1 , message , strlen ( message ) ) ; write ( 1 , "n" , 1 ) ; } bool object_exists ( const char * name ) { 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
if ( rank_ == 0 || ( isP2POp ( opType ) && p2pRank == 0 ) ) { C10D_NCCL_CHECK ( ncclGetUniqueId ( & ncclID ) ) ; } 
data_ = variable . tensor_data ( ) ; } } 
if ( ompleted_ ) { TORCH_CHECK ( false , "Operation timed out!" ) ; } } if ( exception_ ) { 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
std::vector<uint8_t> workerNameVector = rankToNameStore_ . get ( c10::to_string ( workerId ) ) ; 
AT_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; if ( yPoolWindow ) myPoolWindow . reset ( pool -> newPoolWindow ( ) ) ; auto handle = myPoolWindow -> reserve ( device ) ; 
int64_t dim ) { auto self_ = moveBatchDimToFront ( self , self_bdim ) ; auto rank = rankWithoutBatchDim ( self , self_bdim ) ; dim = maybe_wrap_dim ( dim , rank + 1 ) ; if ( self_bdim ) { dim += 1 ; } return { self_ . unsqueeze ( dim ) , valIfNonempty ( self_bdim , 0 ) } ; } TORCH_LIBRARY_IMPL ( aten , FT_BATCHED_KEY , m ) { 
. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , "at::Tensor scalar type %s is not supported on java side" , c10::toString ( scalarType ) ) ; } const auto & tensorShape = tensor . sizes ( ) ; 
if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; C10_CUDA_CHECK ( cudaFree ( nullptr ) ) ; } } 
TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , "renorm: expected maxnorm to be >= 0 but got " , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , "renorm: input needs at least 2 dimensions, got " , ndim , " dimensions" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . push_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . push_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , self . options ( ) . dtype ( c10::kQUInt8 ) , scale , zero_point } ; 
std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { throw std::runtime_error ( "ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0" ) ; } 
if ( -> dim ( ) ) { return nullptr ; } return t -> withDim ( * t -> dim ( ) + 1 ) ; 
listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return listenPort ; } 
void operator ( ) ( int i ) { if ( cuda ) { const auto device_id = i % models_ . size ( ) ; auto d = torch::Device ( torch::DeviceType::CUDA , device_id ) ; to_device ( models_ [ device_id ] . forward ( to_device_vec ( eg_ , d ) ) , 
for ( const IValue & v : stack ) { if ( v . isTensor ( ) ) { const at::Tensor & t = v . toTensor ( ) ; if ( t . defined ( ) && t . requires_grad ( ) ) { return c10::nullopt ; 
const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; 
if ( v . isTensor ( ) ) { v = IValue ( detach ( std::move ( v ) . toTensor ( ) ) ) ; } else if ( v . isTensorList ( ) ) { std::vector<at::Tensor> lst = v . toTensorVector ( ) ; for ( auto & tensor : lst ) { tensor = detach ( tensor ) ; } v = std::move ( lst ) ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
if ( lhs . is_mkldnn ( ) || rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . is_nested ( ) || rhs . is_nested ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } 
inline Variable valueToTensor ( c10::TensorOptions options , PyObject * value , const at::Device & device ) { if ( THPVariable_Check ( value ) ) { return THPVariable_Unpack ( value ) ; } 
if ( ompleted_ ) { TORCH_CHECK ( false , "Operation timed out!" ) ; } } if ( exception_ ) { 
const auto & argument = ( * stack ) [ arguments_begin + arg_idx ] ; if ( batched_tensor_inputs_pos_iter == batched_tensor_inputs_position . end ( ) || ( int64_t ) arg_idx != * batched_tensor_inputs_pos_iter ) { torch::jit::push ( stack , argument ) ; continue ; 
auto iter = TensorIteratorConfig ( ) . add_output ( self ) . add_input ( src ) . resize_outputs ( false ) 
fntype , llvm::Function::PrivateLinkage , "pytorch" , module_ . get ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { if ( rgs [ i ] . isVar ( ) ) { 
if ( s_root_block_ || C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , "Schema is not available, but BlockRunner got kwargs." ) ; const auto total_num_inputs = args . size ( ) + first_input_is_self_ ; TORCH_CHECK ( total_num_inputs == block_info_ . num_inputs ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; ++ i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } 
P = L + ( R - L ) / 2 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; 
auto self_sizes = self . sizes ( ) ; VmapDimVector expanded_sizes ( self_sizes . begin ( ) , self_sizes . end ( ) ) ; expanded_sizes . insert ( expanded_sizes . begin ( ) + out_dim , batch_size ) ; auto result = self . expand ( expanded_sizes ) ; return result ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , "Missing the payload section" ) ; break ; } } 
if ( aStrides . empty ( ) || oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) , buildErrorMessage ( "Dimension mismatch for two accesses in mem dep checker in the fuser." ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; ++ b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; 
nnapi_processed = pyMethod ( wrapped_mod , inp . toTensor ( ) ) ; } else { py::list pyInp ; for ( at::Tensor inpElem : inp . toTensorList ( ) ) { pyInp . append ( inpElem ) ; } nnapi_processed = pyMethod ( wrapped_mod , pyInp ) ; 
auto numel = add_indices . numel ( ) ; int64_t ddim = src . size ( 1 ) ; auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } namespace { 
List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( src -> exception_ptr ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } 
TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , "renorm: expected maxnorm to be >= 0 but got " , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , "renorm: input needs at least 2 dimensions, got " , ndim , " dimensions" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } 
auto input = emitSugaredExpr ( apply . inputs ( ) [ 0 ] , 1 ) -> asValue ( loc , method ) ; if ( input -> type ( ) -> kind ( ) == TypeKind::TupleType ) { return std::make_shared<SimpleValue> ( emitIndex ( loc , self , createTupleUnpack ( input ) ) ) ; } return std::make_shared<SimpleValue> ( emitIndex ( loc , self , { input } ) ) ; } default : 
xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; 
variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( auto & g : grads ) { args . emplace_back ( std::move ( g ) ) ; } for ( const auto & saved : saved_tensors_ ) { args . emplace_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } 
int device ; C10_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; void * r = nullptr ; if ( forceUncachedAllocator ( ) ) { C10_CUDA_CHECK ( cudaMalloc ( & r , size ) ) ; return { r , r , & uncached_delete , Device ( DeviceType::CUDA , device ) } ; } if ( size != 0 ) { caching_allocator . malloc ( & r , device , size , cuda::getCurrentCUDAStream ( device ) ) ; } 
std::vector<uint8_t> workerNameVector = rankToNameStore_ . get ( c10::to_string ( workerId ) ) ; 
hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
std::vector<uint8_t> workerNameVector = rankToNameStore_ . get ( c10::to_string ( workerId ) ) ; 
if ( rank_ == 0 || ( isP2POp ( opType ) && p2pRank == 0 ) ) { C10D_NCCL_CHECK ( ncclGetUniqueId ( & ncclID ) ) ; } 
if ( y::is_none ( size ) ) { d -> set_size ( py::to_int ( size ) ) ; } return std::move ( d ) ; } py::object create_dimlist ( py::object name , py::handle size ) { 
const std::vector < IterDomain * > & new_root_domain , std::vector < IterDomain * > & rfactor_domain ) override { TORCH_INTERNAL_ASSERT ( ( index_ + 1 ) < new_root_domain . size ( ) , "Index: t" , index_ , "t Domain Size:t" , 
xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; 
e -> accept ( this ) ; if ( prec >= self_prec ) { os ( ) << ")" ; } } ; withParens ( v -> ret_val1 ( ) ) ; 
struct BucketKey { BucketKey ( c10::ScalarType type , c10::Device device ) : type ( type ) , device ( device ) { } const c10::ScalarType type ; const c10::Device device ; 
auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; TORCH_INTERNAL_ASSERT_DEBUG_ONLY ( reduce_tensor . scalar_type ( ) == at::ScalarType::Half , "Expected reduced tensor to be fp16 in FP16CompressHook, but got type " , reduce_tensor . scalar_type ( ) ) ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; 
void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; 
std::vector < llvm::Type * > params ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { auto const & arg = args [ i ] ; 
if ( asyncErrorHandling_ ) { std::unique_lock<std::mutex> lock ( workMetaListMutex_ ) ; for ( auto & work : workMetaList_ ) { work . checkAndSetException ( ) ; if ( work . exception ( ) ) { continue ; } 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , "The specified group name has already been " "created, please use a different group name" ) ; } 
int * ipiv , c10::complex<double> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnZgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuDoubleComplex * > ( dA ) , 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; 
static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; 
return { self . repeat ( sizes ) , nullopt } ; } VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return { self_ . repeat ( sizes_with_bdim ) , 0 } ; } 
if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } 
try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; C10_CUDA_CHECK ( cudaEventDestroy ( event_ ) ) ; if ( udaIPCGlobalEntities::alive ) { return ; } 
return [ ] ( ProcessedNode * p_node ) { const auto inputs = p_node -> Input ( 0 ) . toTensorVector ( ) ; TORCH_CHECK ( inputs . size ( ) > 0 , "stack expects non-empty tensor list" ) ; const auto dim = p_node -> Input ( 1 ) . toInt ( ) ; if ( p_node -> Output ( 0 ) . isNone ( ) ) { p_node -> Output ( 0 ) = at::native::_stack_cpu ( inputs , dim ) ; 
c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
TORCH_INTERNAL_ASSERT ( use_value_sizes_strides ) ; refreshMetadata ( ) ; } 
if ( measure_kernel_time_ || isDebugDumpEnabled ( DebugDumpOption::EffectiveBandwidth ) ) { cudaEventCreate ( & start_event ) ; cudaEventCreate ( & finish_event ) ; cudaEventRecord ( start_event ) ; } if ( execute_kernel_ ) { 
static inline Tensor unary_op_impl_with_complex_to_float ( const Tensor & self , OutImpl & out_impl ) { if ( self . is_complex ( ) ) { const auto float_type = c10::toRealValueType ( self . scalar_type ( ) ) ; Tensor result = at::empty_like ( self , self . options ( ) . dtype ( float_type ) ) ; return out_impl ( result , self ) ; } 
data_ = variable . tensor_data ( ) ; } } 
VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) { return impl ( ) -> image ( imageSizes ) ; } 
cpu_kernel ( iter , [ ] ( scalar_t a , scalar_t b ) -> scalar_t { TORCH_CHECK ( b != 0 , "ZeroDivisionError" ) ; scalar_t r = a % b ; if ( ( r != 0 ) && ( c10::is_negative ( r ) != c10::is_negative ( b ) ) ) { r += b ; } return r ; 
hash_key_t static_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
 void print_init_message ( const char * message ) { write ( 1 , message , strlen ( message ) ) ; write ( 1 , "n" , 1 ) ; } bool object_exists ( const char * name ) { 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
bool any_variable_defined ( const variable_list & variables ) { for ( const auto & variable : variables ) { if ( variable . defined ( ) ) { return true ; } 
auto input = qx ; if ( ndim == 4 ) { input = qx . to ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) { 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return address ; } 
void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } 
void emptyCache ( ) { for ( auto & da : device_allocator ) da -> emptyCache ( ) ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; 
fntype , llvm::Function::PrivateLinkage , "pytorch" , module_ . get ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { if ( rgs [ i ] . isVar ( ) ) { 
ComputeUnit & ComputeUnitFactory::get ( const std::string & cacheKey , const std::function < std::shared_ptr<ComputeUnit> ( ) > factoryFn ) { const auto it = computeUnits_ . find ( cacheKey ) ; if ( it != computeUnits_ . end ( ) ) { return * ( it -> second . get ( ) ) ; } 
List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( src -> exception_ptr ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
mobile::serialization::Module * module , ExtraFilesMap & extra_files ) { auto extra_files_offsets = module -> extra_files ( ) ; parseExtraFilesFromVector ( extra_files_offsets , & extra_files ) ; } 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( false ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) 
if ( s_root_block_ || C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , "Schema is not available, but BlockRunner got kwargs." ) ; const auto total_num_inputs = args . size ( ) + first_input_is_self_ ; TORCH_CHECK ( total_num_inputs == block_info_ . num_inputs ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; ++ i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } 
l . computeInline ( p . second -> buf ( ) ) ; 
TORCH_CHECK ( tensor . numel ( ) == 0 || data . data_ptr ( ) , "tolist() shouldn't be called on a tensor with unallocated storage" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } 
case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = "compressAllBuffers(l.root_stmt());n" ; randomization_helper::printHistory ( n_transform , message ) ; 
try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; C10_CUDA_CHECK ( cudaEventDestroy ( event_ ) ) ; if ( udaIPCGlobalEntities::alive ) { return ; } 
if ( self . dim ( ) < 2 ) { channel_dim = 0 ; } auto target_ = target . unsqueeze ( channel_dim ) ; auto grad_output_ = grad_output ; 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } 
cudaEvent_t event = e . first ; Block * block = e . second ; cudaError_t err = C10_CUDA_ERROR_HANDLED ( cudaEventQuery ( event ) ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; 
ParsedArgs < 5 > parsed_args ; auto r = parser . parse ( args , kwargs , parsed_args ) ; if ( r . has_torch_function ( ) ) { return handle_torch_function ( r , args , kwargs , THPNNVariableFunctionsModule , "torch.nn" , "_parse_to" ) ; } auto parsed = parse_to_conversion ( r , false ) ; auto & device = std::get < 0 > ( parsed ) ; 
if ( beta != scalar_t ( 1 ) && beta != scalar_t ( 0 ) ) scal<scalar_t> ( m , beta , y , incy ) ; for ( int64_t j = 0 ; j < n ; j ++ ) { scalar_t * column_ = a + lda * j ; scalar_t z = alpha * x [ j * incx ] ; for ( int64_t i = 0 ; i < m ; i ++ ) { if ( j == 0 && beta == scalar_t ( 0 ) ) { y [ i * incy ] = scalar_t ( 0 ) ; } y [ i * incy ] += z * column_ [ i ] ; } } 
if ( response == detail::CheckResponseType::NOT_READY ) { return false ; } throw std::runtime_error ( "ready or not_ready response expected" ) ; } void TCPStore::wait ( const std::vector<std::string> & keys ) { 
ptr += headerEnt . second ; } if ( ptr != endp ) { TORCH_CHECK ( false , "failed bounds" ) ; } return out ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
if ( visitedFunctions . empty ( ) ) { std::cerr << "[WARNING] could not find registered function for op: " << op << " in function: " << _demangle ( _name ( I -> getFunction ( ) ) ) << " contextualNamespace: " << contextualNamespace << std::endl ; } 
Tensor ravel ( const Tensor & self ) { return self . contiguous ( ) . view ( -1 ) ; } static inline void handle_unflatten_exception ( const std::runtime_error & e , 
bool DeduplicateInitializersByValue ( at::Tensor & t1 , at::Tensor & t2 ) { if ( t1 . dtype ( ) != t2 . dtype ( ) || 1 . sizes ( ) . equals ( t2 . sizes ( ) ) || 1 . strides ( ) . equals ( t2 . strides ( ) ) ) { return false ; } if ( t1 . device ( ) != t2 . device ( ) ) { return t1 . to ( "cpu" ) . equal ( t2 . to ( "cpu" ) ) ; } return t1 . equal ( t2 ) ; } 
if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << "Got exception: " << errMsg ; TORCH_CHECK ( false , errMsg ) ; } } 
{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; if ( ist ) return nullptr ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) return nullptr ; PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; 
payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , "Missing the payload section" ) ; break ; } } 
new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; 
static void THPGenerator_dealloc ( PyObject * _self ) { auto self = reinterpret_cast <THPGenerator*> ( _self ) ; if ( self -> cdata . defined ( ) ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; } Py_TYPE ( _self ) -> tp_free ( _self ) ; } 
auto input = qx ; if ( ndim == 4 ) { input = qx . to ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } 
auto x_nearest = x . round ( ) ; auto y_nearest = y . round ( ) ; auto i_x_nearest = convert_to_int_of_same_size ( x_nearest ) ; auto i_y_nearest = convert_to_int_of_same_size ( y_nearest ) ; 
hash_key_t static_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
l . computeInline ( p . second -> buf ( ) ) ; 
TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( "Unexpected number of bound info entries in cacheAccesses in the fuser." ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad || info . kind == kMutate ; bool hasWrites = info . kind == kStore || info . kind == kMutate ; 
auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) 
VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < ( int64_t ) sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return std::make_tuple ( self_ . repeat ( sizes_with_bdim ) , 0 ) ; 
void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ ++ ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ ++ ; } else { add_next_edge ( autograd::Edge { } ) ; index_ ++ ; } } 
void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , "Failed to create the control pipe to start the " "BackgroundThread run" ) ; } 
auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; 
const Var * metaVar = gpu_thread_vars_ [ gpu_thread_index ] ; body = Substitute ( Stmt::clone ( body ) , { { v -> var ( ) , metaVar } } ) ; 
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout && elapsed > timeout ) { TORCH_CHECK ( false , "Wait timeout" ) ; } 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { throw std::runtime_error ( "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; AT_CUDA_CHECK ( cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
data_ = variable . tensor_data ( ) ; } } 
void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device && static_cast<size_t> ( device ) < device_allocator . size ( ) , "Allocator not initialized for device " , device , ": did you call init?" ) ; 
else { bound_ = true ; dims_ . resize ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; ++ i ) { dims_ [ i ] = Dim::create ( py::unicode_from_format ( "%S%i" , name_ . ptr ( ) , ( int ) i ) ) ; } } 
PyObject * dist_autograd_init ( PyObject * _unused , PyObject * noargs ) { auto autograd_module = THPObjectPtr ( PyImport_ImportModule ( "torch.distributed.autograd" ) ) ; }
std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . emplace_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . emplace_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } 
if ( timeout != kNoTimeout ) { const auto elapsed = std::chrono::high_resolution_clock::now ( ) - start ; if ( elapsed > timeout ) { TORCH_CHECK ( false , kConnectTimeoutMsg ) ; } } std::this_thread::sleep_for ( std::chrono::seconds ( 1 ) ) ; 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; break ; } else { code_ . operators_ [ i ] = * func ; } 
for ( int64_t i = 0 ; i < n ; i ++ ) { recursiveStore ( data , sizes , strides , dim + 1 , tenElementSize , seq [ i ] ) ; data += strides [ dim ] * tenElementSize ; } 
SmallVector < Constant * , 16 > worklist ; SmallPtrSet < Constant * , 16 > visited ; if ( _isCallSite ( & I ) ) { Function * callee = _getCalledFunction ( & I ) ; if ( callee && allee -> isIntrinsic ( ) && visited . insert ( callee ) . second ) { CB ( callee ) ; } 
( uint8_t * ) selfName . c_str ( ) + selfName . length ( ) ) ; rankToNameStore_ . set ( c10::to_string ( selfId ) , selfNameVector ) ; workerIdToInfo_ . emplace ( selfId , WorkerInfo ( selfName , selfId ) ) ; 
std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; for ( auto & da : device_allocator ) { auto snap = da -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } 
l . computeInline ( p . second -> buf ( ) ) ; 
cudaEvent_t event = e . first ; Block * block = e . second ; cudaError_t err = C10_CUDA_ERROR_HANDLED ( cudaEventQuery ( event ) ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; 
for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx ++ ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . emplace_back ( ivalue . toTensor ( ) ) ; } return result ; } 
const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / 
vec_conv . emplace_back ( FusionBehavior::DYNAMIC , pair . second ) ; } else { TORCH_INTERNAL_ASSERT ( false , "FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: " , pair . first ) ; } 
bool CudaIPCCollect ( ) { if ( udaIPCGlobalEntities::alive ) { return true ; } bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; 
auto bdims = batched -> bdims ( ) ; return bdims . back ( ) . level ( ) >= level ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { 
void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; 
if ( ompleted_ ) { TORCH_CHECK ( false , "Operation timed out!" ) ; } } if ( exception_ ) { 
constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 || lda >= std::max<int64_t> ( 1 L , m ) ; } 
void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device && static_cast<size_t> ( device ) < device_allocator . size ( ) , "Allocator not initialized for device " , device , ": did you call init?" ) ; 
nnapi_processed = pyMethod ( wrapped_mod , inp . toTensor ( ) ) ; } else { py::list pyInp ; for ( at::Tensor inpElem : inp . toTensorList ( ) ) { pyInp . append ( inpElem ) ; } nnapi_processed = pyMethod ( wrapped_mod , pyInp ) ; 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , self . options ( ) . dtype ( c10::kQUInt8 ) , scale , zero_point } ; 
static PyObject * THPStorage_elementSize ( PyObject * _self , PyObject * noargs ) { HANDLE_TH_ERRORS return THPUtils_packInt64 ( sizeof ( uint8_t ) ) ; END_HANDLE_TH_ERRORS } 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( "Expected ranks to match in RfactorStoreRewriter in the fuser." ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; ++ i ) { 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
std::tuple < Tensor , Tensor > _symeig_helper_cpu ( const Tensor & self , bool eigenvectors , bool upper ) { auto infos = at::zeros ( { batchCount ( self ) } , self . options ( ) . dtype ( kInt ) ) ; auto self_sizes = self . sizes ( ) . vec ( ) ; self_sizes . pop_back ( ) ; 
bool CudaIPCCollect ( ) { if ( udaIPCGlobalEntities::alive ) { return true ; } bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
if ( comms ) { for ( const auto i : c10::irange ( ndevices ) ) { int dummy_var ; if ( C10_CUDA_ERROR_HANDLED ( cudaGetDevice ( & dummy_var ) ) != cudaSuccess ) { 
ptr += headerEnt . second ; } if ( ptr != endp ) { TORCH_CHECK ( false , "failed bounds" ) ; } return out ; } 
in0_t , dtype , layout , device , pin_memory , memory_format ) ; } auto & out_t = p_node -> Output ( 0 ) . toTensor ( ) ; at::native::resize_ ( out_t , in0_t . sizes ( ) , c10::nullopt ) ; at::native::fill_out ( out_t , in1_s ) ; } ; } ) ; 
makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , "" ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForInterface(): unsupported gloo device" ) ; } return device ; } 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
size_t idx = stack_ . back ( ) . toInt ( ) ; stack_ . pop_back ( ) ; TORCH_CHECK ( idx < globals_ . size ( ) , "Parsing error: out of bounds access to globals_" ) ; globals_ . at ( idx ) ( ) ; } break ; case PickleOpCode::BINPERSID : { 
const std::vector < IterDomain * > & new_root_domain , std::vector < IterDomain * > & rfactor_domain ) override { TORCH_INTERNAL_ASSERT ( ( index_ + 1 ) < new_root_domain . size ( ) , "Index: t" , index_ , "t Domain Size:t" , 
void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; 
public : std::size_t operator ( ) ( hash_key_t const & vec ) const { std::size_t seed = vec . size ( ) ; for ( auto & i : vec ) { seed ^= i + 0x9e3779b9 + ( seed << 6 ) + ( seed >> 2 ) ; 
std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { throw std::runtime_error ( "ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0" ) ; } 
struct THCCachingHostAllocator final : public at::Allocator { at::DataPtr allocate ( size_t size ) const override { void * ptr ; THCudaCheck ( allocator . malloc ( & ptr , size ) ) ; return { ptr , ptr , & THCCachingHostDeleter , at::DeviceType::CPU } ; 
variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( auto & g : grads ) { args . emplace_back ( std::move ( g ) ) ; } for ( const auto & saved : saved_tensors_ ) { args . emplace_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { 
Tensor d = at::full_like ( sizes , D ) ; sizes = at::cat ( { sizes , d } , 1 ) . to ( kCPU ) ; return at::_nested_from_padded ( t , sizes , false ) ; } 
if ( measure_kernel_time_ || isDebugDumpEnabled ( DebugDumpOption::EffectiveBandwidth ) ) { cudaEventCreate ( & start_event ) ; cudaEventCreate ( & finish_event ) ; cudaEventRecord ( start_event ) ; } if ( execute_kernel_ ) { 
return per_bucket_variable_indices ; } std::vector<int64_t> Logger::get_bucket_sizes ( ) { std::vector<int64_t> bucket_sizes ; for ( const auto & bucket : reducer_ -> buckets_ ) { const auto & variables = bucket . variables ; int64_t bucket_size = 0 ; for ( const auto & v : variables ) { bucket_size += v . numel ( ) * v . element_size ( ) ; } 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } 
if ( -> dim ( ) ) { return nullptr ; } return t -> withDim ( * t -> dim ( ) + 1 ) ; 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , "Number of workers for FileStore should be greater than zero" ) ; } } 
DWORD low = 1 , high = 0 ; OVERLAPPED offset = { 0 , 0 , 0 , 0 , NULL } ; if ( ( intptr_t ) hdl < 0 ) return -1 ; switch ( op ) { 
if ( min && max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { maximum_stub ( device_type ( ) , * this ) ; } else if ( max ) { minimum_stub ( device_type ( ) , * this ) ; } } 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
static void printDebugValue ( Value * V ) { if ( auto F = dyn_cast<Function> ( V ) ) { std::cerr << "[FUNC] " << _demangle ( _name ( F ) ) ; } else if ( isa<Constant> ( V ) ) { std::cerr << "[CONST] " << * V ; } else if ( isa<Instruction> ( V ) ) { 
SharedParserData & shared ; } ; Parser::Parser ( const std::shared_ptr<Source> & src ) : pImpl ( new ParserImpl ( src ) ) { } Parser : : ~ Parser ( ) = default ; 
void apply_eig ( const Tensor & self , bool eigenvectors , Tensor & vals_ , Tensor & vecs_ , int * info_ptr ) { if T_BUILD_WITH_LAPACK ( ) TORCH_CHECK ( false , "Calling torch.eig on a CPU tensor requires compiling " , "PyTorch with LAPACK. Please use PyTorch built with LAPACK support." ) ; } 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return address ; } 
auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) { 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshMetadata ( ) ; } 
SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = _name ( F ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; 
std::unordered_map < std::string , std::string > copied_metadata = owner_ -> metadata ( ) ; if ( owner_ -> metadata ( ) . find ( "model_name" ) == owner_ -> metadata ( ) . end ( ) ) { copied_metadata [ "model_name" ] = owner_ -> name ( ) ; } if ( observer ) { observer -> onEnterRunMethod ( copied_metadata , function_ -> name ( ) ) ; 
void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ ++ ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ ++ ; } else { add_next_edge ( autograd::Edge { } ) ; index_ ++ ; } } 
isOpSupportedInMobile ( op ) , toString ( op ) , " is not supported in mobile module." ) ; code_ -> instructions_with_handles_ . emplace_back ( Instruction ( op , X , N ) , dbg_handle ) ; } bool Function::append_operator ( 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
struct timeval timeoutTV = { static_cast<long> ( value . count ( ) / 1000 ) , static_cast<long> ( ( value . count ( ) % 1000 ) * 1000 ) } ; else struct timeval timeoutTV = { . tv_sec = value . count ( ) / 1000 , . tv_usec = ( value . count ( ) % 1000 ) * 1000 } ; 
Tensor std_backward ( const Tensor & result , const Tensor & grad , const Tensor & self , bool unbiased ) { return var_backward ( ( grad / ( result * 2 ) ) . masked_fill_ ( result == 0 , 0 ) , self , unbiased ) ; } 
PyObject * dist_autograd_init ( PyObject * _unused , PyObject * noargs ) { auto autograd_module = THPObjectPtr ( PyImport_ImportModule ( "torch.distributed.autograd" ) ) ; }
collectCatNodes ( graph_ -> block ( ) ) ; bool changed = false ; for ( auto c : cat_nodes_ ) { changed = replaceWithVariadicCat ( c ) || changed ; } return changed ; } 
int64_t output_zero_point ) { return apply_impl<true> ( input , output_scale , output_zero_point ) ; } 
auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; 
static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; 
auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } else if ( _bdim && _bdim ) { return { at::dot ( A_ , B_ ) , nullopt } ; } else { return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } } 
( uint8_t * ) selfName . c_str ( ) + selfName . length ( ) ) ; rankToNameStore_ . set ( c10::to_string ( selfId ) , selfNameVector ) ; workerIdToInfo_ . emplace ( selfId , WorkerInfo ( selfName , selfId ) ) ; 
{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; if ( ist ) return nullptr ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) return nullptr ; PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; 
void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; 
{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; if ( ist ) return nullptr ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) return nullptr ; PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( const auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
py::sequence_view seq = sizes ; auto size = seq . size ( ) ; self -> bind_len ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; ++ i ) { self -> dims_ [ i ] -> set_size ( py::to_int ( seq [ i ] ) ) ; } Py_RETURN_NONE ; 
bool CudaIPCCollect ( ) { if ( udaIPCGlobalEntities::alive ) { return true ; } bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = "compressAllBuffers(l.root_stmt());n" ; randomization_helper::printHistory ( n_transform , message ) ; 
void operator ( ) ( int i ) { if ( cuda ) { const auto device_id = i % models_ . size ( ) ; auto d = torch::Device ( torch::DeviceType::CUDA , device_id ) ; to_device ( models_ [ device_id ] . forward ( to_device_vec ( eg_ , d ) ) , 
void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device && static_cast<size_t> ( device ) < device_allocator . size ( ) , "Allocator not initialized for device " , device , ": did you call init?" ) ; 
if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data || ( advance_data_ptr == 0 ) ) ; data += advance_data_ptr ; } return list . release ( ) ; 
if ( -> dim ( ) ) { return nullptr ; } return t -> withDim ( * t -> dim ( ) + 1 ) ; 
makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , "" ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForInterface(): unsupported gloo device" ) ; } return device ; } 
std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; for ( auto & da : device_allocator ) { auto snap = da -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } 
vec_conv . emplace_back ( FusionBehavior::DYNAMIC , pair . second ) ; } else { TORCH_INTERNAL_ASSERT ( false , "FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: " , pair . first ) ; } 
vec_conv . emplace_back ( FusionBehavior::DYNAMIC , pair . second ) ; } else { TORCH_INTERNAL_ASSERT ( false , "FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: " , pair . first ) ; } 
if ( lhs . is_mkldnn ( ) || rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . is_nested ( ) || rhs . is_nested ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
cpu_kernel ( iter , [ ] ( scalar_t a , scalar_t b ) -> scalar_t { TORCH_CHECK ( b != 0 , "ZeroDivisionError" ) ; scalar_t r = a % b ; if ( ( r != 0 ) && ( c10::is_negative ( r ) != c10::is_negative ( b ) ) ) { r += b ; } return r ; 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; TORCH_INTERNAL_ASSERT_DEBUG_ONLY ( reduce_tensor . scalar_type ( ) == at::ScalarType::Half , "Expected reduced tensor to be fp16 in FP16CompressHook, but got type " , reduce_tensor . scalar_type ( ) ) ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; 
. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; 
TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( "Expected ranks to match in RfactorStoreRewriter in the fuser." ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; ++ i ) { 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
auto input = qx ; if ( ndim == 4 ) { input = qx . to ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
else { bound_ = true ; dims_ . resize ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; ++ i ) { dims_ [ i ] = Dim::create ( py::unicode_from_format ( "%S%i" , name_ . ptr ( ) , ( int ) i ) ) ; } } 
if ( HPVariable_Check ( _new_state ) ) { throw torch::TypeError ( "expected a torch.ByteTensor, but got %s" , Py_TYPE ( _new_state ) -> tp_name ) ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . emplace_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . emplace_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } 
if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; cudaFree ( nullptr ) ; } } 
else { const auto weight_data = weight_contig . data_ptr<float> ( ) ; at::parallel_for ( 0 , embedding_rows , 1 , [ & ] ( int64_t start_idx , int64_t end_idx ) { fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float> ( 
std::vector <const Expr*> indices ; for ( int i = 0 ; i < buf -> ndim ( ) ; i ++ ) { indices . push_back ( this -> args_ [ i ] ) ; } 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( const auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
data_ = variable . tensor_data ( ) ; } } 
std::vector < std::vector<at::Tensor> > & , std::vector<at::Tensor> & , const AllgatherOptions & ) { TORCH_CHECK ( false , "ProcessGroupMPI does not support allgather_coalesced" ) ; } 
else { const auto weight_data = weight_contig . data_ptr<float> ( ) ; at::parallel_for ( 0 , embedding_rows , 1 , [ & ] ( int64_t start_idx , int64_t end_idx ) { fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float> ( 
if ( visitedFunctions . empty ( ) ) { std::cerr << "[WARNING] could not find registered function for op: " << op << " in function: " << _demangle ( _name ( I -> getFunction ( ) ) ) << " contextualNamespace: " << contextualNamespace << std::endl ; } 
auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) 
at::from_blob ( data , c10::IntArrayRef ( shapes ) , c10::IntArrayRef ( strides ) , at::kChar ) . to ( at::kCPU ) ; data_node -> t_ ( Symbol::attr ( "value" ) , data_value . clone ( ) ) ; 
makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , "" ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForInterface(): unsupported gloo device" ) ; } return device ; } 
SmallVector < Constant * , 16 > worklist ; SmallPtrSet < Constant * , 16 > visited ; if ( _isCallSite ( & I ) ) { Function * callee = _getCalledFunction ( & I ) ; if ( callee && allee -> isIntrinsic ( ) && visited . insert ( callee ) . second ) { CB ( callee ) ; } 
new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; 
if ( self . dim ( ) < 2 ) { channel_dim = 0 ; } auto target_ = target . unsqueeze ( channel_dim ) ; auto grad_output_ = grad_output ; 
collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , "No device(s) specified" ) ; } 
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce_coalesced ( std::vector<at::Tensor> & tensors , const AllreduceCoalescedOptions & opts ) { throw std::runtime_error ( "allreduce_coalesced is currently not supported with NCCL" ) ; } 
void _index_put_impl__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
std::vector < llvm::Type * > params ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { auto const & arg = args [ i ] ; 
isOpSupportedInMobile ( op ) , toString ( op ) , " is not supported in mobile module." ) ; code_ -> instructions_with_handles_ . emplace_back ( Instruction ( op , X , N ) , dbg_handle ) ; } bool Function::append_operator ( 
if ( elf -> is_bound ( ) ) { py::raise_error ( DimensionBindError ( ) , "DimList not bound" ) ; } if ( idx < 0 || ( size_t ) idx >= self -> dims_ . size ( ) ) { py::raise_error ( PyExc_IndexError , "index out of bounds" ) ; } py::object r = self -> dims_ [ idx ] ; 
ParsedArgs < 5 > parsed_args ; auto r = parser . parse ( args , kwargs , parsed_args ) ; if ( r . has_torch_function ( ) ) { return handle_torch_function ( r , args , kwargs , THPNNVariableFunctionsModule , "torch.nn" , "_parse_to" ) ; } auto parsed = parse_to_conversion ( r , false ) ; auto & device = std::get < 0 > ( parsed ) ; 
SmallVector < Constant * , 16 > worklist ; SmallPtrSet < Constant * , 16 > visited ; if ( _isCallSite ( & I ) ) { Function * callee = _getCalledFunction ( & I ) ; if ( callee && allee -> isIntrinsic ( ) && visited . insert ( callee ) . second ) { CB ( callee ) ; } 
if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } 
struct timeval timeoutTV = { static_cast<long> ( value . count ( ) / 1000 ) , static_cast<long> ( ( value . count ( ) % 1000 ) * 1000 ) } ; else struct timeval timeoutTV = { . tv_sec = value . count ( ) / 1000 , . tv_usec = ( value . count ( ) % 1000 ) * 1000 } ; 
{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , "at::Tensor scalar type %s is not supported on java side" , c10::toString ( scalarType ) ) ; } const auto & tensorShape = tensor . sizes ( ) ; 
VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) { return impl ( ) -> image ( imageSizes ) ; } 
Tensor weight_ ; if ( weight && weight -> defined ( ) ) { auto self_ = self ; auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } 
Tensor unbind_backward ( const variable_list & grads , int64_t dim ) { IntArrayRef sizes ; at::TensorOptions o ; for ( const auto & v : grads ) { if ( v . defined ( ) ) { sizes = v . sizes ( ) ; o = static_cast<Tensor> ( v ) . options ( ) ; 
if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << "[INFO] ignore extra op schema str: " << * schemaStr << " in: " << _demangle ( _name ( src -> getFunction ( ) ) ) << ", because already found valid op schema str: " << * visitedOps -> begin ( ) << std::endl ; } 
if ( measure_kernel_time_ || isDebugDumpEnabled ( DebugDumpOption::EffectiveBandwidth ) ) { cudaEventCreate ( & start_event ) ; cudaEventCreate ( & finish_event ) ; cudaEventRecord ( start_event ) ; } if ( execute_kernel_ ) { 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } namespace { 
auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( false ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) 
std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } 
if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data || ( advance_data_ptr == 0 ) ) ; data += advance_data_ptr ; } return list . release ( ) ; 
auto numel = add_indices . numel ( ) ; int64_t ddim = src . size ( 1 ) ; auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
py::sequence_view seq = sizes ; auto size = seq . size ( ) ; self -> bind_len ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; ++ i ) { self -> dims_ [ i ] -> set_size ( py::to_int ( seq [ i ] ) ) ; } Py_RETURN_NONE ; 
TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , "renorm: expected maxnorm to be >= 0 but got " , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , "renorm: input needs at least 2 dimensions, got " , ndim , " dimensions" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } 
hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , "Number of workers for FileStore should be greater than zero" ) ; } } 
static VmapDimVector range ( int64_t start , int64_t stop ) { TORCH_INTERNAL_ASSERT ( stop >= start ) ; VmapDimVector dims ; dims . reserve ( stop - start ) ; 
std::vector < std::vector<at::Tensor> > & , std::vector<at::Tensor> & , const AllgatherOptions & ) { TORCH_CHECK ( false , "ProcessGroupMPI does not support allgather_coalesced" ) ; } 
if ( is_up_to_date ( ) ) { return ; } apply_updates ( ) ; regenerate_from_base ( ) ; } void FunctionalTensorWrapper::regenerate_from_base ( ) { 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , "The specified group name has already been " "created, please use a different group name" ) ; } 
PyObject * dist_autograd_init ( PyObject * _unused , PyObject * noargs ) { auto autograd_module = THPObjectPtr ( PyImport_ImportModule ( "torch.distributed.autograd" ) ) ; }
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; 
Tensor max_indices = std::get < 0 > ( indices . max ( 1 , false ) ) ; Tensor cpu_min_indices , cpu_max_indices ; if ( ndices . is_cpu ( ) ) { cpu_min_indices = min_indices . to ( at::DeviceType::CPU ) ; cpu_max_indices = max_indices . to ( at::DeviceType::CPU ) ; } 
int64_t size ( ) const { return cache_ . size ( ) ; } void clear ( ) { cache_ . clear ( ) ; } 
auto x_nearest = x . round ( ) ; auto y_nearest = y . round ( ) ; auto i_x_nearest = convert_to_int_of_same_size ( x_nearest ) ; auto i_y_nearest = convert_to_int_of_same_size ( y_nearest ) ; 
auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( false ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) 
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices ) ; } 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } 
at::optional<Section> ElfFile::findSection ( const char * name ) const { MULTIPY_CHECK ( name != nullptr , "Null name" ) ; at::optional<Section> found = at::nullopt ; for ( const auto & section : sections_ ) { if ( strcmp ( name , section . name ) == 0 ) { 
invalidArgument ( "unsupported layout" ) ; } } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
nnapi_processed = pyMethod ( wrapped_mod , inp . toTensor ( ) ) ; } else { py::list pyInp ; for ( at::Tensor inpElem : inp . toTensorList ( ) ) { pyInp . append ( inpElem ) ; } nnapi_processed = pyMethod ( wrapped_mod , pyInp ) ; 
std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } 
MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 || size < 0 ) { throw std::runtime_error ( "Failed to get the world_size / rank" ) ; } } } 
at::set_record_function_tls_ ( state . rf_tls_ ) ; SavedTensorDefaultHooks::set_hooks ( state . saved_tensors_default_hooks_ . first , state . saved_tensors_default_hooks_ . second ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; 
auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } else if ( _bdim && _bdim ) { return { at::dot ( A_ , B_ ) , nullopt } ; } else { return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } } 
int nested_if_then_else_ { 0 } ; const Store * nested_store_ { nullptr } ; bool nested_let_ { false } ; std::unordered_set < const Var * > thread_local_bufs_ ; 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; if ( resize_output ( result , { total_nonzero , ndim } ) ) { result . as_strided_ ( { total_nonzero , ndim } , { 1 , total_nonzero } ) ; } if ( result . numel ( ) == 0 ) { return result ; 
std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } 
const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = utils::RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / std::sqrt ( std::max ( rstd_val , T ( 0 ) ) + eps ) ; if ( gamma_null && beta_null ) { T * Y_ptr = Y_data + i * inner_size ; 
if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshMetadata ( ) ; } 
static void THPGenerator_dealloc ( PyObject * _self ) { auto self = reinterpret_cast <THPGenerator*> ( _self ) ; if ( self -> cdata . defined ( ) ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; } Py_TYPE ( _self ) -> tp_free ( _self ) ; } 
if ( timeout != kNoTimeout ) { const auto elapsed = std::chrono::high_resolution_clock::now ( ) - start ; if ( elapsed > timeout ) { TORCH_CHECK ( false , kConnectTimeoutMsg ) ; } } std::this_thread::sleep_for ( std::chrono::seconds ( 1 ) ) ; 
makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , "" ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForInterface(): unsupported gloo device" ) ; } return device ; } 
void _index_put_impl__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
int nested_if_then_else_ { 0 } ; const Store * nested_store_ { nullptr } ; bool nested_let_ { false } ; std::unordered_set < const Var * > thread_local_bufs_ ; 
} if ( FLAGS_torch_jit_enable_rethrow_caught_exception ) { if ( future_ ) { future_ -> setError ( std::current_exception ( ) ) ; return false ; } throw ; } 
auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { TORCH_CHECK ( false , "Stop_waiting response is expected" ) ; } } 
auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } else if ( _bdim && _bdim ) { return { at::dot ( A_ , B_ ) , nullopt } ; } else { return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } } 
if ( n -> kind ( ) == prim::CallMethod ) { auto m_opt = getInvokedModuleOpt ( module , n , graph -> inputs ( ) [ 0 ] ) ; if ( m_opt . has_value ( ) ) { invoked_methods . push_back ( std::make_pair ( * m_opt , n -> s ( attr::name ) ) ) ; } } 
std::shared_ptr<Source> findSourceInArchiveFromQualifier ( caffe2::serialize::PyTorchStreamReader & reader , const std::string & export_prefix , const std::string & qualifier ) 
static inline void assertValidDevice ( int device ) { const auto device_num = caching_allocator . device_allocator . size ( ) ; TORCH_CHECK ( 0 <= device && device < device_num , "Invalid device argument." ) ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; 
if ( y::is_none ( size ) ) { d -> set_size ( py::to_int ( size ) ) ; } return std::move ( d ) ; } py::object create_dimlist ( py::object name , py::handle size ) { 
ss << "Wrapper[" ; if ( wrapped -> level ( ) . has_value ( ) ) { ss << "lvl=" << wrapped -> level ( ) . value ( ) << ", " ; } else { ss << "dead, " ; } 
variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( auto & g : grads ) { args . emplace_back ( std::move ( g ) ) ; } for ( const auto & saved : saved_tensors_ ) { args . emplace_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { 
Tensor tensor ; if ( tensor_ . is_quantized ( ) ) { tensor = tensor_ . dequantize ( ) . to ( kCPU , kDouble ) . contiguous ( ) ; } else { tensor = tensor_ . to ( kCPU , kDouble ) . contiguous ( ) ; } 
AT_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; if ( yPoolWindow ) myPoolWindow . reset ( pool -> newPoolWindow ( ) ) ; auto handle = myPoolWindow -> reserve ( device ) ; 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; break ; } else { code_ . operators_ [ i ] = * func ; } 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } 
const std::vector < IterDomain * > & new_root_domain , std::vector < IterDomain * > & rfactor_domain ) override { TORCH_INTERNAL_ASSERT ( ( index_ + 1 ) < new_root_domain . size ( ) , "Index: t" , index_ , "t Domain Size:t" , 
static PyObject * THPStorage_elementSize ( PyObject * _self , PyObject * noargs ) { HANDLE_TH_ERRORS return THPUtils_packInt64 ( sizeof ( uint8_t ) ) ; END_HANDLE_TH_ERRORS } 
size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_with_handles_ [ pc ] . instruction ; 
TORCH_INTERNAL_ASSERT ( ( typeConstraints & ( kQintTypes | kComplexTypes ) ) == 0 , buildErrorMessage ( "Qint and Complex types are not supported in the fuser." ) ) ; return false ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
void VImage::addImageMemoryBarrierToGeneral ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_GENERAL ) ; } 
TORCH_CHECK ( operand . size ( j ) == operand . size ( dim ) , "einsum(): subscript " , einsum_index_to_label ( label ) , " is repeated for operand " , i , " but the sizes don't match, " , 
KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( "KernelScope() must be constructed before calling this" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } 
TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( "Unexpected number of bound info entries in cacheAccesses in the fuser." ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad || info . kind == kMutate ; bool hasWrites = info . kind == kStore || info . kind == kMutate ; 
P = L + ( R - L ) / 2 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; 
const auto & argument = ( * stack ) [ arguments_begin + arg_idx ] ; if ( batched_tensor_inputs_pos_iter == batched_tensor_inputs_position . end ( ) || ( int64_t ) arg_idx != * batched_tensor_inputs_pos_iter ) { torch::jit::push ( stack , argument ) ; continue ; 
if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << "[INFO] ignore extra op schema str: " << * schemaStr << " in: " << _demangle ( _name ( src -> getFunction ( ) ) ) << ", because already found valid op schema str: " << * visitedOps -> begin ( ) << std::endl ; } 
std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; for ( auto & da : device_allocator ) { auto snap = da -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } 
s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } 
collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , "No device(s) specified" ) ; } 
if ( HPVariable_Check ( _new_state ) ) { throw torch::TypeError ( "expected a torch.ByteTensor, but got %s" , Py_TYPE ( _new_state ) -> tp_name ) ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , "Failed to create the control pipe to start the " "BackgroundThread run" ) ; } 
if ( s_input ) { set_history ( tensor , grad_fn ) ; } grad_fn -> saved_tensors_ . emplace_back ( tensor , s_input ) ; } } torch::jit::push ( stack , result ) ; 
parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; ++ i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block , buildErrorMessage ( "Expected parent stmt to be a non-null Block in reorder transformation the fuser." ) ) ; block -> remove_stmt ( loops [ i ] ) ; } 
int nested_if_then_else_ { 0 } ; const Store * nested_store_ { nullptr } ; bool nested_let_ { false } ; std::unordered_set < const Var * > thread_local_bufs_ ; 
const auto & ty = static_cast < detail::ListImpl * > ( payload . u . as_intrusive_ptr ) -> elementType ; const auto expected_ty = c10::getTypePtr < c10::optional<at::Tensor> > ( ) ; return expected_ty == ty ; } bool IValue::isIntList ( ) const { 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
TORCH_CHECK ( operand . size ( j ) == operand . size ( dim ) , "einsum(): subscript " , einsum_index_to_label ( label ) , " is repeated for operand " , i , " but the sizes don't match, " , 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = _name ( F ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; 
if ( nsafe ) { auto min_length = lengths_value . min ( ) . item<int64_t> ( ) ; TORCH_CHECK ( ( min_length >= 0 ) , "lengths contains negative value!" ) ; TORCH_CHECK ( lengths_value . sum ( ) . item<int64_t> ( ) == data . size ( axis ) ) ; } 
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices ) ; } 
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout && elapsed > timeout ) { TORCH_CHECK ( false , "Wait timeout" ) ; } 
size_t idx = stack_ . back ( ) . toInt ( ) ; stack_ . pop_back ( ) ; TORCH_CHECK ( idx < globals_ . size ( ) , "Parsing error: out of bounds access to globals_" ) ; globals_ . at ( idx ) ( ) ; } break ; case PickleOpCode::BINPERSID : { 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
Tensor max_indices = std::get < 0 > ( indices . max ( 1 , false ) ) ; Tensor cpu_min_indices , cpu_max_indices ; if ( ndices . is_cpu ( ) ) { cpu_min_indices = min_indices . to ( at::DeviceType::CPU ) ; cpu_max_indices = max_indices . to ( at::DeviceType::CPU ) ; } 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; 
auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) { 
for ( const IValue & v : stack ) { if ( v . isTensor ( ) ) { const at::Tensor & t = v . toTensor ( ) ; if ( t . defined ( ) && t . requires_grad ( ) ) { return c10::nullopt ; 
const auto size = tuple PyTuple_GET_SIZE ( obj ) : PyList_GET_SIZE ( obj ) ; if ( size == 0 ) { return true ; } 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { throw std::runtime_error ( "Stop_waiting response is expected" ) ; } } 
if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
void index_put__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , "The specified group name has already been " "created, please use a different group name" ) ; } 
const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; if ( resize_output ( result , { total_nonzero , ndim } ) ) { result . as_strided_ ( { total_nonzero , ndim } , { 1 , total_nonzero } ) ; } if ( result . numel ( ) == 0 ) { return result ; 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
size_t idx = stack_ . back ( ) . toInt ( ) ; stack_ . pop_back ( ) ; TORCH_CHECK ( idx < globals_ . size ( ) , "Parsing error: out of bounds access to globals_" ) ; globals_ . at ( idx ) ( ) ; } break ; case PickleOpCode::BINPERSID : { 
if ( result . device ( ) == kMeta && result . dim ( ) > 0 ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; 
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout && elapsed > timeout ) { TORCH_CHECK ( false , "Wait timeout" ) ; } 
dims = { arguments [ dim_arg_pos ] . toInt ( ) } ; } else if ( arguments [ dim_arg_pos ] . isNone ( ) ) { reduction_case = ReductionCase::DimArray ; if ( logical_dim == 0 ) { dims = { 0 } ; } else { auto all_dims = range ( 0 , self . dim ( ) - 1 ) ; dims = std::vector<int64_t> ( all_dims . begin ( ) , all_dims . end ( ) ) ; } } else { TORCH_INTERNAL_ASSERT ( false , "Unexpected dtype found at dims" ) ; 
c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } 
auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } else if ( _bdim && _bdim ) { return { at::dot ( A_ , B_ ) , nullopt } ; } else { return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } } 
at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; if ( ( weight_dtype == at::kHalf || weight_dtype == at::kBFloat16 ) && bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } 
new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; 
TORCH_INTERNAL_ASSERT ( false , "unrecognized message type " , message . type ( ) ) ; } return true ; } 
: data ( std::move ( data ) ) , size ( size ) , deserializer ( nullptr ) , unpickled_records ( nullptr ) { } void ConcreteSourceRangeUnpickler::unpickle ( ) { 
std::vector <const Expr*> indices ; for ( int i = 0 ; i < buf -> ndim ( ) ; i ++ ) { indices . push_back ( this -> args_ [ i ] ) ; } 
fntype , llvm::Function::PrivateLinkage , "pytorch" , module_ . get ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { if ( rgs [ i ] . isVar ( ) ) { 
watchHandler ( socket ) ; } else { throw std::runtime_error ( "Unexpected query type" ) ; } } 
TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , "renorm: expected maxnorm to be >= 0 but got " , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , "renorm: input needs at least 2 dimensions, got " , ndim , " dimensions" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } 
auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; 
{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , "at::Tensor scalar type %s is not supported on java side" , c10::toString ( scalarType ) ) ; } const auto & tensorShape = tensor . sizes ( ) ; 
KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( "KernelScope() must be constructed before calling this" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } 
auto input = qx ; if ( ndim == 4 ) { input = qx . to ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
void emptyCache ( ) { for ( auto & da : device_allocator ) da -> emptyCache ( ) ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
auto numel = add_indices . numel ( ) ; int64_t ddim = src . size ( 1 ) ; auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( "KernelScope() must be constructed before calling this" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' && line_end < str . size ( ) ) { + line_end ; } 
void index_put__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; 
watchHandler ( socket ) ; } else { throw std::runtime_error ( "Unexpected query type" ) ; } } 
if ( self . dim ( ) < 2 ) { channel_dim = 0 ; } auto target_ = target . unsqueeze ( channel_dim ) ; auto grad_output_ = grad_output ; 
hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , "Failed to create the control pipe to start the " "BackgroundThread run" ) ; } 
while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + "_" + c10::to_string ( uid ++ ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; 
at::from_blob ( data , c10::IntArrayRef ( shapes ) , c10::IntArrayRef ( strides ) , at::kChar ) . to ( at::kCPU ) ; data_node -> t_ ( Symbol::attr ( "value" ) , data_value . clone ( ) ) ; 
HANDLE_TH_ERRORS TORCH_CHECK ( PyTuple_Size ( args ) == 2 , "_new_with_file takes exactly two arguments" ) ; int fd = PyObject_AsFileDescriptor ( PyTuple_GetItem ( args , 0 ) ) ; THPUtils_assert ( fd != -1 , "_new_with_file couldn't retrieve a file " "descriptor from given object" ) ; 
try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; C10_CUDA_CHECK ( cudaEventDestroy ( event_ ) ) ; if ( udaIPCGlobalEntities::alive ) { return ; } 
std::shared_ptr<Source> findSourceInArchiveFromQualifier ( caffe2::serialize::PyTorchStreamReader & reader , const std::string & export_prefix , const std::string & qualifier ) 
std::vector<Tensor> makeBatchedVector ( const std::vector<Tensor> & tensors , optional<int64_t> bdim , int64_t level ) { std::vector<Tensor> res ; for ( size_t idx = 0 ; idx < tensors . size ( ) ; idx ++ ) { res . emplace_back ( makeBatched ( tensors [ idx ] , bdim , level ) ) ; } return res ; } 
if ( s_input ) { set_history ( tensor , grad_fn ) ; } grad_fn -> saved_tensors_ . emplace_back ( tensor , s_input ) ; } } torch::jit::push ( stack , result ) ; 
void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } 
SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = _name ( F ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; 
if ( s_input ) { set_history ( tensor , grad_fn ) ; } grad_fn -> saved_tensors_ . emplace_back ( tensor , s_input ) ; } } torch::jit::push ( stack , result ) ; 
at::set_record_function_tls_ ( state . rf_tls_ ) ; SavedTensorDefaultHooks::set_hooks ( state . saved_tensors_default_hooks_ . first , state . saved_tensors_default_hooks_ . second ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; 
void set_data ( Tensor & self , const Tensor & new_data ) { AT_ERROR ( "set_data is not implemented for Tensor" ) ; } 
P = L + ( R - L ) / 2 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; 
int device ; C10_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; void * r = nullptr ; if ( forceUncachedAllocator ( ) ) { C10_CUDA_CHECK ( cudaMalloc ( & r , size ) ) ; return { r , r , & uncached_delete , Device ( DeviceType::CUDA , device ) } ; } if ( size != 0 ) { caching_allocator . malloc ( & r , device , size , cuda::getCurrentCUDAStream ( device ) ) ; } 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << "[INFO] ignore extra op schema str: " << * schemaStr << " in: " << _demangle ( _name ( src -> getFunction ( ) ) ) << ", because already found valid op schema str: " << * visitedOps -> begin ( ) << std::endl ; } 
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices ) ; } 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ ++ ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ ++ ; } else { add_next_edge ( autograd::Edge { } ) ; index_ ++ ; } } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; ++ i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block , buildErrorMessage ( "Expected parent stmt to be a non-null Block in reorder transformation the fuser." ) ) ; block -> remove_stmt ( loops [ i ] ) ; } 
if ( future_ ) { future_ -> setError ( std::make_exception_ptr ( Future::FutureError ( ss . str ( ) ) ) ) ; } else if ( is_jit_exception ) { throw JITException ( ss . str ( ) ) ; } 
SharedParserData & shared ; } ; Parser::Parser ( const std::shared_ptr<Source> & src ) : pImpl ( new ParserImpl ( src ) ) { } Parser : : ~ Parser ( ) = default ; 
auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; TORCH_INTERNAL_ASSERT_DEBUG_ONLY ( reduce_tensor . scalar_type ( ) == at::ScalarType::Half , "Expected reduced tensor to be fp16 in FP16CompressHook, but got type " , reduce_tensor . scalar_type ( ) ) ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; 
auto bdims = batched -> bdims ( ) ; return bdims . back ( ) . level ( ) >= level ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
in0_t , dtype , layout , device , pin_memory , memory_format ) ; } auto & out_t = p_node -> Output ( 0 ) . toTensor ( ) ; at::native::resize_ ( out_t , in0_t . sizes ( ) , c10::nullopt ) ; at::native::fill_out ( out_t , in1_s ) ; } ; } ) ; 
static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; 
for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) || ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( "Tensors are not equal in size or data type" ) ; } checkSingleTensorHelper ( tensor ) ; } 
if ( s_root_block_ || C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , "Schema is not available, but BlockRunner got kwargs." ) ; const auto total_num_inputs = args . size ( ) + first_input_is_self_ ; TORCH_CHECK ( total_num_inputs == block_info_ . num_inputs ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; ++ i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { throw std::runtime_error ( "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
if ( v . isTensor ( ) ) { v = IValue ( detach ( std::move ( v ) . toTensor ( ) ) ) ; } else if ( v . isTensorList ( ) ) { std::vector<at::Tensor> lst = v . toTensorVector ( ) ; for ( auto & tensor : lst ) { tensor = detach ( tensor ) ; } v = std::move ( lst ) ; } 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; ++ i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block , buildErrorMessage ( "Expected parent stmt to be a non-null Block in reorder transformation the fuser." ) ) ; block -> remove_stmt ( loops [ i ] ) ; } 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { throw std::runtime_error ( "The specified group name has already been " "created, please use a different group name" ) ; } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshMetadata ( ) ; } 
std::vector < llvm::Type * > params ; for ( size_t i = 0 ; i < args . size ( ) ; i ++ ) { auto const & arg = args [ i ] ; 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
std::vector<Tensor> makeBatchedVector ( const std::vector<Tensor> & tensors , optional<int64_t> bdim , int64_t level ) { std::vector<Tensor> res ; for ( size_t idx = 0 ; idx < tensors . size ( ) ; idx ++ ) { res . emplace_back ( makeBatched ( tensors [ idx ] , bdim , level ) ) ; } return res ; } 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; 
dims = { arguments [ dim_arg_pos ] . toInt ( ) } ; } else if ( arguments [ dim_arg_pos ] . isNone ( ) ) { reduction_case = ReductionCase::DimArray ; if ( logical_dim == 0 ) { dims = { 0 } ; } else { auto all_dims = range ( 0 , self . dim ( ) - 1 ) ; dims = std::vector<int64_t> ( all_dims . begin ( ) , all_dims . end ( ) ) ; } } else { TORCH_INTERNAL_ASSERT ( false , "Unexpected dtype found at dims" ) ; 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
if ( HPVariable_Check ( _new_state ) ) { throw torch::TypeError ( "expected a torch.ByteTensor, but got %s" , Py_TYPE ( _new_state ) -> tp_name ) ; } 
std::vector <const Expr*> indices ; for ( int i = 0 ; i < buf -> ndim ( ) ; i ++ ) { indices . push_back ( this -> args_ [ i ] ) ; } 
auto input = emitSugaredExpr ( apply . inputs ( ) [ 0 ] , 1 ) -> asValue ( loc , method ) ; if ( input -> type ( ) -> kind ( ) == TypeKind::TupleType ) { return std::make_shared<SimpleValue> ( emitIndex ( loc , self , createTupleUnpack ( input ) ) ) ; } return std::make_shared<SimpleValue> ( emitIndex ( loc , self , { input } ) ) ; } default : 
s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } 
public : std::size_t operator ( ) ( hash_key_t const & vec ) const { std::size_t seed = vec . size ( ) ; for ( auto & i : vec ) { seed ^= i + 0x9e3779b9 + ( seed << 6 ) + ( seed >> 2 ) ; 
xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; 
auto self_sizes = self . sizes ( ) ; VmapDimVector expanded_sizes ( self_sizes . begin ( ) , self_sizes . end ( ) ) ; expanded_sizes . insert ( expanded_sizes . begin ( ) + out_dim , batch_size ) ; auto result = self . expand ( expanded_sizes ) ; return result ; } 
parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; ++ i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block , buildErrorMessage ( "Expected parent stmt to be a non-null Block in reorder transformation the fuser." ) ) ; block -> remove_stmt ( loops [ i ] ) ; } 
if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . deepcopy ( ) ; std::string observer_name = "_observer_" + c10::to_string ( uid_ ++ ) ; 
if ( at::detail::getCUDAHooks ( ) . hasPrimaryContext ( idx ) ) { endif caller_current_streams_ [ idx ] = guard . getStream ( { c10::DeviceType::CUDA , idx } ) ; } else { caller_current_streams_ [ idx ] = c10::nullopt ; } } } 
mobile::serialization::Module * module , ExtraFilesMap & extra_files ) { auto extra_files_offsets = module -> extra_files ( ) ; parseExtraFilesFromVector ( extra_files_offsets , & extra_files ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
TORCH_CHECK ( tensor . numel ( ) == 0 || data . data_ptr ( ) , "tolist() shouldn't be called on a tensor with unallocated storage" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
void emptyCache ( ) { for ( auto & da : device_allocator ) da -> emptyCache ( ) ; } 
Tensor max_indices = std::get < 0 > ( indices . max ( 1 , false ) ) ; Tensor cpu_min_indices , cpu_max_indices ; if ( ndices . is_cpu ( ) ) { cpu_min_indices = min_indices . to ( at::DeviceType::CPU ) ; cpu_max_indices = max_indices . to ( at::DeviceType::CPU ) ; } 
static inline void assertValidDevice ( int device ) { const auto device_num = caching_allocator . device_allocator . size ( ) ; TORCH_CHECK ( 0 <= device && device < device_num , "Invalid device argument." ) ; } 
TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( "Unexpected number of bound info entries in cacheAccesses in the fuser." ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad || info . kind == kMutate ; bool hasWrites = info . kind == kStore || info . kind == kMutate ; 
 void print_init_message ( const char * message ) { write ( 1 , message , strlen ( message ) ) ; write ( 1 , "n" , 1 ) ; } bool object_exists ( const char * name ) { 
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce_coalesced ( std::vector<at::Tensor> & tensors , const AllreduceCoalescedOptions & opts ) { throw std::runtime_error ( "allreduce_coalesced is currently not supported with NCCL" ) ; } 
const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / 
TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( "Expected ranks to match in RfactorStoreRewriter in the fuser." ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; ++ i ) { 
at::set_record_function_tls_ ( state . rf_tls_ ) ; SavedTensorDefaultHooks::set_hooks ( state . saved_tensors_default_hooks_ . first , state . saved_tensors_default_hooks_ . second ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; 
{ key . bias_alignment = -1 ; } key . kReluFused = kReluFused ; auto run = [ & ] ( cudnn_frontend::ManagedOpaqueDescriptor plan_desc ) { auto workspace_size = 0 ; } 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
{ AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( 
P = L + ( R - L ) / 2 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; 
{ key . bias_alignment = -1 ; } key . kReluFused = kReluFused ; auto run = [ & ] ( cudnn_frontend::ManagedOpaqueDescriptor plan_desc ) { auto workspace_size = 0 ; } 
} if ( FLAGS_torch_jit_enable_rethrow_caught_exception ) { if ( future_ ) { future_ -> setError ( std::current_exception ( ) ) ; return false ; } throw ; } 
hash_key_t static_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
if ( nsafe ) { auto min_length = lengths_value . min ( ) . item<int64_t> ( ) ; TORCH_CHECK ( ( min_length >= 0 ) , "lengths contains negative value!" ) ; TORCH_CHECK ( lengths_value . sum ( ) . item<int64_t> ( ) == data . size ( axis ) ) ; } 
TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( "Expected ranks to match in RfactorStoreRewriter in the fuser." ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; ++ i ) { 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , "Number of workers for FileStore should be greater than zero" ) ; } } 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } namespace { 
at::from_blob ( data , c10::IntArrayRef ( shapes ) , c10::IntArrayRef ( strides ) , at::kChar ) . to ( at::kCPU ) ; data_node -> t_ ( Symbol::attr ( "value" ) , data_value . clone ( ) ) ; 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
auto self_sizes = self . sizes ( ) ; VmapDimVector expanded_sizes ( self_sizes . begin ( ) , self_sizes . end ( ) ) ; expanded_sizes . insert ( expanded_sizes . begin ( ) + out_dim , batch_size ) ; auto result = self . expand ( expanded_sizes ) ; return result ; } 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } 
struct timeval timeoutTV = { static_cast<long> ( value . count ( ) / 1000 ) , static_cast<long> ( ( value . count ( ) % 1000 ) * 1000 ) } ; else struct timeval timeoutTV = { . tv_sec = value . count ( ) / 1000 , . tv_usec = ( value . count ( ) % 1000 ) * 1000 } ; 
if ( result . device ( ) == kMeta && result . dim ( ) > 0 ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; 
TORCH_INTERNAL_ASSERT ( ( typeConstraints & ( kQintTypes | kComplexTypes ) ) == 0 , buildErrorMessage ( "Qint and Complex types are not supported in the fuser." ) ) ; return false ; } 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { throw std::runtime_error ( "The specified group name has already been " "created, please use a different group name" ) ; } 
return per_bucket_variable_indices ; } std::vector<int64_t> Logger::get_bucket_sizes ( ) { std::vector<int64_t> bucket_sizes ; for ( const auto & bucket : reducer_ -> buckets_ ) { const auto & variables = bucket . variables ; int64_t bucket_size = 0 ; for ( const auto & v : variables ) { bucket_size += v . numel ( ) * v . element_size ( ) ; } 
int * ipiv , c10::complex<float> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnCgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuComplex * > ( dA ) , 
if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } 
c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } 
if ( at::detail::getCUDAHooks ( ) . hasPrimaryContext ( idx ) ) { endif caller_current_streams_ [ idx ] = guard . getStream ( { c10::DeviceType::CUDA , idx } ) ; } else { caller_current_streams_ [ idx ] = c10::nullopt ; } } } 
at::assert_no_internal_overlap ( result ) ; const Tensor * pnotSkippedTensor = [ ] ( const TensorList & tensors ) -> const Tensor * { for ( auto const & tensor : tensors ) { if ( should_skip ( tensor ) ) { continue ; 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
bool CudaIPCCollect ( ) { if ( udaIPCGlobalEntities::alive ) { return true ; } bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; 
if ( HPVariable_Check ( _new_state ) ) { throw torch::TypeError ( "expected a torch.ByteTensor, but got %s" , Py_TYPE ( _new_state ) -> tp_name ) ; } 
for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx ++ ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . emplace_back ( ivalue . toTensor ( ) ) ; } return result ; } 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
std::end ( nextParameterIValues ) ) ; } } for ( auto n : nodesToDestroy ) { n -> destroy ( ) ; } return parameterIValues ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' && line_end < str . size ( ) ) { + line_end ; } 
if ( batch_mode ) { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; auto nbatch = input . size ( 0 ) ; 
size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_with_handles_ [ pc ] . instruction ; 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , "No device(s) specified" ) ; } 
static void THPGenerator_dealloc ( PyObject * _self ) { auto self = reinterpret_cast <THPGenerator*> ( _self ) ; if ( self -> cdata . defined ( ) ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; } Py_TYPE ( _self ) -> tp_free ( _self ) ; } 
collectCatNodes ( graph_ -> block ( ) ) ; bool changed = false ; for ( auto c : cat_nodes_ ) { changed = replaceWithVariadicCat ( c ) || changed ; } return changed ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
for ( int64_t i = 0 ; i < n ; i ++ ) { recursiveStore ( data , sizes , strides , dim + 1 , tenElementSize , seq [ i ] ) ; data += strides [ dim ] * tenElementSize ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
{ key . bias_alignment = -1 ; } key . kReluFused = kReluFused ; auto run = [ & ] ( cudnn_frontend::ManagedOpaqueDescriptor plan_desc ) { auto workspace_size = 0 ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; 
void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } 
constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 || lda >= std::max<int64_t> ( 1 L , m ) ; } 
if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . deepcopy ( ) ; std::string observer_name = "_observer_" + c10::to_string ( uid_ ++ ) ; 
for ( int64_t i = 0 ; i < n ; i ++ ) { recursiveStore ( data , sizes , strides , dim + 1 , tenElementSize , seq [ i ] ) ; data += strides [ dim ] * tenElementSize ; } 
const auto & ty = static_cast < detail::ListImpl * > ( payload . u . as_intrusive_ptr ) -> elementType ; const auto expected_ty = c10::getTypePtr < c10::optional<at::Tensor> > ( ) ; return expected_ty == ty ; } bool IValue::isIntList ( ) const { 
ComputeUnit & ComputeUnitFactory::get ( const std::string & cacheKey , const std::function < std::shared_ptr<ComputeUnit> ( ) > factoryFn ) { const auto it = computeUnits_ . find ( cacheKey ) ; if ( it != computeUnits_ . end ( ) ) { return * ( it -> second . get ( ) ) ; } 
Tensor tensor ; if ( tensor_ . is_quantized ( ) ) { tensor = tensor_ . dequantize ( ) . to ( kCPU , kDouble ) . contiguous ( ) ; } else { tensor = tensor_ . to ( kCPU , kDouble ) . contiguous ( ) ; } 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } 
void PyDefaultSavedVariableHooks::reset_hooks ( ) { std::lock_guard<std::mutex> lock ( mutex_ ) ; if ( Py_IsInitialized ( ) ) { py::gil_scoped_acquire gil ; Py_XDECREF ( pack_hook_ ) ; 
bool DeduplicateInitializersByValue ( at::Tensor & t1 , at::Tensor & t2 ) { if ( t1 . dtype ( ) != t2 . dtype ( ) || 1 . sizes ( ) . equals ( t2 . sizes ( ) ) || 1 . strides ( ) . equals ( t2 . strides ( ) ) ) { return false ; } if ( t1 . device ( ) != t2 . device ( ) ) { return t1 . to ( "cpu" ) . equal ( t2 . to ( "cpu" ) ) ; } return t1 . equal ( t2 ) ; } 
int * ipiv , c10::complex<float> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnCgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuComplex * > ( dA ) , 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } 
if ( future_ ) { future_ -> setError ( std::make_exception_ptr ( Future::FutureError ( ss . str ( ) ) ) ) ; } else if ( is_jit_exception ) { throw JITException ( ss . str ( ) ) ; } 
auto iter = TensorIteratorConfig ( ) . add_output ( self ) . add_input ( src ) . resize_outputs ( false ) 
hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
Tensor std_backward ( const Tensor & result , const Tensor & grad , const Tensor & self , bool unbiased ) { return var_backward ( ( grad / ( result * 2 ) ) . masked_fill_ ( result == 0 , 0 ) , self , unbiased ) ; } 
if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; cudaFree ( nullptr ) ; } } 
static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , "polygamma" , [ & ] ( ) { cpu_kernel ( 
HANDLE_TH_ERRORS TORCH_CHECK ( PyTuple_Size ( args ) == 2 , "_new_with_file takes exactly two arguments" ) ; int fd = PyObject_AsFileDescriptor ( PyTuple_GetItem ( args , 0 ) ) ; THPUtils_assert ( fd != -1 , "_new_with_file couldn't retrieve a file " "descriptor from given object" ) ; 
std::chrono::milliseconds ( kWaitForAbortCommStoreKey ) ) ; auto val = store_ -> get ( storeKey ) ; std::string rank ( reinterpret_cast <char*> ( val . data ( ) ) , val . size ( ) ) ; LOG ( INFO ) << "[Rank " << rank_ << "] Found key in store: " << storeKey << ", from rank: " << rank << ", aborting appropriate communicators" ; 
if ( n -> kind ( ) == prim::CallMethod ) { auto m_opt = getInvokedModuleOpt ( module , n , graph -> inputs ( ) [ 0 ] ) ; if ( m_opt . has_value ( ) ) { invoked_methods . push_back ( std::make_pair ( * m_opt , n -> s ( attr::name ) ) ) ; } } 
listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return listenPort ; } 
uint8_t output_alignment ; int8_t bias_alignment ; bool kReluFused ; } ; std::unordered_map < CacheKey , cudnn_frontend::ManagedOpaqueDescriptor , at::native::ParamsHash<CacheKey> , at::native::ParamsEqual<CacheKey> > execution_plan_cache ; } 
TORCH_INTERNAL_ASSERT ( ( typeConstraints & ( kQintTypes | kComplexTypes ) ) == 0 , buildErrorMessage ( "Qint and Complex types are not supported in the fuser." ) ) ; return false ; } 
py::sequence_view seq = sizes ; auto size = seq . size ( ) ; self -> bind_len ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; ++ i ) { self -> dims_ [ i ] -> set_size ( py::to_int ( seq [ i ] ) ) ; } Py_RETURN_NONE ; 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << "Got exception: " << errMsg ; TORCH_CHECK ( false , errMsg ) ; } } 
AT_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; if ( yPoolWindow ) myPoolWindow . reset ( pool -> newPoolWindow ( ) ) ; auto handle = myPoolWindow -> reserve ( device ) ; 
int device ; C10_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; void * r = nullptr ; if ( forceUncachedAllocator ( ) ) { C10_CUDA_CHECK ( cudaMalloc ( & r , size ) ) ; return { r , r , & uncached_delete , Device ( DeviceType::CUDA , device ) } ; } if ( size != 0 ) { caching_allocator . malloc ( & r , device , size , cuda::getCurrentCUDAStream ( device ) ) ; } 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( "Number of workers for FileStore should be greater than zero" ) ; } } 
if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( _name ( F ) ) ; } if ( Verbose > 1 ) { std::cerr << "[DEBUG][FUNC] " << _demangle ( _name ( F ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } 
case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = "compressAllBuffers(l.root_stmt());n" ; randomization_helper::printHistory ( n_transform , message ) ; 
{ AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( 
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices ) ; } 
std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { throw std::runtime_error ( "ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0" ) ; } 
if ( check_has_torch_function ( self ) ) { return handle_torch_function_setter ( ( THPVariable * ) self , "names" , names ) ; } auto & var = ( ( THPVariable * ) self ) -> cdata ;
: data ( std::move ( data ) ) , size ( size ) , deserializer ( nullptr ) , unpickled_records ( nullptr ) { } void ConcreteSourceRangeUnpickler::unpickle ( ) { 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
int64_t output_zero_point ) { return apply_impl<true> ( input , output_scale , output_zero_point ) ; } 
std::vector<Tensor> makeBatchedVector ( const std::vector<Tensor> & tensors , optional<int64_t> bdim , int64_t level ) { std::vector<Tensor> res ; for ( size_t idx = 0 ; idx < tensors . size ( ) ; idx ++ ) { res . emplace_back ( makeBatched ( tensors [ idx ] , bdim , level ) ) ; } return res ; } 
VaryingShape<size_t> stride_indices ; VaryingShape<int64_t> strides ; VaryingShape<int64_t> sizes ; if ( t . layout ( ) == at::kStrided && . is_nested ( ) ) { sizes = VaryingShape<int64_t> { t . sizes ( ) . vec ( ) } ; strides = VaryingShape<int64_t> { t . strides ( ) . vec ( ) } ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
std::vector <const Expr*> indices ; for ( int i = 0 ; i < buf -> ndim ( ) ; i ++ ) { indices . push_back ( this -> args_ [ i ] ) ; } 
while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + "_" + c10::to_string ( uid ++ ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; 
void index_put__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) 
if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( _name ( F ) ) ; } if ( Verbose > 1 ) { std::cerr << "[DEBUG][FUNC] " << _demangle ( _name ( F ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , "renorm: expected maxnorm to be >= 0 but got " , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , "renorm: input needs at least 2 dimensions, got " , ndim , " dimensions" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } 
if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data || ( advance_data_ptr == 0 ) ) ; data += advance_data_ptr ; } return list . release ( ) ; 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return address ; } 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in MPI process group" ) ; } 
for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get <1> ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } 
if ( str != "None" ) { default_string = parse_string_literal ( str ) ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
void apply_eig ( const Tensor & self , bool eigenvectors , Tensor & vals_ , Tensor & vecs_ , int * info_ptr ) { if T_BUILD_WITH_LAPACK ( ) TORCH_CHECK ( false , "Calling torch.eig on a CPU tensor requires compiling " , "PyTorch with LAPACK. Please use PyTorch built with LAPACK support." ) ; } 
if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( 
if ( aStrides . empty ( ) || oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) , buildErrorMessage ( "Dimension mismatch for two accesses in mem dep checker in the fuser." ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; ++ b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
{ AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
data_ = variable . tensor_data ( ) ; } } 
if ( at::detail::getCUDAHooks ( ) . hasPrimaryContext ( idx ) ) { endif caller_current_streams_ [ idx ] = guard . getStream ( { c10::DeviceType::CUDA , idx } ) ; } else { caller_current_streams_ [ idx ] = c10::nullopt ; } } } 
bool DeduplicateInitializersByValue ( at::Tensor & t1 , at::Tensor & t2 ) { if ( t1 . dtype ( ) != t2 . dtype ( ) || 1 . sizes ( ) . equals ( t2 . sizes ( ) ) || 1 . strides ( ) . equals ( t2 . strides ( ) ) ) { return false ; } if ( t1 . device ( ) != t2 . device ( ) ) { return t1 . to ( "cpu" ) . equal ( t2 . to ( "cpu" ) ) ; } return t1 . equal ( t2 ) ; } 
public : std::size_t operator ( ) ( hash_key_t const & vec ) const { std::size_t seed = vec . size ( ) ; for ( auto & i : vec ) { seed ^= i + 0x9e3779b9 + ( seed << 6 ) + ( seed >> 2 ) ; 
{ auto stream = c10::cuda::getCurrentCUDAStream ( device . index ( ) ) ; C10_CUDA_CHECK ( cudaStreamSynchronize ( stream ) ) ; event_ = nullptr ; event_sync_required_ = false ; } else 
void operator ( ) ( int i ) { if ( cuda ) { const auto device_id = i % models_ . size ( ) ; auto d = torch::Device ( torch::DeviceType::CUDA , device_id ) ; to_device ( models_ [ device_id ] . forward ( to_device_vec ( eg_ , d ) ) , 
if ( node -> kind ( ) == prim::CallMethod ) { Value * instance = node -> inputs ( ) [ 0 ] ; auto child_opt = getInvokedModuleOpt ( source , node , self ) ; if ( child_opt . has_value ( ) ) { auto qconfig = module_qconfig_map . at ( child_opt -> _ivalue ( ) ) ; instance -> setType ( type_remap_fn ( instance -> type ( ) , qconfig ) ) ; } } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
if ( node -> kind ( ) == prim::CallMethod ) { Value * instance = node -> inputs ( ) [ 0 ] ; auto child_opt = getInvokedModuleOpt ( source , node , self ) ; if ( child_opt . has_value ( ) ) { auto qconfig = module_qconfig_map . at ( child_opt -> _ivalue ( ) ) ; instance -> setType ( type_remap_fn ( instance -> type ( ) , qconfig ) ) ; } } 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
DWORD low = 1 , high = 0 ; OVERLAPPED offset = { 0 , 0 , 0 , 0 , NULL } ; if ( ( intptr_t ) hdl < 0 ) return -1 ; switch ( op ) { 
auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) 
KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( "KernelScope() must be constructed before calling this" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } 
cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; break ; } else if ( err != cudaSuccess ) { return err ; 
if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( _name ( F ) ) ; } if ( Verbose > 1 ) { std::cerr << "[DEBUG][FUNC] " << _demangle ( _name ( F ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } 
return { self . repeat ( sizes ) , nullopt } ; } VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return { self_ . repeat ( sizes_with_bdim ) , 0 } ; } 
VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < ( int64_t ) sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return std::make_tuple ( self_ . repeat ( sizes_with_bdim ) , 0 ) ; 
void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , "Failed to create the control pipe to start the " "BackgroundThread run" ) ; } 
invalidArgument ( "unsupported layout" ) ; } } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; 
loaded_sources_ . insert ( qualifier ) ; std::shared_ptr<Source> src = source_loader_ ( qualifier ) ; 
TORCH_CHECK ( tensor . numel ( ) == 0 || data . data_ptr ( ) , "tolist() shouldn't be called on a tensor with unallocated storage" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } 
if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . deepcopy ( ) ; std::string observer_name = "_observer_" + c10::to_string ( uid_ ++ ) ; 
auto attrVal = tryInsertConstant ( * graph , attr ) ; n -> output ( ) -> replaceAllUsesWith ( * attrVal ) ; nodesToDestroy . emplace ( n ) ; } } } 
const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; if ( resize_output ( result , { total_nonzero , ndim } ) ) { result . as_strided_ ( { total_nonzero , ndim } , { 1 , total_nonzero } ) ; } if ( result . numel ( ) == 0 ) { return result ; 
if ( s_root_block_ || C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , "Schema is not available, but BlockRunner got kwargs." ) ; const auto total_num_inputs = args . size ( ) + first_input_is_self_ ; TORCH_CHECK ( total_num_inputs == block_info_ . num_inputs ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; ++ i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } 
ComputeUnit & ComputeUnitFactory::get ( const std::string & cacheKey , const std::function < std::shared_ptr<ComputeUnit> ( ) > factoryFn ) { const auto it = computeUnits_ . find ( cacheKey ) ; if ( it != computeUnits_ . end ( ) ) { return * ( it -> second . get ( ) ) ; } 
if ( aStrides . empty ( ) || oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) , buildErrorMessage ( "Dimension mismatch for two accesses in mem dep checker in the fuser." ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; ++ b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; 
const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } 
static void THPGenerator_dealloc ( PyObject * _self ) { auto self = reinterpret_cast <THPGenerator*> ( _self ) ; if ( self -> cdata . defined ( ) ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; } Py_TYPE ( _self ) -> tp_free ( _self ) ; } 
work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , "waiting for processes to " "connect has timed out" ) ; } else if ( res == -1 ) { 
SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = _name ( F ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( "Number of workers for FileStore should be greater than zero" ) ; } } 
if ( result . device ( ) == kMeta && result . dim ( ) > 0 ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; 
ptr += headerEnt . second ; } if ( ptr != endp ) { TORCH_CHECK ( false , "failed bounds" ) ; } return out ; } 
void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; 
while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , "waiting for processes to " "connect has timed out" ) ; } else if ( res == -1 ) { 
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , "The specified group name has already been " "created, please use a different group name" ) ; } 
const auto size = tuple PyTuple_GET_SIZE ( obj ) : PyList_GET_SIZE ( obj ) ; if ( size == 0 ) { return true ; } 
std::unordered_map < std::string , std::string > copied_metadata = owner_ -> metadata ( ) ; if ( owner_ -> metadata ( ) . find ( "model_name" ) == owner_ -> metadata ( ) . end ( ) ) { copied_metadata [ "model_name" ] = owner_ -> name ( ) ; } if ( observer ) { observer -> onEnterRunMethod ( copied_metadata , function_ -> name ( ) ) ; 
return [ ] ( ProcessedNode * p_node ) { const auto inputs = p_node -> Input ( 0 ) . toTensorVector ( ) ; TORCH_CHECK ( inputs . size ( ) > 0 , "stack expects non-empty tensor list" ) ; const auto dim = p_node -> Input ( 1 ) . toInt ( ) ; if ( p_node -> Output ( 0 ) . isNone ( ) ) { p_node -> Output ( 0 ) = at::native::_stack_cpu ( inputs , dim ) ; 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } 
std::map < ParallelType , bool > ParallelTypeBitmap::getMap ( ) const { std::map < ParallelType , bool > map ; for ( const auto & pt_offset : pt_to_offset_ ) { map . emplace ( pt_offset . first , bitset_ [ pt_offset . second ] ) ; } return map ; } 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
Tensor weight_ ; if ( weight && weight -> defined ( ) ) { auto self_ = self ; auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } 
std::chrono::milliseconds ( kWaitForAbortCommStoreKey ) ) ; auto val = store_ -> get ( storeKey ) ; std::string rank ( reinterpret_cast <char*> ( val . data ( ) ) , val . size ( ) ) ; LOG ( INFO ) << "[Rank " << rank_ << "] Found key in store: " << storeKey << ", from rank: " << rank << ", aborting appropriate communicators" ; 
return per_bucket_variable_indices ; } std::vector<int64_t> Logger::get_bucket_sizes ( ) { std::vector<int64_t> bucket_sizes ; for ( const auto & bucket : reducer_ -> buckets_ ) { const auto & variables = bucket . variables ; int64_t bucket_size = 0 ; for ( const auto & v : variables ) { bucket_size += v . numel ( ) * v . element_size ( ) ; } 
const auto & ty = static_cast < detail::ListImpl * > ( payload . u . as_intrusive_ptr ) -> elementType ; const auto expected_ty = c10::getTypePtr < c10::optional<at::Tensor> > ( ) ; return expected_ty == ty ; } bool IValue::isIntList ( ) const { 
xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } 
if ( asyncErrorHandling_ ) { std::unique_lock<std::mutex> lock ( workMetaListMutex_ ) ; for ( auto & work : workMetaList_ ) { work . checkAndSetException ( ) ; if ( work . exception ( ) ) { continue ; } 
if ( node -> kind ( ) == prim::CallMethod ) { Value * instance = node -> inputs ( ) [ 0 ] ; auto child_opt = getInvokedModuleOpt ( source , node , self ) ; if ( child_opt . has_value ( ) ) { auto qconfig = module_qconfig_map . at ( child_opt -> _ivalue ( ) ) ; instance -> setType ( type_remap_fn ( instance -> type ( ) , qconfig ) ) ; } } 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } 
TORCH_INTERNAL_ASSERT ( use_value_sizes_strides ) ; refreshMetadata ( ) ; } 
const auto operand = operands [ i ] ; const auto labels = op_labels [ i ] ; const auto ndims = operand . dim ( ) ; int64_t nlabels = static_cast<int64_t> ( labels . size ( ) ) ; bool has_ellipsis = false ; for ( const auto & label : labels ) { 
while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + "_" + c10::to_string ( uid ++ ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; 
size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_with_handles_ [ pc ] . instruction ; 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } namespace { 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = "compressAllBuffers(l.root_stmt());n" ; randomization_helper::printHistory ( n_transform , message ) ; 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
if ( nsafe ) { auto min_length = lengths_value . min ( ) . item<int64_t> ( ) ; TORCH_CHECK ( ( min_length >= 0 ) , "lengths contains negative value!" ) ; TORCH_CHECK ( lengths_value . sum ( ) . item<int64_t> ( ) == data . size ( axis ) ) ; } 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } 
c10::intrusive_ptr<xnnpack::LinearOpContext> createLinearClampPrePackOpContext ( Tensor weight , c10::optional<Tensor> bias , c10::optional<Scalar> output_min , c10::optional<Scalar> output_max ) { return xnnpack::XNNPackLinearOpContext::create_context ( std::move ( weight ) , std::move ( bias ) , output_min , output_max ) ; } 
e -> accept ( this ) ; if ( prec >= self_prec ) { os ( ) << ")" ; } } ; withParens ( v -> ret_val1 ( ) ) ; 
for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx ++ ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . emplace_back ( ivalue . toTensor ( ) ) ; } return result ; } 
static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; 
at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; if ( ( weight_dtype == at::kHalf || weight_dtype == at::kBFloat16 ) && bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } 
if ( aStrides . empty ( ) || oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) , buildErrorMessage ( "Dimension mismatch for two accesses in mem dep checker in the fuser." ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; ++ b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; 
const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = utils::RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / std::sqrt ( std::max ( rstd_val , T ( 0 ) ) + eps ) ; if ( gamma_null && beta_null ) { T * Y_ptr = Y_data + i * inner_size ; 
listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return listenPort ; } 
Tensor grad_input ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( buffer ) 
public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; 
if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( 
cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; break ; } else if ( err != cudaSuccess ) { return err ; 
if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . deepcopy ( ) ; std::string observer_name = "_observer_" + c10::to_string ( uid_ ++ ) ; 
if ( batch_mode ) { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; auto nbatch = input . size ( 0 ) ; 
std::tuple < Tensor , Tensor > _symeig_helper_cpu ( const Tensor & self , bool eigenvectors , bool upper ) { auto infos = at::zeros ( { batchCount ( self ) } , self . options ( ) . dtype ( kInt ) ) ; auto self_sizes = self . sizes ( ) . vec ( ) ; self_sizes . pop_back ( ) ; 
TORCH_INTERNAL_ASSERT ( use_value_sizes_strides ) ; refreshMetadata ( ) ; } 
int * ipiv , c10::complex<double> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnZgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuDoubleComplex * > ( dA ) , 
{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; if ( ist ) return nullptr ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) return nullptr ; PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; 
Tensor weight_ ; if ( weight && weight -> defined ( ) ) { auto self_ = self ; auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } 
at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; if ( ( weight_dtype == at::kHalf || weight_dtype == at::kBFloat16 ) && bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } 
listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return listenPort ; } 
if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( _name ( F ) ) ; } if ( Verbose > 1 ) { std::cerr << "[DEBUG][FUNC] " << _demangle ( _name ( F ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } 
if ( check_has_torch_function ( self ) ) { return handle_torch_function_setter ( ( THPVariable * ) self , "names" , names ) ; } auto & var = ( ( THPVariable * ) self ) -> cdata ;
std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; 
char * other_data = data [ 1 ] ; for ( const auto i : c10::irange ( dim_size ) ) { ( void ) i ; if ( c10::load<scalar_t> ( self_data ) != c10::load<scalar_t> ( other_data ) ) { result = false ; return ; } 
shdrList_ = ( Elf64_Shdr * ) ( fileData + ehdr_ -> e_shoff ) ; auto strtabSecNo = ehdr_ -> e_shstrndx ; MULTIPY_CHECK ( strtabSecNo >= 0 && strtabSecNo < numSections_ , "e_shstrndx out of range" ) ; 
std::end ( nextParameterIValues ) ) ; } } for ( auto n : nodesToDestroy ) { n -> destroy ( ) ; } return parameterIValues ; } 
variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( auto & g : grads ) { args . emplace_back ( std::move ( g ) ) ; } for ( const auto & saved : saved_tensors_ ) { args . emplace_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { 
void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ ++ ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ ++ ; } else { add_next_edge ( autograd::Edge { } ) ; index_ ++ ; } } 
( uint8_t * ) selfName . c_str ( ) + selfName . length ( ) ) ; rankToNameStore_ . set ( c10::to_string ( selfId ) , selfNameVector ) ; workerIdToInfo_ . emplace ( selfId , WorkerInfo ( selfName , selfId ) ) ; 
if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshMetadata ( ) ; } 
cleanupKey_ ( "cleanup/" ) , regularPrefix_ ( "/" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( "Number of workers for FileStore should be greater than zero" ) ; } } 
while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + "_" + c10::to_string ( uid ++ ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; 
cudaEvent_t event = e . first ; Block * block = e . second ; cudaError_t err = C10_CUDA_ERROR_HANDLED ( cudaEventQuery ( event ) ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; 
constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 || lda >= std::max<int64_t> ( 1 L , m ) ; } 
uint8_t output_alignment ; int8_t bias_alignment ; bool kReluFused ; } ; std::unordered_map < CacheKey , cudnn_frontend::ManagedOpaqueDescriptor , at::native::ParamsHash<CacheKey> , at::native::ParamsEqual<CacheKey> > execution_plan_cache ; } 
Tensor ravel ( const Tensor & self ) { return self . contiguous ( ) . view ( -1 ) ; } static inline void handle_unflatten_exception ( const std::runtime_error & e , 
if ( check_has_torch_function ( self ) ) { return handle_torch_function_setter ( ( THPVariable * ) self , "names" , names ) ; } auto & var = ( ( THPVariable * ) self ) -> cdata ;
at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; if ( ( weight_dtype == at::kHalf || weight_dtype == at::kBFloat16 ) && bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return address ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , "ProcessGroupGloo does not support reduce_scatter" ) ; } 
at::set_record_function_tls_ ( state . rf_tls_ ) ; SavedTensorDefaultHooks::set_hooks ( state . saved_tensors_default_hooks_ . first , state . saved_tensors_default_hooks_ . second ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; 
: data ( std::move ( data ) ) , size ( size ) , deserializer ( nullptr ) , unpickled_records ( nullptr ) { } void ConcreteSourceRangeUnpickler::unpickle ( ) { 
c10::intrusive_ptr<xnnpack::LinearOpContext> createLinearClampPrePackOpContext ( Tensor weight , c10::optional<Tensor> bias , c10::optional<Scalar> output_min , c10::optional<Scalar> output_max ) { return xnnpack::XNNPackLinearOpContext::create_context ( std::move ( weight ) , std::move ( bias ) , output_min , output_max ) ; } 
invalidArgument ( "unsupported layout" ) ; } } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
auto numel = add_indices . numel ( ) ; int64_t ddim = src . size ( 1 ) ; auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
if ( y::is_none ( size ) ) { d -> set_size ( py::to_int ( size ) ) ; } return std::move ( d ) ; } py::object create_dimlist ( py::object name , py::handle size ) { 
c10::intrusive_ptr<xnnpack::LinearOpContext> createLinearClampPrePackOpContext ( Tensor weight , c10::optional<Tensor> bias , c10::optional<Scalar> output_min , c10::optional<Scalar> output_max ) { return xnnpack::XNNPackLinearOpContext::create_context ( std::move ( weight ) , std::move ( bias ) , output_min , output_max ) ; } 
Tensor grad_input ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( buffer ) 
bool any_variable_defined ( const variable_list & variables ) { for ( const auto & variable : variables ) { if ( variable . defined ( ) ) { return true ; } 
std::chrono::milliseconds ( kWaitForAbortCommStoreKey ) ) ; auto val = store_ -> get ( storeKey ) ; std::string rank ( reinterpret_cast <char*> ( val . data ( ) ) , val . size ( ) ) ; LOG ( INFO ) << "[Rank " << rank_ << "] Found key in store: " << storeKey << ", from rank: " << rank << ", aborting appropriate communicators" ; 
if ( asyncErrorHandling_ ) { std::unique_lock<std::mutex> lock ( workMetaListMutex_ ) ; for ( auto & work : workMetaList_ ) { work . checkAndSetException ( ) ; if ( work . exception ( ) ) { continue ; } 
if ( elf -> is_bound ( ) ) { py::raise_error ( DimensionBindError ( ) , "DimList not bound" ) ; } if ( idx < 0 || ( size_t ) idx >= self -> dims_ . size ( ) ) { py::raise_error ( PyExc_IndexError , "index out of bounds" ) ; } py::object r = self -> dims_ [ idx ] ; 
char * other_data = data [ 1 ] ; for ( const auto i : c10::irange ( dim_size ) ) { ( void ) i ; if ( c10::load<scalar_t> ( self_data ) != c10::load<scalar_t> ( other_data ) ) { result = false ; return ; } 
std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . push_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . push_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } 
int64_t size ( ) const { return cache_ . size ( ) ; } void clear ( ) { cache_ . clear ( ) ; } 
const Var * metaVar = gpu_thread_vars_ [ gpu_thread_index ] ; body = Substitute ( Stmt::clone ( body ) , { { v -> var ( ) , metaVar } } ) ; 
std::end ( nextParameterIValues ) ) ; } } for ( auto n : nodesToDestroy ) { n -> destroy ( ) ; } return parameterIValues ; } 
cpu_kernel ( iter , [ ] ( scalar_t a , scalar_t b ) -> scalar_t { TORCH_CHECK ( b != 0 , "ZeroDivisionError" ) ; scalar_t r = a % b ; if ( ( r != 0 ) && ( c10::is_negative ( r ) != c10::is_negative ( b ) ) ) { r += b ; } return r ; 
if ( rank_ == 0 || ( isP2POp ( opType ) && p2pRank == 0 ) ) { C10D_NCCL_CHECK ( ncclGetUniqueId ( & ncclID ) ) ; } 
TORCH_CHECK ( tensor . numel ( ) == 0 || data . data_ptr ( ) , "tolist() shouldn't be called on a tensor with unallocated storage" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx ++ ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . emplace_back ( ivalue . toTensor ( ) ) ; } return result ; } 
auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } 
auto input = emitSugaredExpr ( apply . inputs ( ) [ 0 ] , 1 ) -> asValue ( loc , method ) ; if ( input -> type ( ) -> kind ( ) == TypeKind::TupleType ) { return std::make_shared<SimpleValue> ( emitIndex ( loc , self , createTupleUnpack ( input ) ) ) ; } return std::make_shared<SimpleValue> ( emitIndex ( loc , self , { input } ) ) ; } default : 
if ( result . device ( ) == kMeta && result . dim ( ) > 0 ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; 
PyObject * c10d_init ( PyObject * _unused , PyObject * noargs ) { C10_LOG_API_USAGE_ONCE ( "c10d.python.import" ) ; auto c10d_module = THPObjectPtr ( PyImport_ImportModule ( "torch.distributed" ) ) ; }
if ( min && max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { maximum_stub ( device_type ( ) , * this ) ; } else if ( max ) { minimum_stub ( device_type ( ) , * this ) ; } } 
. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } 
MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 || size < 0 ) { throw std::runtime_error ( "Failed to get the world_size / rank" ) ; } } } 
std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . push_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . push_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } 
if ( lhs . is_mkldnn ( ) || rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . is_nested ( ) || rhs . is_nested ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } 
static Tensor norm ( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype ) { if ( self . is_sparse ( ) ) { return at::native_norm ( self , p , dim , keepdim , opt_dtype ) ; } else { Tensor result ; return at::native::norm_out ( result , self , p , dim , keepdim , opt_dtype ) ; } } 
if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; C10_CUDA_CHECK ( cudaFree ( nullptr ) ) ; } } 
TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( "Expected ranks to match in RfactorStoreRewriter in the fuser." ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; ++ i ) { 
if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } 
if ( elf -> is_bound ( ) ) { py::raise_error ( DimensionBindError ( ) , "DimList not bound" ) ; } if ( idx < 0 || ( size_t ) idx >= self -> dims_ . size ( ) ) { py::raise_error ( PyExc_IndexError , "index out of bounds" ) ; } py::object r = self -> dims_ [ idx ] ; 
if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } 
"schema" , [ ] ( Node & n ) { std::stringstream ss ; if ( n . maybeSchema ( ) ) { ss << n . schema ( ) ; } else { ss << "(no schema)" ; }
llvm::BasicBlock * bb_ ; llvm::Value * value_ { nullptr } ; llvm::JITTargetAddress kernelAddress_ ; 
at::optional<Section> ElfFile::findSection ( const char * name ) const { MULTIPY_CHECK ( name != nullptr , "Null name" ) ; at::optional<Section> found = at::nullopt ; for ( const auto & section : sections_ ) { if ( strcmp ( name , section . name ) == 0 ) { 
payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , "Missing the payload section" ) ; break ; } } 
std::vector < std::vector<at::Tensor> > & , std::vector<at::Tensor> & , const AllgatherOptions & ) { TORCH_CHECK ( false , "ProcessGroupMPI does not support allgather_coalesced" ) ; } 
static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; 
weights , weight_descriptors , _ = extract_weights ( model ) buffers , buf_descriptors , _ = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) 
fns = [ getattr ( cls , name ) for name in cls . __dict__ if inspect . isroutine ( getattr ( cls , name ) ) ] captures = { } for fn in fns : 
def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( "wb" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut 
if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) 
composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backward = _backward 
d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) 
def forward ( self , * args , * * kwargs ) : return compiled_f ( dict ( self . orig_module . named_parameters ( ) ) , dict ( self . orig_module . named_buffers ( ) ) , * args , * * kwargs ) 
else : d_bias = grad_out else : d_bias = None return ( d_input , d_weight , d_bias ) 
if str ( t . elem ) == nsor if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( BaseCType ( tensorT ) ) ) 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
elif node . target in FUNS_IO_TYPE_INT8 : return ( NodeInputOrOutputType . INT8 , NodeInputOrOutputType . INT8 ) elif node . target in FUNS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) ( _prev_node_input_type , 
index = self . _next_index ( ) may raise StopIteration data = self . _dataset_fetcher . fetch ( index ) may raise StopIteration if self . _pin_memory : data = _utils . pin_memory . pin_memory ( data , self . _pin_memory_device ) return data 
if found_tensor : n . meta [ nsor_meta = meta n . meta [ pe = type ( result ) return result 
def f ( * inps , out_tensors = None ) : inps = fx_model . graph . flatten_inps ( * inps ) if out_tensors is None : results = [ torch . empty ( shape , dtype = dtype ) for shape , dtype in outs [ 1 ] ] results = alloc_results else : results = out_tensors full_inps = module_stuff + list ( inps ) + results 
T = TypeVar ( 'T' , bound = ion [ _StorageBase , _TypedStorage ] class _StorageBase ( object ) : _cdata : Any is_sparse : bool = False is_sparse_csr : bool = False device : torch . device 
Overriding fused_module __call__ ( ) function with lower_to_prims_and_execute ( ) for node in fused_graph_module . graph . nodes : TODO : use a better way to identify fused submodule if node . op == "call_module" and "fused_" in node . name : fused_module = getattr ( fused_graph_module , node . name ) fused_module . _wrapped_call = self . lower_to_prims_and_execute 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
self . _module_copies [ 0 ] = self . module for module_copy in self . _module_copies [ 1 : ] : for param , copy_param in zip ( self . module . parameters ( ) , parameters ( module_copy ) ) : copy_param . requires_grad = param . requires_grad 
raise AttributeError ( f "'_OpNamespace' '{self.name}' object has no attribute '{op_name}'" ) from e 
if func == torch . ops . aten . is_contiguous . default : return True TODO : hack , we don currently support symbolic strides properly if func == torch . ops . aten . stride . default : return create_contiguous ( args [ 0 ] . shape ) TODO : An incomplete list 
tensor_type : OptionalCType = OptionalCType ( BaseCType ( tensorT ) ) if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( tensor_type ) ) 
ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if v is None : continue if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue 
def register_all ( mod ) : for name in dir ( mod ) : v = getattr ( mod , name ) if callable ( v ) and not _is_special_functional_bound_op ( v ) : _builtin_ops . append ( ( v , "aten::" + name ) ) for mod in _modules_containing_builtins : register_all ( mod ) 
n = ( k if n is None else n ) if X is None else X . shape [ -1 ] if ( m < 3 * n ) : raise ValueError ( BPCG algorithm is not applicable when the number of A rows ( = { } ) s smaller than 3 x the number of requested eigenpairs ( = { } ) . format ( m , n ) ) method = tho f method is None else method 
if has_sparse_grad is None : has_sparse_grad = any ( grad . is_sparse for grad in grads ) if maximize : grads = torch . _foreach_neg ( tuple ( grads ) ) type : ignore [ assignment ] if weight_decay != 0 : grads = torch . _foreach_add ( grads , params , alpha = weight_decay ) 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) 
if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . add_node ( node ) partition . used_mem_bytes += total_size_of_input_nodes 
def replace_node_module ( node : fx . Node , modules : Dict [ str , Any ] , new_module : torch . nn . Module ) : assert ( isinstance ( node . target , str ) ) parent_name , name = _parent_name ( node . target ) 
def _onnx_opset_unsupported_detailed ( op_name : str , current_opset : int , supported_opset : int , reason : str ) : raise RuntimeError ( f "Unsupported: ONNX export of {op_name} in " f "opset {current_opset}. {reason}. Please try opset version {supported_opset}." ) 
with preserve_rng_state ( ) : Set input tensors that require grad to leaves flat_tensor_args = pytree . tree_map ( lambda x : x . detach ( ) . requires_grad_ ( x . requires_grad ) if isinstance ( x , Tensor ) else x , flat_tensor_args ) with torch . set_grad_enabled ( grad_state ) : out = flat_fn ( * flat_tensor_args ) 
DEFAULT = "default (compile if called from a exported function / forward)" COPY_TO_SCRIPT_WRAPPER = "if this method is not scripted, copy the python method onto the scripted model" def export ( fn ) : 
pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , [ pad , pad ] , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) 
if mapping is None : mapping = get_default_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) 
def gen_vmap_plumbing ( native_function : NativeFunction ) -> Optional [ str ] : schema = native_function . func sig = DispatcherSignature . from_schema ( schema ) returns = schema . returns 
step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad / batch_size del param . grad_sample 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , use_error_feedback = use_error_feedback , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) 
if allow_list is None : allow_list = get_default_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) 
index = self . _next_index ( ) may raise StopIteration data = self . _dataset_fetcher . fetch ( index ) may raise StopIteration if self . _pin_memory : data = _utils . pin_memory . pin_memory ( data , self . _pin_memory_device ) return data 
def __str__ ( self ) -> str : all_arguments : List [ str ] = [ ] all_arguments . extend ( map ( str , self . flat_positional ) ) if self . flat_kwarg_only or self . out : all_arguments . append ( '*' ) all_arguments . extend ( map ( str , self . flat_kwarg_only ) ) all_arguments . extend ( map ( str , self . out ) ) return oin ( all_arguments ) 
out_args : List [ Dict [ str , Any ] ] = [ ] for arg in f . func . arguments . flat_positional : if arg . default is not None : continue 
def forward ( self , input ) : return self . activation_post_process ( F . relu ( ConvBn2d . _forward ( self , input ) ) ) 
def get_default_qat_qconfig ( backend = gemm version = 1 ) : Histogram observer is too slow for quantization aware training if version is None : if backend == gemm 
else : num_outs = 1 joint_inputs = ( flat_args , out ) with torch . enable_grad ( ) : fx_g = make_fx ( joint_forward_backward ) ( * joint_inputs ) fw_module , bw_module = partition_fn ( fx_g , joint_inputs ) 
def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None 
def forward ( self , input ) : return F . relu ( ConvBn2d . _forward ( self , input ) ) 
try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : _wrap_exception ( e ) } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result 
def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None 
out_and_self = list ( self . arguments . out ) + [ arg for arg in self . arguments . flat_positional if arg . name == "self" ] mutable_returns = [ ret for ret in self . returns if ret . annotation is not None and ret . annotation . is_write ] 
instance to the hook as the first parameter . handle = hooks . RemovableHandle ( self . _load_state_dict_pre_hooks ) self . _load_state_dict_pre_hooks [ handle . id ] = _WrappedHook ( hook , self if with_module else None ) return handle def register_load_state_dict_post_hook ( self , hook ) : 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
def _coalescing_manager ( group , reqs ) : if group is None : group = _get_default_group ( ) group . _start_coalescing ( ) try : yield finally : group . _end_coalescing ( reqs ) def batch_isend_irecv ( p2p_op_list ) : 
def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride . default ] def handle_symbolic_op ( func , args , kwargs ) : 
if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device 
if mapping is None : mapping = get_default_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
return_annotation = signature . return_annotation valid_return_annotation_type = return_annotation is not inspect . Parameter . empty and not isinstance ( return_annotation , str ) if valid_literal_annotation and valid_return_annotation_type : 
def get_type ( arg ) : if isinstance ( arg , fx . Node ) : return n . meta [ pe if pe n n . meta else None return type ( arg ) arg_types = map_aggregate ( n . args , get_type ) assert ( isinstance ( arg_types , tuple ) ) arg_types = tuple ( [ create_type_hint ( i ) for i in arg_types ] ) 
if do_write : with clean_ctx . open ( fout_path , 'w' , encoding = f -8 as fout : fout . write ( output_source ) return "ok" 
def get_output_device ( devices , op ) : The device propagation is a bit sketchy . aten::index ( CPU , CUDA ) = > CPU tensor aten::index ( CUDA , CPU ) = > CUDA tensor if op == aten . index : return devices [ 0 ] devices = list ( set ( devices ) ) if len ( devices ) == 1 : return devices [ 0 ] else : 
self . scale = torch . tensor ( [ 1 . 0 ] ) self . zero_point = torch . tensor ( [ 0 ] ) self . dtype = self . activation_post_process . dtype self . qscheme = self . activation_post_process . qscheme 
def __rpow__ ( self , other ) : dtype = torch . result_type ( other , self ) return torch . tensor ( other , dtype = dtype , device = self . device ) * * self 
return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) 
start_daemon = rank == 0 assert result . hostname is not None store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) 
def __getitem__ ( self , idx ) : if isinstance ( idx , list ) : return self . dataset [ [ self . indices [ i ] for i in idx ] ] return self . dataset [ self . indices [ idx ] ] 
g = Graph ( ) output_vals = g . graph_copy ( self , val_map = memo , return_output_node = True ) assert isinstance ( output_vals , tuple ) output_val , old_output_val = output_vals g . output ( output_val , type_expr = getattr ( old_output_val , pe None ) ) return g 
if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . add_node ( node ) partition . used_mem_bytes += total_size_of_input_nodes 
LockType = _dummy_thread . LockType boolean_dispatched : akref . WeakKeyDictionary [ Callable , Dict [ str , Callable ] ] weakref . WeakKeyDictionary ( ) def createResolutionCallbackFromEnv ( lookup_base ) : 
def forward ( self , input ) : return self . activation_post_process ( F . relu ( ConvBn2d . _forward ( self , input ) ) ) 
optimized_scripted_module = optimize_for_mobile ( traced_script_module ) exported_optimized_scripted_module = optimized_scripted_module . _save_for_lite_interpreter ( "model.ptl" ) 
def meta_dot ( self , tensor ) : check ( self . dim ( ) == 1 and tensor . dim ( ) == 1 , lambda : f "1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors" ) return self . new_empty ( ( ) ) 
if self . dispatch_key not in f . dispatch : return None if f . manual_kernel_registration : return None op_name = f "aten::{f.func.name}" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : 
symbolic_helper . _set_opset_version ( opset_version ) symbolic_helper . _set_operator_export_type ( operator_export_type ) with exporter_context ( model , training , verbose ) : val_keep_init_as_ip = _decide_keep_init_as_input ( keep_initializers_as_inputs , operator_export_type , opset_version 
weight = g . op ( "Constant" , value_t = weight_value ) if bias is None or _is_none ( bias ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) bias_value = torch . tensor ( [ 0 . 0 ] * channel_size , 
def meta_dot ( self , tensor ) : check ( self . dim ( ) == 1 and tensor . dim ( ) == 1 , lambda : f "1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors" ) return self . new_empty ( ( ) ) 
n = ( k if n is None else n ) if X is None else X . shape [ -1 ] if ( m < 3 * n ) : raise ValueError ( BPCG algorithm is not applicable when the number of A rows ( = { } ) s smaller than 3 x the number of requested eigenpairs ( = { } ) . format ( m , n ) ) method = tho f method is None else method 
res = kernel32 . LoadLibraryW ( dll ) if res is None : err = ctypes . WinError ( ctypes . get_last_error ( ) ) 
if norm_type == inf : norms = [ p . grad . detach ( ) . abs ( ) . max ( ) . to ( device ) for p in parameters ] total_norm = norms [ 0 ] if len ( norms ) == 1 else torch . max ( torch . stack ( norms ) ) 
def issubtype ( self , other ) : if isinstance ( other . param , _GenericAlias ) : if getattr ( other . param , origin__ None ) is Generic : return True if isinstance ( other , _DataPipeType ) : return issubtype ( self . param , other . param ) 
def _onnx_opset_unsupported_detailed ( op_name : str , current_opset : int , supported_opset : int , reason : str ) : raise RuntimeError ( f "Unsupported: ONNX export of {op_name} in " f "opset {current_opset}. {reason}. Please try opset version {supported_opset}." ) 
r = torch . Tensor . _make_subclass ( cls , elem , elem . requires_grad ) r . proxy = proxy if elem . is_sparse : proxy . node . meta [ nsor_meta = { } else : proxy . node . meta [ nsor_meta = _extract_tensor_metadata ( r ) return r 
if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . add_node ( node ) partition . used_mem_bytes += total_size_of_input_nodes 
try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : _wrap_exception ( e ) } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result 
B_ = bB [ i ] if bB is not None else None X_ = torch . randn ( ( m , n ) , dtype = dtype , device = device ) if bX is None else bX [ i ] assert len ( X_ . shape ) == 2 and X_ . shape == ( m , n ) , ( X_ . shape , ( m , n ) ) 
contiguous_args = [ t . contiguous ( ) for t in flat_args ] contiguous_args = [ t for t in flat_args ] out = normalize_as_list ( compiled_bw ( * ctx . saved_tensors , * contiguous_args ) ) return tuple ( out ) return CompiledFunction 
instance to the hook as the first parameter . handle = hooks . RemovableHandle ( self . _load_state_dict_pre_hooks ) self . _load_state_dict_pre_hooks [ handle . id ] = _WrappedHook ( hook , self if with_module else None ) return handle def register_load_state_dict_post_hook ( self , hook ) : 
tensor_read_requests = [ ] bytes_read_requests = [ ] storage_md = cast ( Dict [ MetadataIndex , str ] , metadata_from_storage . storage_data ) for fqn , obj in state_dict . items ( ) : md = metadata_from_storage . state_dict_metadata [ fqn ] if isinstance ( obj , ShardedTensor ) : 
if mpositeImplicitAutograd n dispatchKeys and mpositeExplicitAutograd n dispatchKeys : raise RuntimeError ( "Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed." ) 
def register_all ( mod ) : for name in dir ( mod ) : v = getattr ( mod , name ) if callable ( v ) and not _is_special_functional_bound_op ( v ) : _builtin_ops . append ( ( v , "aten::" + name ) ) for mod in _modules_containing_builtins : register_all ( mod ) 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) if custom_module_class_mapping is None : 
if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : 
class PythonTensor ( object ) : def __init__ ( self , out , proxy ) : if isinstance ( out , torch . Tensor ) : self . value = torch . clone ( out ) else : self . value = torch . empty ( out ) self . proxy = proxy 
T = TypeVar ( 'T' , bound = ion [ _StorageBase , _TypedStorage ] class _StorageBase ( object ) : _cdata : Any is_sparse : bool = False is_sparse_csr : bool = False device : torch . device 
def _div_aten ( a , b ) : is_integral = isinstance ( a , ( bool , int ) ) or ( isinstance ( a , torch . Tensor ) and utils . is_integer_dtype ( a . dtype ) ) if is_integral : return torch . div ( a , b , rounding_mode = "trunc" ) else : return torch . true_divide ( a , b ) div = _make_elementwise_binary_prim ( 
if str ( t . elem ) == nsor if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( BaseCType ( tensorT ) ) ) 
def log_abs_det_jacobian ( self , x , y ) : shape = x . shape scale = self . scale if isinstance ( scale , numbers . Real ) : result = torch . full_like ( x , math . log ( abs ( scale ) ) ) else : result = torch . abs ( scale ) . log ( ) 
raise RuntimeError ( f "The codegen expects to be able to generate '{generated_fns_str}'." f " To do so, it expects a line: 'autogen: {generated_fns_str}'." f " Instead, it found 'autogen: {expected_generated_fns_str}'" ) def signature ( self ) -> "FunctionSchema" : 
def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_symint_fn ( ) : name += "_symint" if func . is_out_fn ( ) : 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device 
def size ( g , self , dim = None ) : if dim is None : return g . op ( "Shape" , self ) return sym_help . _size_helper ( g , self , dim ) 
partition . logical_device_ids . append ( device . logical_id ) partition . add_node ( node ) partition_to_left_mem_bytes [ partition ] -= total_size_of_input_nodes 
func = func_overload . overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE : with proxy_mode . restore ( ) : return CURRENT_DECOMPOSITION_TABLE [ func_overload ] ( * args , * * kwargs ) 
for opt_tensor in optional_tensors : unwraps += unwrap_optional_tensor ( opt_tensor , cur_level_var ) unwrap_code = "n" . join ( unwraps ) unwrapped_arg_list = [ ] for arg in arg_names : 
paths . append ( p ) if not found_one : raise RuntimeError ( "Didn't find any test reports in s3, there is probably a bug!" ) return paths 
powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , use_error_feedback = use_error_feedback , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) 
super ( ) . __init__ ( ) self . _checkpoint_wrapped_module = mod self . checkpoint_impl = checkpoint_impl self . offload_to_cpu = offload_to_cpu state_dict post hook to remove prefix to allow loading into a 
torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
def hardtanh ( g , self , min_val , max_val ) : dtype = self . type ( ) . scalarType ( ) if dtype is None : dtype = 6 else : dtype = sym_help . scalar_type_to_onnx . index ( sym_help . cast_pytorch_to_onnx [ dtype ] ) 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
return_annotation = signature . return_annotation valid_return_annotation_type = return_annotation is not inspect . Parameter . empty and not isinstance ( return_annotation , str ) if valid_literal_annotation and valid_return_annotation_type : 
self . quant_min : int = quant_min self . quant_max : int = quant_max self . register_buffer ( "fake_quant_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . register_buffer ( "observer_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . is_symmetric_quant = _is_symmetric_quant ( self . activation_post_process . qscheme ) 
return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) 
functools . wraps ( next_func ) def wrap_next ( * args , * * kwargs ) : if torch . autograd . _profiler_enabled ( ) : return next_func ( * args , * * kwargs ) else : return next_func ( * args , * * kwargs ) namespace [ next__ = wrap_next 
return CompiledFunction def compiled_function ( fn , fw_compiler , bw_compiler , partition_fn = default_partition ) : saved_fn = None def returned_function ( * args , * * kwargs ) : 
def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride . default ] def handle_symbolic_op ( func , args , kwargs ) : 
def forward ( self , input ) : return F . relu ( F . linear ( input , self . weight_fake_quant ( self . weight ) , self . bias ) ) 
for arg in f . args . args : arg_annotation_str = get_annotation_str ( arg . annotation ) if arg . annotation else None 
return CompiledFunction def compiled_function ( fn , fw_compiler , bw_compiler , partition_fn = default_partition ) : saved_fn = None def returned_function ( * args , * * kwargs ) : 
Overriding fused_module __call__ ( ) function with lower_to_prims_and_execute ( ) for node in fused_graph_module . graph . nodes : TODO : use a better way to identify fused submodule if node . op == "call_module" and "fused_" in node . name : fused_module = getattr ( fused_graph_module , node . name ) fused_module . _wrapped_call = self . lower_to_prims_and_execute 
def compiled_module ( mod , * args , * * kwargs ) : def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , * args , * * kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) class CompiledModule ( nn . Module ) : def __init__ ( self ) : 
elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( "c10::List<c10::optional<Tensor>>" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) 
eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group . get ( ximize False ) for p in group [ rams if p . grad is not None : 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight , self . zero_bias ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
register_decomposition ( aten . tanh_backward ) def tanh_backward_decomposition ( out_grad : Tensor , y : Tensor ) : return out_grad * ( 1 - y * y ) register_decomposition ( aten . sigmoid_backward ) def sigmoid_backward_decomposition ( out_grad : Tensor , y : Tensor ) : 
X = torch . triangular_solve ( phi , L . transpose ( -1 , -2 ) . conj ( ) , upper = True ) . solution A_grad = torch . triangular_solve ( X . transpose ( -1 , -2 ) . conj ( ) P . transpose ( -1 , -2 ) , U , upper = True ) . solution . transpose ( -1 , -2 ) . conj ( ) return A_grad , None , None 
def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None 
elif node . op == tput if node . type is not None : maybe_return_annotation [ 0 ] = f " -> {type_repr(node.type)}" body . append ( f turn { repr ( node . args [ 0 ] ) } return 
pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , [ pad , pad ] , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) 
return sum / n register_decomposition ( aten . std . correction ) def std_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : return torch . sqrt ( torch . var ( x , dims , correction = correction , keepdim = keepdim ) ) 
fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) 
continue storage_key = storage_metadata [ MetadataIndex ( fqn , storage_md . offsets ) ] target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] 
contiguous_args = [ t . contiguous ( ) for t in flat_args ] contiguous_args = [ t for t in flat_args ] out = normalize_as_list ( compiled_bw ( * ctx . saved_tensors , * contiguous_args ) ) return tuple ( out ) return CompiledFunction 
if curr_modules . keys ( ) != prev_modules . keys ( ) : raise ValueError ( "The keys to the given mappings must have the same set of names of modules" ) summed_norms = torch . tensor ( 0 . ) if None in prev_modules . values ( ) : return False 
assert ( len ( flat_args ) == len ( flat_inps ) ) for idx , arg in enumerate ( flat_args ) : if isinstance ( flat_inps [ idx ] , torch . Tensor ) : flat_args [ idx ] = addPythonKey ( PythonTensor ( flat_inps [ idx ] , arg ) ) else : flat_args [ idx ] = flat_inps [ idx ] tree_args = pytree . tree_unflatten ( flat_args , args_spec ) 
def log_abs_det_jacobian ( self , x , y ) : shape = x . shape scale = self . scale if isinstance ( scale , numbers . Real ) : result = torch . full_like ( x , math . log ( abs ( scale ) ) ) else : result = torch . abs ( scale ) . log ( ) 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : 
assert ( len ( flat_args ) == len ( flat_inps ) ) for idx , arg in enumerate ( flat_args ) : if isinstance ( flat_inps [ idx ] , torch . Tensor ) : flat_args [ idx ] = addPythonKey ( PythonTensor ( flat_inps [ idx ] , arg ) ) else : flat_args [ idx ] = flat_inps [ idx ] tree_args = pytree . tree_unflatten ( flat_args , args_spec ) 
def replace_node_module ( node : fx . Node , modules : Dict [ str , Any ] , new_module : torch . nn . Module ) : assert ( isinstance ( node . target , str ) ) parent_name , name = _parent_name ( node . target ) 
has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if not maximize else - d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) 
if self . use_cuda : torch . cuda . synchronize ( ) 
def gen_returns ( returns : Tuple [ Return , . . . ] , cur_level_var : str , results_var : str ) -> str : idx = 0 wrapped_returns = [ ] for ret in returns : 
def rsub_Tensor ( self : Tensor , other : Tensor , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) def rsub_Scalar ( self : Tensor , other : float , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) 
continue storage_key = storage_metadata [ MetadataIndex ( fqn , storage_md . offsets ) ] target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] 
bytes_io = io . BytesIO ( ) brr = BytesReadRequest ( bytes = bytes_io , storage_key = storage_md [ MetadataIndex ( fqn ) ] , fqn = fqn ) bytes_read_requests . append ( brr ) 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
self . _module_copies = [ self . module ] self . modules_params = [ list ( parameters ( m ) ) for m in self . _module_copies ] self . modules_buffers = [ list ( m . buffers ( ) ) for m in self . _module_copies ] 
class PythonTensor ( object ) : def __init__ ( self , out , proxy ) : if isinstance ( out , torch . Tensor ) : self . value = torch . clone ( out ) else : self . value = torch . empty ( out ) self . proxy = proxy 
return mem_sz * 2 for node in full_bw_graph . nodes : if node in tangent_closure and node . op != tput nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) continue 
elif node . target in FUNS_IO_TYPE_INT8 : return ( NodeInputOrOutputType . INT8 , NodeInputOrOutputType . INT8 ) elif node . target in FUNS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) ( _prev_node_input_type , 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
optimized_scripted_module = optimize_for_mobile ( traced_script_module ) exported_optimized_scripted_module = optimized_scripted_module . _save_for_lite_interpreter ( "model.ptl" ) 
step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad / batch_size del param . grad_sample 
for arg in f . args . args : arg_annotation_str = get_annotation_str ( arg . annotation ) if arg . annotation else None 
def __getitem__ ( self , idx ) : if isinstance ( idx , list ) : return self . dataset [ [ self . indices [ i ] for i in idx ] ] return self . dataset [ self . indices [ idx ] ] 
def gen_unwraps ( flat_arguments : Sequence [ Argument ] , cur_level_var : str ) -> Tuple [ str , List [ str ] ] : arg_names = [ a . name for a in flat_arguments ] arg_types = [ a . type for a in flat_arguments ] 
if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta and node . op == ll_function users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : 
if curr_modules . keys ( ) != prev_modules . keys ( ) : raise ValueError ( "The keys to the given mappings must have the same set of names of modules" ) summed_norms = torch . tensor ( 0 . ) if None in prev_modules . values ( ) : return False 
self . tracker = tracker m = iparams [ 'm' ] n = iparams [ 'n' ] variable parameters self . X = X 
elif node . op == tput if node . type is not None : maybe_return_annotation [ 0 ] = f " -> {type_repr(node.type)}" body . append ( f turn { repr ( node . args [ 0 ] ) } return 
if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device 
if curr_modules . keys ( ) != prev_modules . keys ( ) : raise ValueError ( "The keys to the given mappings must have the same set of names of modules" ) summed_norms = torch . tensor ( 0 . ) if None in prev_modules . values ( ) : return False 
if mapping is None : mapping = get_default_static_quant_module_mappings ( ) if convert_custom_config_dict is None : convert_custom_config_dict = { } 
else : d_bias = grad_out else : d_bias = aten . new_empty ( input , ( 0 , ) ) return ( d_input , d_weight , d_bias ) register_decomposition ( aten . addmm ) 
if len ( body ) != 1 or not ( is_pass ( body [ 0 ] ) or is_ellipsis ( body [ 0 ] ) ) : msg = "Only `pass` statement or `...` can be the body of overload declaration:n" msg += 'n' . join ( parsed_def . source . split ( "n" ) [ : 3 ] ) 
tensor_read_requests = [ ] bytes_read_requests = [ ] storage_md = cast ( Dict [ MetadataIndex , str ] , metadata_from_storage . storage_data ) for fqn , obj in state_dict . items ( ) : md = metadata_from_storage . state_dict_metadata [ fqn ] if isinstance ( obj , ShardedTensor ) : 
elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( "c10::List<c10::optional<Tensor>>" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) 
def rsub_Tensor ( self : Tensor , other : Tensor , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) def rsub_Scalar ( self : Tensor , other : float , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) 
def __rpow__ ( self , other ) : dtype = torch . result_type ( other , self ) return torch . tensor ( other , dtype = dtype , device = self . device ) * * self 
if mapping is None : mapping = get_default_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
weights , weight_descriptors , _ = extract_weights ( model ) buffers , buf_descriptors , _ = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) 
_all_gather_sequence_id += 1 is_leader = leader_name == self_name if timeout == UNSET_RPC_TIMEOUT : timeout = get_rpc_timeout ( ) if is_leader : 
buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) , False ) buf_value = buf . getvalue ( ) 
def __setattr__ ( self , key : Any , value : Any ) -> None : if getattr ( self , "_initialized" , False ) : if not hasattr ( self , key ) and not isinstance ( value , torch . nn . Parameter ) : warnings . warn ( "Setting attributes on ParameterDict is not supported." ) super ( ParameterDict , self ) . __setattr__ ( key , value ) 
torch . _C . _log_api_usage_once ( "quantization_api._numeric_suite_fx.extract_weights" ) if base_name_to_sets_of_related_ops is None : base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops ( ) type_a_related_to_b = get_type_a_related_to_b ( base_name_to_sets_of_related_ops ) 
def get_post_build_suffix ( self ) -> str : if self . no_build_suffix : return "" if self . gpu_arch_type == "cuda" : return f "+cu{self.gpu_arch_version.replace('.', '')}" return f "+{self.gpu_arch_type}{self.gpu_arch_version}" 
if start_record is None and name == start_profile start_record = record elif cuda_start_event n name : assert record . device ( ) != -1 
with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and preserve_rng_state and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . 
def compiled_module ( mod , * args , * * kwargs ) : def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , * args , * * kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) class CompiledModule ( nn . Module ) : def __init__ ( self ) : 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
Y = torch . rand ( N , 1 , device = device ) Predefined nu_alpha and nu_beta , nu_alpha . shape : ( 1 , 1 ) , nu_beta . shape : ( 1 , 1 ) nu_alpha = torch . rand ( 1 , 1 , device = device ) nu_beta = torch . rand ( 1 , 1 , device = device ) nu = dist . Gamma ( nu_alpha , nu_beta ) 
for opt_tensor in optional_tensors : unwraps += unwrap_optional_tensor ( opt_tensor , cur_level_var ) unwrap_code = "n" . join ( unwraps ) unwrapped_arg_list = [ ] for arg in arg_names : 
if node . op == tput rv = map_arg ( node . args [ 0 ] , lambda n : val_map [ n ] ) return rv if not return_output_node else ( rv , node ) val_map [ node ] = self . node_copy ( node , lambda n : val_map [ n ] ) return None 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward 
def issubtype ( self , other ) : if isinstance ( other . param , _GenericAlias ) : if getattr ( other . param , origin__ None ) is Generic : return True if isinstance ( other , _DataPipeType ) : return issubtype ( self . param , other . param ) 
if getattr ( ret . __class__ , "__setstate__" , Tensor . __setstate__ ) is not Tensor . __setstate__ : ret . __setstate__ ( state ) else : if isinstance ( state , tuple ) : 
def var_decomposition ( x : Tensor , dims : Optional [ List [ int ] ] , correction : int = 0 , keepdim : bool = False ) : if dims is None : dims = [ ] if len ( dims ) == 0 : n = x . numel ( ) else : n = 1 
buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) , False ) buf_value = buf . getvalue ( ) 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , M ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( svd_lowrank , tensor_ops , A , q = q , niter = niter , M = M ) return _svd_lowrank ( A , q = q , niter = niter , M = M ) 
def gen_all_vmap_plumbing ( native_functions : Sequence [ NativeFunction ] ) -> str : body = "n" . join ( list ( mapMaybe ( ComputeBatchRulePlumbing ( ) , native_functions ) ) ) return f "" 
if RUN_BUILD_DEPS : build_deps ( ) extensions , cmdclass , packages , entry_points , extra_install_requires = configure_extension_build ( ) install_requires += extra_install_requires 
def arguments ( func : FunctionSchema ) -> Sequence [ MetaArgument ] : assert not func . arguments . out return list ( map ( argument , func . arguments . flat_non_out ) ) 
elif isinstance ( real_out , list ) : return list ( [ wrap_with_proxy ( e , proxy_out [ idx ] ) for idx , e in enumerate ( real_out ) ] ) elif isinstance ( real_out , torch . Tensor ) : return wrap_with_proxy ( real_out , proxy_out ) else : return real_out 
tune_option = auto_scheduler . TuningOptions ( num_measure_trials = 100 , change this to 20000 to achieve the best performance measure_callbacks = [ auto_scheduler . RecordToFile ( log_file ) ] , early_stopping = 1000 , verbose = 2 , ) tuner . tune ( tune_option ) 
super ( ) . __init__ ( ) self . _checkpoint_wrapped_module = mod self . checkpoint_impl = checkpoint_impl self . offload_to_cpu = offload_to_cpu state_dict post hook to remove prefix to allow loading into a 
def __rpow__ ( self , other ) : dtype = torch . result_type ( other , self ) return torch . tensor ( other , dtype = dtype , device = self . device ) * * self 
if self . use_cuda : torch . cuda . synchronize ( ) 
_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( "." ) , name , ( tensor , ) ) try : yield finally : for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( "." ) , name , ( ) ) def _apply_func_submodules ( 
g = Graph ( ) output_vals = g . graph_copy ( self , val_map = memo , return_output_node = True ) assert isinstance ( output_vals , tuple ) output_val , old_output_val = output_vals g . output ( output_val , type_expr = getattr ( old_output_val , pe None ) ) return g 
def gen_all_vmap_plumbing ( native_functions : Sequence [ NativeFunction ] ) -> str : body = "n" . join ( list ( mapMaybe ( ComputeBatchRulePlumbing ( ) , native_functions ) ) ) return f "" 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) cpu_fm . write ( 
powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , use_error_feedback = use_error_feedback , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) 
grad_input = grad_output - torch . exp ( output ) * torch . sum ( grad_output , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) TODO : the type annotations on arguments are not quite right 
def rsub_Tensor ( self : Tensor , other : Tensor , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) def rsub_Scalar ( self : Tensor , other : float , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) 
def register_all ( mod ) : for name in dir ( mod ) : v = getattr ( mod , name ) if callable ( v ) and not _is_special_functional_bound_op ( v ) : _builtin_ops . append ( ( v , "aten::" + name ) ) for mod in _modules_containing_builtins : register_all ( mod ) 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
def __init__ ( self , scale : float , zero_point : int , negative_slope : float = 1 e -2 , inplace : bool = False ) : super ( ) . __init__ ( negative_slope , inplace ) self . register_buffer ( ale torch . tensor ( scale ) ) self . register_buffer ( ro_point torch . tensor ( zero_point ) ) 
then we need to make sure get_attr is copied to the new graph . for x in flatten ( output_node . args [ 0 ] ) : if x . op == "get_attr" : setattr ( main_root , x . name , getattr_recursive ( gm , x . target ) ) type : ignore [ arg - type ] return torch . fx . GraphModule ( main_root , main_g ) 
def _dict_flatten ( d : Dict [ Any , Any ] ) -> Tuple [ List [ Any ] , Context ] : keys = sorted ( d . keys ( ) ) values = [ d [ key ] for key in keys ] return values , keys 
def __str__ ( self ) -> str : all_arguments : List [ str ] = [ ] all_arguments . extend ( map ( str , self . flat_positional ) ) if self . flat_kwarg_only or self . out : all_arguments . append ( '*' ) all_arguments . extend ( map ( str , self . flat_kwarg_only ) ) all_arguments . extend ( map ( str , self . out ) ) return oin ( all_arguments ) 
if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : 
try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : _wrap_exception ( e ) } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result 
_all_gather_sequence_id += 1 is_leader = leader_name == self_name if timeout == UNSET_RPC_TIMEOUT : timeout = get_rpc_timeout ( ) if is_leader : 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension 
if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) 
assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : 
_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( "." ) , name , ( tensor , ) ) try : yield finally : for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( "." ) , name , ( ) ) def _apply_func_submodules ( 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
def gen_unwraps ( flat_arguments : Sequence [ Argument ] , cur_level_var : str ) -> Tuple [ str , List [ str ] ] : arg_names = [ a . name for a in flat_arguments ] arg_types = [ a . type for a in flat_arguments ] 
training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , steps = training_size , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) 
def numpy_T ( g , input ) : ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( reversed ( range ( 0 , ndim ) ) ) return g . op ( "Transpose" , input , perm_i = perm ) 
def var_decomposition ( x : Tensor , dims : Optional [ List [ int ] ] , correction : int = 0 , keepdim : bool = False ) : if dims is None : dims = [ ] if len ( dims ) == 0 : n = x . numel ( ) else : n = 1 
def _functorch_str ( tensor , * , tensor_contents = None ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) 
fin_path = os . path . join ( output_directory , filepath ) with open ( fin_path , 'r' , encoding = f -8 as fin : output_source = fin . read ( ) fout_path = os . path . join ( output_directory , get_hip_file_path ( filepath ) ) 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) cpu_fm . write ( 
if allow_list is None : allow_list = get_default_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) 
grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output grad_input = new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) register_decomposition ( aten . _log_softmax_backward_data ) 
def _coalescing_manager ( group , reqs ) : if group is None : group = _get_default_group ( ) group . _start_coalescing ( ) try : yield finally : group . _end_coalescing ( reqs ) def batch_isend_irecv ( p2p_op_list ) : 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( "." ) , name , ( tensor , ) ) try : yield finally : for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( "." ) , name , ( ) ) def _apply_func_submodules ( 
def forward ( self , input ) : return self . activation_post_process ( F . relu ( ConvBn2d . _forward ( self , input ) ) ) 
kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) sig_group = CppSignatureGroup . from_schema ( f . func , method = False ) 
def _functorch_str ( tensor , * , tensor_contents = None ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) 
def __init__ ( self , dtype = torch . float32 , custom_op_name = "" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) 
def __eq__ ( self , other ) : if isinstance ( other , _DataPipeType ) : return self . param == other . param return NotImplemented 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
else : d_p = buf param . add_ ( d_p , alpha = - lr ) def _multi_tensor_sgd ( params : List [ Tensor ] , 
if weight is None or _is_none ( weight ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , 
if mapping is None : mapping = get_default_static_quant_module_mappings ( ) if convert_custom_config_dict is None : convert_custom_config_dict = { } 
func = func_overload . overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE : with proxy_mode . restore ( ) : return CURRENT_DECOMPOSITION_TABLE [ func_overload ] ( * args , * * kwargs ) 
if self . dispatch_key not in f . dispatch : return None if f . manual_kernel_registration : return None op_name = f "aten::{f.func.name}" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : 
"default" : default , "arg_name" : arg_name } return fn 
except ImportError : raise RuntimeError ( "Need networkx installed to perform smart recomputation heuristics" ) joint_module . graph . eliminate_dead_code ( ) joint_module . recompile ( ) fx_g = joint_module . graph 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output grad_input = new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) register_decomposition ( aten . _log_softmax_backward_data ) 
if len ( body ) != 1 or not ( is_pass ( body [ 0 ] ) or is_ellipsis ( body [ 0 ] ) ) : msg = "Only `pass` statement or `...` can be the body of overload declaration:n" msg += 'n' . join ( parsed_def . source . split ( "n" ) [ : 3 ] ) 
default_decompositions = get_decompositions ( default_decompositions ) def print_compile ( fx_g , _ ) : 
ops = set ( [ i . target for i in joint_module . graph . nodes if i . op == ll_function print ( ops - recomputable_ops ) AGGRESSIVE_RECOMPUTATION = False for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) 
prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) 
return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) 
if right is Any or left == right : return True if isinstance ( right , _GenericAlias ) : if getattr ( right , origin__ None ) is Generic : return True if right == type ( None ) : return False 
def get_post_build_suffix ( self ) -> str : if self . no_build_suffix : return "" if self . gpu_arch_type == "cuda" : return f "+cu{self.gpu_arch_version.replace('.', '')}" return f "+{self.gpu_arch_type}{self.gpu_arch_version}" 
def forward ( self , input ) : return F . relu ( F . linear ( input , self . weight_fake_quant ( self . weight ) , self . bias ) ) 
for i in range ( len ( current_commits . commits ) ) : c = current_commits . commits [ i ] if categorized n str ( c ) : feature_item = get_commit_data_cache ( ) . get ( c . commit_hash ) features = features_to_dict ( feature_item ) category , topic = CommitList . categorize ( features ) current_commits [ i ] = dataclasses . replace ( c , category = category , topic = topic ) current_commits . write_result ( ) def get_hash_or_pr_url ( commit : Commit ) : 
for i in range ( 2 ) : assert current_frame is not None Optional [ FrameType ] current_frame = current_frame . f_back 
then we need to make sure get_attr is copied to the new graph . for x in flatten ( output_node . args [ 0 ] ) : if x . op == "get_attr" : setattr ( main_root , x . name , getattr_recursive ( gm , x . target ) ) type : ignore [ arg - type ] return torch . fx . GraphModule ( main_root , main_g ) 
eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group . get ( ximize False ) for p in group [ rams if p . grad is not None : 
if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors , _ = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) 
return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) 
eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group . get ( ximize False ) for p in group [ rams if p . grad is not None : 
def get_default_qat_qconfig ( backend = gemm version = 1 ) : Histogram observer is too slow for quantization aware training if version is None : if backend == gemm 
with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and preserve_rng_state and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . 
def get_default_qat_qconfig ( backend = gemm version = 1 ) : Histogram observer is too slow for quantization aware training if version is None : if backend == gemm 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) self . _pre_backward_hook_full_params_prefetched = False Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ "all_gather" ] ) 
grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output grad_input = new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) register_decomposition ( aten . _log_softmax_backward_data ) 
group = p2p_op_list [ 0 ] . group if not all ( group == p2p_op . group for p2p_op in p2p_op_list ) : raise RuntimeError ( "All ops need to use the same group." ) def is_mpi_available ( ) : 
func_args = _wrap_all_tensors_to_functional ( args , func_level ) func_kwargs = _wrap_all_tensors_to_functional ( kwargs , func_level ) flattened_unwrapped_args , _ = tree_flatten ( args ) flattened_wrapped_args , _ = tree_flatten ( func_args ) flattened_unwrapped_kwargs , _ = tree_flatten ( kwargs ) flattened_wrapped_kwargs , _ = tree_flatten ( func_kwargs ) func_outputs = func ( * func_args , * * func_kwargs ) outputs = _unwrap_all_tensors_from_functional ( func_outputs ) 
args = tuple ( a for a in cpp_args if isinstance ( a , Argument ) ) input_arg_set = set ( a . name for a in f . func . arguments . flat_positional ) kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) 
if mpositeImplicitAutograd n dispatchKeys and mpositeExplicitAutograd n dispatchKeys : raise RuntimeError ( "Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed." ) 
def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( 
for node in graph . nodes : if node . op == ll_function cnt [ node . target . __name__ ] += 1 print ( sorted ( cnt . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) ) def min_cut_rematerialization_partition ( 
args = tuple ( a for a in cpp_args if isinstance ( a , Argument ) ) input_arg_set = set ( a . name for a in f . func . arguments . flat_positional ) kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) 
All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) self . _pre_backward_hook_full_params_prefetched = False Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ "all_gather" ] ) 
self . _op_impls = set ( ) self . kind = kind self . dispatch_key = dispatch_key def __repr__ ( self ) : return "Library(kind={}, ns={}, dispatch_key={})>" . format ( self . kind , self . ns , self . dispatch_key ) 
assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) if isinstance ( embed_dim , torch . Tensor ) : head_dim = embed_dim . div ( num_heads , rounding_mode = unc else : head_dim = embed_dim assert head_dim * num_heads == embed_dim , "embed_dim must be divisible by num_heads" scaling = float ( head_dim ) * * -0 . 5 
min_val = self . min_val max_val = self . max_val prev_zeros = False if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : prev_zeros = ( min_val . item ( ) == 0 ) and ( max_val . item ( ) == 0 ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or prev_zeros : min_val = torch . min ( x ) max_val = torch . max ( x ) 
group = p2p_op_list [ 0 ] . group if not all ( group == p2p_op . group for p2p_op in p2p_op_list ) : raise RuntimeError ( "All ops need to use the same group." ) def is_mpi_available ( ) : 
functools . wraps ( next_func ) def wrap_next ( * args , * * kwargs ) : if torch . autograd . _profiler_enabled ( ) : return next_func ( * args , * * kwargs ) else : return next_func ( * args , * * kwargs ) namespace [ next__ = wrap_next 
if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) 
All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) self . _pre_backward_hook_full_params_prefetched = False Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ "all_gather" ] ) 
if has_sparse_grad is None : has_sparse_grad = any ( grad . is_sparse for grad in grads ) if maximize : grads = torch . _foreach_neg ( tuple ( grads ) ) type : ignore [ assignment ] if weight_decay != 0 : grads = torch . _foreach_add ( grads , params , alpha = weight_decay ) 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) if custom_module_class_mapping is None : 
else : d_bias = grad_out else : d_bias = None return ( d_input , d_weight , d_bias ) 
def compiled_module ( mod , * args , * * kwargs ) : def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , * args , * * kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) class CompiledModule ( nn . Module ) : def __init__ ( self ) : 
model = copy . deepcopy ( model ) model . eval ( ) prepare ( model , inplace = True ) run_fn ( model , * run_args ) convert ( model , mapping , inplace = True ) return model 
self . _op_impls = set ( ) self . kind = kind self . dispatch_key = dispatch_key def __repr__ ( self ) : return "Library(kind={}, ns={}, dispatch_key={})>" . format ( self . kind , self . ns , self . dispatch_key ) 
tune_option = auto_scheduler . TuningOptions ( num_measure_trials = 100 , change this to 20000 to achieve the best performance measure_callbacks = [ auto_scheduler . RecordToFile ( log_file ) ] , early_stopping = 1000 , verbose = 2 , ) tuner . tune ( tune_option ) 
for arg in f . args . args : arg_annotation_str = get_annotation_str ( arg . annotation ) if arg . annotation else None 
_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( "." ) , name , ( tensor , ) ) try : yield finally : for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( "." ) , name , ( ) ) def _apply_func_submodules ( 
elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( "c10::List<c10::optional<Tensor>>" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] 
if node . op == aceholder return True return not all ( is_fusible ( node , user ) for user in node . users ) def get_node_weight ( node ) : mem_sz = _size_of ( node . meta [ nsor_meta 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) if custom_module_class_mapping is None : 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if not maximize else - d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
def __init__ ( self , dtype = torch . float32 , custom_op_name = "" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) 
def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : 
assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) if isinstance ( embed_dim , torch . Tensor ) : head_dim = embed_dim . div ( num_heads , rounding_mode = unc else : head_dim = embed_dim assert head_dim * num_heads == embed_dim , "embed_dim must be divisible by num_heads" scaling = float ( head_dim ) * * -0 . 5 
def get_type ( arg ) : if isinstance ( arg , fx . Node ) : return n . meta [ pe if pe n n . meta else None return type ( arg ) arg_types = map_aggregate ( n . args , get_type ) assert ( isinstance ( arg_types , tuple ) ) arg_types = tuple ( [ create_type_hint ( i ) for i in arg_types ] ) 
if node . op == tput rv = map_arg ( node . args [ 0 ] , lambda n : val_map [ n ] ) return rv if not return_output_node else ( rv , node ) val_map [ node ] = self . node_copy ( node , lambda n : val_map [ n ] ) return None 
def __init__ ( self , dtype = torch . float32 , custom_op_name = "" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) 
default_decompositions = get_decompositions ( default_decompositions ) def print_compile ( fx_g , _ ) : 
Y = torch . rand ( N , 1 , device = device ) Predefined nu_alpha and nu_beta , nu_alpha . shape : ( 1 , 1 ) , nu_beta . shape : ( 1 , 1 ) nu_alpha = torch . rand ( 1 , 1 , device = device ) nu_beta = torch . rand ( 1 , 1 , device = device ) nu = dist . Gamma ( nu_alpha , nu_beta ) 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
if node . op == tput rv = map_arg ( node . args [ 0 ] , lambda n : val_map [ n ] ) return rv if not return_output_node else ( rv , node ) val_map [ node ] = self . node_copy ( node , lambda n : val_map [ n ] ) return None 
def forward ( self , input ) : return F . relu ( ConvBn2d . _forward ( self , input ) ) 
def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : 
if mapping is None : mapping = get_default_qat_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
def numpy_T ( g , input ) : ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( reversed ( range ( 0 , ndim ) ) ) return g . op ( "Transpose" , input , perm_i = perm ) 
if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : 
else : d_bias = grad_out else : d_bias = aten . new_empty ( input , ( 0 , ) ) return ( d_input , d_weight , d_bias ) register_decomposition ( aten . addmm ) 
if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors , _ = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) 
start_daemon = rank == 0 assert result . hostname is not None store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) 
def draw_joint_graph ( graph , joint_inputs , file_name = "full_graph.png" ) : draw_graph ( graph , file_name ) return default_partition ( graph , joint_inputs ) def create_compiled_function ( flat_fn , fw_compiler , bw_compiler , partition_fn ) : joint_forward_backward = create_joint_forward_backward ( flat_fn ) 
else : d_p = buf param . add_ ( d_p , alpha = - lr ) def _multi_tensor_sgd ( params : List [ Tensor ] , 
for overloads and raise an exception if there are more than one . qualified_op_name = : : { } ormat ( self . name , op_name ) op = torch . _C . _jit_get_operation ( qualified_op_name ) 
return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
def __sizeof__ ( self ) : return ( super ( _StorageBase , self ) . __sizeof__ ( ) + self . element_size ( ) * self . size ( ) ) 
arranged_img_CHW = make_grid ( make_np ( label_img ) , ncols = nrow ) arranged_augment_square_HWC = np . zeros ( ( arranged_img_CHW . shape [ 2 ] , arranged_img_CHW . shape [ 2 ] , 3 ) ) arranged_img_HWC = arranged_img_CHW . transpose ( 1 , 2 , 0 ) 
if RUN_BUILD_DEPS : build_deps ( ) extensions , cmdclass , packages , entry_points , extra_install_requires = configure_extension_build ( ) install_requires += extra_install_requires 
def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , args , kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
def meta_dot ( self , tensor ) : check ( self . dim ( ) == 1 and tensor . dim ( ) == 1 , lambda : f "1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors" ) return self . new_empty ( ( ) ) 
else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) 
grad_input = grad_output - torch . exp ( output ) * torch . sum ( grad_output , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) TODO : the type annotations on arguments are not quite right 
pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) return _VF . stft ( input , n_fft , hop_length , win_length , window , normalized , onesided ) del torch . unique_dim 
def __setattr__ ( self , key : Any , value : Any ) -> None : if getattr ( self , "_initialized" , False ) : if not hasattr ( self , key ) and not isinstance ( value , torch . nn . Parameter ) : warnings . warn ( "Setting attributes on ParameterDict is not supported." ) super ( ParameterDict , self ) . __setattr__ ( key , value ) 
bytes_io = io . BytesIO ( ) brr = BytesReadRequest ( bytes = bytes_io , storage_key = storage_md [ MetadataIndex ( fqn ) ] , fqn = fqn ) bytes_read_requests . append ( brr ) 
if node . op == aceholder return True return not all ( is_fusible ( node , user ) for user in node . users ) def get_node_weight ( node ) : mem_sz = _size_of ( node . meta [ nsor_meta 
_all_gather_sequence_id += 1 is_leader = leader_name == self_name if timeout == UNSET_RPC_TIMEOUT : timeout = get_rpc_timeout ( ) if is_leader : 
def main ( args : List [ str ] ) -> None : parser = argparse . ArgumentParser ( description = "Generate unboxing source files" ) 
X = torch . triangular_solve ( phi , L . transpose ( -1 , -2 ) . conj ( ) , upper = True ) . solution A_grad = torch . triangular_solve ( X . transpose ( -1 , -2 ) . conj ( ) P . transpose ( -1 , -2 ) , U , upper = True ) . solution . transpose ( -1 , -2 ) . conj ( ) return A_grad , None , None 
self . _module_copies = [ self . module ] self . modules_params = [ list ( parameters ( m ) ) for m in self . _module_copies ] self . modules_buffers = [ list ( m . buffers ( ) ) for m in self . _module_copies ] 
def size ( g , self , dim = None ) : if dim is None : return g . op ( "Shape" , self ) return sym_help . _size_helper ( g , self , dim ) 
if node . op == ll_module if is_activation_post_process ( self . modules [ node . target ] ) : observer_module = self . modules [ node . target ] prev_node = node . args [ 0 ] if observer_module . dtype == torch . float16 : 
if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta and node . op == ll_function users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : 
proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) 
def arguments ( func : FunctionSchema ) -> Sequence [ MetaArgument ] : assert not func . arguments . out return list ( map ( argument , func . arguments . flat_non_out ) ) 
torch . _C . _log_api_usage_once ( "quantization_api.quantize.prepare" ) if prepare_custom_config_dict is None : prepare_custom_config_dict = get_default_custom_config_dict ( ) custom_module_class_mapping = prepare_custom_config_dict . get ( "float_to_observed_custom_module_class" , { } ) if not inplace : 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
def safe_unflatten ( tensor , dim , shape ) : if len ( shape ) == 0 : assert tensor . shape [ dim ] == 1 return tensor . squeeze ( dim ) return tensor . unflatten ( dim , shape ) def jacfwd ( f , argnums = 0 ) : 
composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict 
def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( 
raise AttributeError ( f "'_OpNamespace' '{self.name}' object has no attribute '{op_name}'" ) from e 
partition . logical_device_ids . append ( device . logical_id ) partition . add_node ( node ) partition_to_left_mem_bytes [ partition ] -= total_size_of_input_nodes 
ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if v is None : continue if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
if self . dispatch_key not in f . dispatch : return None if f . manual_kernel_registration : return None op_name = f "aten::{f.func.name}" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : 
self . module_groups = [ ] self . enable_mask_update = False self . activation_handles = [ ] self . bias_handles = [ ] self . model = model 
torch . _C . _log_api_usage_once ( "quantization_api.quantize.prepare" ) if prepare_custom_config_dict is None : prepare_custom_config_dict = get_default_custom_config_dict ( ) custom_module_class_mapping = prepare_custom_config_dict . get ( "float_to_observed_custom_module_class" , { } ) if not inplace : 
if mpositeImplicitAutograd n dispatchKeys and mpositeExplicitAutograd n dispatchKeys : raise RuntimeError ( "Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed." ) 
ops = set ( [ i . target for i in joint_module . graph . nodes if i . op == ll_function print ( ops - recomputable_ops ) AGGRESSIVE_RECOMPUTATION = False for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
LockType = _dummy_thread . LockType boolean_dispatched : akref . WeakKeyDictionary [ Callable , Dict [ str , Callable ] ] weakref . WeakKeyDictionary ( ) def createResolutionCallbackFromEnv ( lookup_base ) : 
elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( "c10::List<c10::optional<Tensor>>" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) 
raise RuntimeError ( f "The codegen expects to be able to generate '{generated_fns_str}'." f " To do so, it expects a line: 'autogen: {generated_fns_str}'." f " Instead, it found 'autogen: {expected_generated_fns_str}'" ) def signature ( self ) -> "FunctionSchema" : 
args = tuple ( a for a in cpp_args if isinstance ( a , Argument ) ) input_arg_set = set ( a . name for a in f . func . arguments . flat_positional ) kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( 
pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) return _VF . stft ( input , n_fft , hop_length , win_length , window , normalized , onesided ) del torch . unique_dim 
if norm_type == inf : norms = [ p . grad . detach ( ) . abs ( ) . max ( ) . to ( device ) for p in parameters ] total_norm = norms [ 0 ] if len ( norms ) == 1 else torch . max ( torch . stack ( norms ) ) 
paths . append ( p ) if not found_one : print ( "::warning title=s3 artifacts not found::" "Didn't find any test reports in s3, there might be a bug!" ) return paths 
Y = torch . rand ( N , 1 , device = device ) Predefined nu_alpha and nu_beta , nu_alpha . shape : ( 1 , 1 ) , nu_beta . shape : ( 1 , 1 ) nu_alpha = torch . rand ( 1 , 1 , device = device ) nu_beta = torch . rand ( 1 , 1 , device = device ) nu = dist . Gamma ( nu_alpha , nu_beta ) 
with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and preserve_rng_state and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backward = _backward 
tensor_type : OptionalCType = OptionalCType ( BaseCType ( tensorT ) ) if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( tensor_type ) ) 
tensor , scale , zero_point = unpacked_qtensors [ : 3 ] axis = unpacked_qtensors [ 3 ] if len ( unpacked_qtensors ) >= 4 else None axis_i = _get_const ( axis , "i" , "axis" ) input_scalar_type = tensor . type ( ) . scalarType ( ) assert input_scalar_type is not None input_qdtype = _type_utils . JitScalarType . from_name ( tensor . type ( ) . scalarType ( ) ) if qdtype is None : if input_qdtype is not None : 
def with_cache ( self , cache_size = 1 ) : assert self . _inv is not None return self . inv . with_cache ( cache_size ) . inv 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
def safe_unflatten ( tensor , dim , shape ) : if len ( shape ) == 0 : assert tensor . shape [ dim ] == 1 return tensor . squeeze ( dim ) return tensor . unflatten ( dim , shape ) def jacfwd ( f , argnums = 0 ) : 
if start_record is None and name == start_profile start_record = record elif cuda_start_event n name : assert record . device ( ) != -1 
start_daemon = rank == 0 assert result . hostname is not None store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) 
except ImportError : raise RuntimeError ( "Need networkx installed to perform smart recomputation heuristics" ) joint_module . graph . eliminate_dead_code ( ) joint_module . recompile ( ) fx_g = joint_module . graph 
torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] 
def gen_all_vmap_plumbing ( native_functions : Sequence [ NativeFunction ] ) -> str : body = "n" . join ( list ( mapMaybe ( ComputeBatchRulePlumbing ( ) , native_functions ) ) ) return f "" 
r = torch . Tensor . _make_subclass ( cls , elem , elem . requires_grad ) r . proxy = proxy if elem . is_sparse : proxy . node . meta [ nsor_meta = { } else : proxy . node . meta [ nsor_meta = _extract_tensor_metadata ( r ) return r 
assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : 
def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( "wb" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut 
rank = symbolic_helper . _get_tensor_rank ( x1 ) assert rank is not None broadcasted_x1 = symbolic_helper . _unsqueeze_helper ( g , x1 , [ rank - 1 ] ) broadcasted_x2 = symbolic_helper . _unsqueeze_helper ( g , x2 , [ rank - 2 ] ) return pairwise_distance ( 
def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride . default ] def handle_symbolic_op ( func , args , kwargs ) : 
min_val = self . min_val max_val = self . max_val prev_zeros = False if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : prev_zeros = ( min_val . item ( ) == 0 ) and ( max_val . item ( ) == 0 ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or prev_zeros : min_val = torch . min ( x ) max_val = torch . max ( x ) 
raise RuntimeError ( f "The codegen expects to be able to generate '{generated_fns_str}'." f " To do so, it expects a line: 'autogen: {generated_fns_str}'." f " Instead, it found 'autogen: {expected_generated_fns_str}'" ) def signature ( self ) -> "FunctionSchema" : 
auto maybe_layer = maybeCurrentDynamicLayer ( ) ; TORCH_INTERNAL_ASSERT ( maybe_layer . has_value ( ) ) ; int64_t cur_level = maybe_layer -> layerId ( ) ; { textwrap . indent ( bdims_all_none_case , "  " ) } { textwrap . indent ( unwraps , "  " ) } auto results = batch_rule ( { oin ( unwrapped_arg_list ) } ) ; { wrapped_returns } 
kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) sig_group = CppSignatureGroup . from_schema ( f . func , method = False ) 
torch . _C . _log_api_usage_once ( "quantization_api.quantize.prepare" ) if prepare_custom_config_dict is None : prepare_custom_config_dict = get_default_custom_config_dict ( ) custom_module_class_mapping = prepare_custom_config_dict . get ( "float_to_observed_custom_module_class" , { } ) if not inplace : 
cur_weight = _unsqueeze_to_dim ( weight , self . dim ( ) - 1 ) input_grad = torch . where ( self > 0 , grad_output , cur_weight * grad_output ) weight_grad_collector = torch . where ( self > 0 , grad_output . new_zeros ( ( ) ) , self * grad_output ) out = weight_grad_collector . sum_to_size ( cur_weight . shape ) 
assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) if isinstance ( embed_dim , torch . Tensor ) : head_dim = embed_dim . div ( num_heads , rounding_mode = unc else : head_dim = embed_dim assert head_dim * num_heads == embed_dim , "embed_dim must be divisible by num_heads" scaling = float ( head_dim ) * * -0 . 5 
self . register_parameter ( _proj_bias None ) self . out_proj = NonDynamicallyQuantizableLinear ( embed_dim , embed_dim , bias = bias , * * factory_kwargs ) if add_bias_kv : self . bias_k = Parameter ( torch . empty ( ( 1 , 1 , embed_dim ) , * * factory_kwargs ) ) 
self . scale = torch . tensor ( [ 1 . 0 ] ) self . zero_point = torch . tensor ( [ 0 ] ) self . dtype = self . activation_post_process . dtype self . qscheme = self . activation_post_process . qscheme 
self . quant_min : int = quant_min self . quant_max : int = quant_max self . register_buffer ( "fake_quant_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . register_buffer ( "observer_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . is_symmetric_quant = _is_symmetric_quant ( self . activation_post_process . qscheme ) 
def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_symint_fn ( ) : name += "_symint" if func . is_out_fn ( ) : 
self . tracker = tracker m = iparams [ 'm' ] n = iparams [ 'n' ] variable parameters self . X = X 
if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta and node . op == ll_function users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : 
for i in range ( len ( current_commits . commits ) ) : c = current_commits . commits [ i ] if categorized n str ( c ) : feature_item = get_commit_data_cache ( ) . get ( c . commit_hash ) features = features_to_dict ( feature_item ) category , topic = CommitList . categorize ( features ) current_commits [ i ] = dataclasses . replace ( c , category = category , topic = topic ) current_commits . write_result ( ) def get_hash_or_pr_url ( commit : Commit ) : 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) 
proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get 
self . module_groups = [ ] self . enable_mask_update = False self . activation_handles = [ ] self . bias_handles = [ ] self . model = model 
def replace_node_module ( node : fx . Node , modules : Dict [ str , Any ] , new_module : torch . nn . Module ) : assert ( isinstance ( node . target , str ) ) parent_name , name = _parent_name ( node . target ) 
def __init__ ( self , scale : float , zero_point : int , negative_slope : float = 1 e -2 , inplace : bool = False ) : super ( ) . __init__ ( negative_slope , inplace ) self . register_buffer ( ale torch . tensor ( scale ) ) self . register_buffer ( ro_point torch . tensor ( zero_point ) ) 
return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) 
def _functorch_str ( tensor , * , tensor_contents = None ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , M ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( svd_lowrank , tensor_ops , A , q = q , niter = niter , M = M ) return _svd_lowrank ( A , q = q , niter = niter , M = M ) 
if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta and node . op == ll_function users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : 
def op_name_from_native_function ( f : NativeFunction ) -> str : return f "{f.namespace}::{f.func.name}" 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
raise RuntimeError ( f "The codegen expects to be able to generate '{generated_fns_str}'." f " To do so, it expects a line: 'autogen: {generated_fns_str}'." f " Instead, it found 'autogen: {expected_generated_fns_str}'" ) def signature ( self ) -> "FunctionSchema" : 
"default" : default , "arg_name" : arg_name } return fn 
if do_write : with clean_ctx . open ( fout_path , 'w' , encoding = f -8 as fout : fout . write ( output_source ) return "ok" 
func_args = _wrap_all_tensors_to_functional ( args , func_level ) func_kwargs = _wrap_all_tensors_to_functional ( kwargs , func_level ) flattened_unwrapped_args , _ = tree_flatten ( args ) flattened_wrapped_args , _ = tree_flatten ( func_args ) flattened_unwrapped_kwargs , _ = tree_flatten ( kwargs ) flattened_wrapped_kwargs , _ = tree_flatten ( func_kwargs ) func_outputs = func ( * func_args , * * func_kwargs ) outputs = _unwrap_all_tensors_from_functional ( func_outputs ) 
for overloads and raise an exception if there are more than one . qualified_op_name = : : { } ormat ( self . name , op_name ) op = torch . _C . _jit_get_operation ( qualified_op_name ) 
if node . op == aceholder return True return not all ( is_fusible ( node , user ) for user in node . users ) def get_node_weight ( node ) : mem_sz = _size_of ( node . meta [ nsor_meta 
continue storage_key = storage_metadata [ MetadataIndex ( fqn , storage_md . offsets ) ] target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] 
DEFAULT = "default (compile if called from a exported function / forward)" COPY_TO_SCRIPT_WRAPPER = "if this method is not scripted, copy the python method onto the scripted model" def export ( fn ) : 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
out_and_self = list ( self . arguments . out ) + [ arg for arg in self . arguments . flat_positional if arg . name == "self" ] mutable_returns = [ ret for ret in self . returns if ret . annotation is not None and ret . annotation . is_write ] 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg , is_out = func . is_out_fn ( ) ) ] 
if GLOBALS . export_onnx_opset_version < 13 : raise errors . SymbolicValueError ( "Opset version must be >= 13 for Squeeze with dynamic axes." , input ) axes_t = axes_i [ 0 ] axes_rank = _get_tensor_rank ( axes_t ) assert axes_rank is not None if axes_rank > 1 : raise errors . SymbolicValueError ( "For Squeeze axses as input, the axes rank must be one in ONNX spec." , input ) 
def _dict_flatten ( d : Dict [ Any , Any ] ) -> Tuple [ List [ Any ] , Context ] : keys = sorted ( d . keys ( ) ) values = [ d [ key ] for key in keys ] return values , keys 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backward = _backward 
tensor , scale , zero_point = unpacked_qtensors [ : 3 ] axis = unpacked_qtensors [ 3 ] if len ( unpacked_qtensors ) >= 4 else None axis_i = _get_const ( axis , "i" , "axis" ) input_scalar_type = tensor . type ( ) . scalarType ( ) assert input_scalar_type is not None input_qdtype = _type_utils . JitScalarType . from_name ( tensor . type ( ) . scalarType ( ) ) if qdtype is None : if input_qdtype is not None : 
def _functorch_str ( tensor , * , tensor_contents = None ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) 
B_ = bB [ i ] if bB is not None else None X_ = torch . randn ( ( m , n ) , dtype = dtype , device = device ) if bX is None else bX [ i ] assert len ( X_ . shape ) == 2 and X_ . shape == ( m , n ) , ( X_ . shape , ( m , n ) ) 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
self . register_parameter ( _proj_bias None ) self . out_proj = NonDynamicallyQuantizableLinear ( embed_dim , embed_dim , bias = bias , * * factory_kwargs ) if add_bias_kv : self . bias_k = Parameter ( torch . empty ( ( 1 , 1 , embed_dim ) , * * factory_kwargs ) ) 
def _onnx_opset_unsupported_detailed ( op_name : str , current_opset : int , supported_opset : int , reason : str ) : raise RuntimeError ( f "Unsupported: ONNX export of {op_name} in " f "opset {current_opset}. {reason}. Please try opset version {supported_opset}." ) 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
fin_path = os . path . join ( output_directory , filepath ) with open ( fin_path , 'r' , encoding = f -8 as fin : output_source = fin . read ( ) fout_path = os . path . join ( output_directory , get_hip_file_path ( filepath ) ) 
powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , use_error_feedback = use_error_feedback , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) 
return mem_sz * 2 for node in full_bw_graph . nodes : if node in tangent_closure and node . op != tput nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) continue 
def with_cache ( self , cache_size = 1 ) : assert self . _inv is not None return self . inv . with_cache ( cache_size ) . inv 
res = kernel32 . LoadLibraryW ( dll ) if res is None : err = ctypes . WinError ( ctypes . get_last_error ( ) ) 
def get_post_build_suffix ( self ) -> str : if self . no_build_suffix : return "" if self . gpu_arch_type == "cuda" : return f "+cu{self.gpu_arch_version.replace('.', '')}" return f "+{self.gpu_arch_type}{self.gpu_arch_version}" 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
def gen_returns ( returns : Tuple [ Return , . . . ] , cur_level_var : str , results_var : str ) -> str : idx = 0 wrapped_returns = [ ] for ret in returns : 
E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) 
do_write = True if os . path . exists ( fout_path ) : with open ( fout_path , 'r' , encoding = f -8 as fout_old : do_write = fout_old . read ( ) != output_source 
func = func_overload . overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE : with proxy_mode . restore ( ) : return CURRENT_DECOMPOSITION_TABLE [ func_overload ] ( * args , * * kwargs ) 
e1 = symbols [ n . args [ 0 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e1 , n . args [ 1 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter 
if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) 
def op_name_from_native_function ( f : NativeFunction ) -> str : return f "{f.namespace}::{f.func.name}" 
eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group . get ( ximize False ) for p in group [ rams if p . grad is not None : 
count += 1 assert count >= prev_count , e number of converged eigenpairs as { } , got { } ) cannot decrease ormat ( prev_count , count ) 
if allow_list is None : allow_list = get_default_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) 
class PythonTensor ( object ) : def __init__ ( self , out , proxy ) : if isinstance ( out , torch . Tensor ) : self . value = torch . clone ( out ) else : self . value = torch . empty ( out ) self . proxy = proxy 
symbolic_helper . _set_opset_version ( opset_version ) symbolic_helper . _set_operator_export_type ( operator_export_type ) with exporter_context ( model , training , verbose ) : val_keep_init_as_ip = _decide_keep_init_as_input ( keep_initializers_as_inputs , operator_export_type , opset_version 
elif isinstance ( real_out , list ) : return list ( [ wrap_with_proxy ( e , proxy_out [ idx ] ) for idx , e in enumerate ( real_out ) ] ) elif isinstance ( real_out , torch . Tensor ) : return wrap_with_proxy ( real_out , proxy_out ) else : return real_out 
if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) 
def with_cache ( self , cache_size = 1 ) : assert self . _inv is not None return self . inv . with_cache ( cache_size ) . inv 
if self . dispatch_key not in f . dispatch : return None if f . manual_kernel_registration : return None op_name = f "aten::{f.func.name}" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : 
def __eq__ ( self , other ) : if isinstance ( other , _DataPipeType ) : return self . param == other . param return NotImplemented 
def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( "wb" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut 
do_write = True if os . path . exists ( fout_path ) : with open ( fout_path , 'r' , encoding = f -8 as fout_old : do_write = fout_old . read ( ) != output_source 
grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output grad_input = new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) register_decomposition ( aten . _log_softmax_backward_data ) 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward 
register_decomposition ( aten . tanh_backward ) def tanh_backward_decomposition ( out_grad : Tensor , y : Tensor ) : return out_grad * ( 1 - y * y ) register_decomposition ( aten . sigmoid_backward ) def sigmoid_backward_decomposition ( out_grad : Tensor , y : Tensor ) : 
out_and_self = list ( self . arguments . out ) + [ arg for arg in self . arguments . flat_positional if arg . name == "self" ] mutable_returns = [ ret for ret in self . returns if ret . annotation is not None and ret . annotation . is_write ] 
def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( 
if GLOBALS . export_onnx_opset_version < 13 : raise errors . SymbolicValueError ( "Opset version must be >= 13 for Squeeze with dynamic axes." , input ) axes_t = axes_i [ 0 ] axes_rank = _get_tensor_rank ( axes_t ) assert axes_rank is not None if axes_rank > 1 : raise errors . SymbolicValueError ( "For Squeeze axses as input, the axes rank must be one in ONNX spec." , input ) 
if mapping is None : mapping = get_default_qat_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
def get_output_device ( devices , op ) : The device propagation is a bit sketchy . aten::index ( CPU , CUDA ) = > CPU tensor aten::index ( CUDA , CPU ) = > CUDA tensor if op == aten . index : return devices [ 0 ] devices = list ( set ( devices ) ) if len ( devices ) == 1 : return devices [ 0 ] else : 
out_args : List [ Dict [ str , Any ] ] = [ ] for arg in f . func . arguments . flat_positional : if arg . default is not None : continue 
training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , steps = training_size , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) 
weight = g . op ( "Constant" , value_t = weight_value ) if bias is None or _is_none ( bias ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) bias_value = torch . tensor ( [ 0 . 0 ] * channel_size , 
paths . append ( p ) if not found_one : print ( "::warning title=s3 artifacts not found::" "Didn't find any test reports in s3, there might be a bug!" ) return paths 
if getattr ( ret . __class__ , "__setstate__" , Tensor . __setstate__ ) is not Tensor . __setstate__ : ret . __setstate__ ( state ) else : if isinstance ( state , tuple ) : 
ops = set ( [ i . target for i in joint_module . graph . nodes if i . op == ll_function print ( ops - recomputable_ops ) AGGRESSIVE_RECOMPUTATION = False for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) 
t_params = params for k in range ( 5 ) : t_f = net ( t_params , t_x ) t_loss = F . l1_loss ( t_f , t_y ) grads = torch . autograd . grad ( t_loss , t_params , create_graph = True ) 
torch . _C . _log_api_usage_once ( "quantization_api._numeric_suite_fx.extract_weights" ) if base_name_to_sets_of_related_ops is None : base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops ( ) type_a_related_to_b = get_type_a_related_to_b ( base_name_to_sets_of_related_ops ) 
count += 1 assert count >= prev_count , e number of converged eigenpairs as { } , got { } ) cannot decrease ormat ( prev_count , count ) 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
if len ( body ) != 1 or not ( is_pass ( body [ 0 ] ) or is_ellipsis ( body [ 0 ] ) ) : msg = "Only `pass` statement or `...` can be the body of overload declaration:n" msg += 'n' . join ( parsed_def . source . split ( "n" ) [ : 3 ] ) 
if node . op == ll_module if is_activation_post_process ( self . modules [ node . target ] ) : observer_module = self . modules [ node . target ] prev_node = node . args [ 0 ] if observer_module . dtype == torch . float16 : 
E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol 
Only support cases where all returns are Tensors or vector<Tensor> if len ( returns ) == 0 : return gen_vmap_plumbing_no_returns ( native_function ) if not all ( ret . type . is_tensor_like ( ) for ret in returns ) : return None if not accepts_at_least_one_tensor_input ( schema ) : 
if func == torch . ops . aten . is_contiguous . default : return True TODO : hack , we don currently support symbolic strides properly if func == torch . ops . aten . stride . default : return create_contiguous ( args [ 0 ] . shape ) TODO : An incomplete list 
if right is Any or left == right : return True if isinstance ( right , _GenericAlias ) : if getattr ( right , origin__ None ) is Generic : return True if right == type ( None ) : return False 
ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if v is None : continue if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue 
if mapping is None : mapping = get_default_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) 
def _div_aten ( a , b ) : is_integral = isinstance ( a , ( bool , int ) ) or ( isinstance ( a , torch . Tensor ) and utils . is_integer_dtype ( a . dtype ) ) if is_integral : return torch . div ( a , b , rounding_mode = "trunc" ) else : return torch . true_divide ( a , b ) div = _make_elementwise_binary_prim ( 
else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight , self . zero_bias ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
partition . logical_device_ids . append ( device . logical_id ) partition . add_node ( node ) partition_to_left_mem_bytes [ partition ] -= total_size_of_input_nodes 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension 
if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors , _ = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) 
def f ( * inps , out_tensors = None ) : inps = fx_model . graph . flatten_inps ( * inps ) if out_tensors is None : results = [ torch . empty ( shape , dtype = dtype ) for shape , dtype in outs [ 1 ] ] results = alloc_results else : results = out_tensors full_inps = module_stuff + list ( inps ) + results 
cur_weight = _unsqueeze_to_dim ( weight , self . dim ( ) - 1 ) input_grad = torch . where ( self > 0 , grad_output , cur_weight * grad_output ) weight_grad_collector = torch . where ( self > 0 , grad_output . new_zeros ( ( ) ) , self * grad_output ) out = weight_grad_collector . sum_to_size ( cur_weight . shape ) 
def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : 
if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . add_node ( node ) partition . used_mem_bytes += total_size_of_input_nodes 
if right is Any or left == right : return True if isinstance ( right , _GenericAlias ) : if getattr ( right , origin__ None ) is Generic : return True if right == type ( None ) : return False 
prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) 
auto maybe_layer = maybeCurrentDynamicLayer ( ) ; TORCH_INTERNAL_ASSERT ( maybe_layer . has_value ( ) ) ; int64_t cur_level = maybe_layer -> layerId ( ) ; { textwrap . indent ( bdims_all_none_case , "  " ) } { textwrap . indent ( unwraps , "  " ) } auto results = batch_rule ( { oin ( unwrapped_arg_list ) } ) ; { wrapped_returns } 
def gen_vmap_plumbing ( native_function : NativeFunction ) -> Optional [ str ] : schema = native_function . func sig = DispatcherSignature . from_schema ( schema ) returns = schema . returns 
def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , args , kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) 
Only support cases where all returns are Tensors or vector<Tensor> if len ( returns ) == 0 : return gen_vmap_plumbing_no_returns ( native_function ) if not all ( ret . type . is_tensor_like ( ) for ret in returns ) : return None if not accepts_at_least_one_tensor_input ( schema ) : 
def forward ( self , input ) : return self . activation_post_process ( self . _forward ( input ) ) 
def size ( g , self , dim = None ) : if dim is None : return g . op ( "Shape" , self ) return sym_help . _size_helper ( g , self , dim ) 
e1 = symbols [ n . args [ 0 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e1 , n . args [ 1 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
rank = symbolic_helper . _get_tensor_rank ( x1 ) assert rank is not None broadcasted_x1 = symbolic_helper . _unsqueeze_helper ( g , x1 , [ rank - 1 ] ) broadcasted_x2 = symbolic_helper . _unsqueeze_helper ( g , x2 , [ rank - 2 ] ) return pairwise_distance ( 
