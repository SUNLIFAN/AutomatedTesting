const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout && elapsed > timeout ) { TORCH_CHECK ( false , "Wait timeout" ) ; } 
auto attrVal = tryInsertConstant ( * graph , attr ) ; n -> output ( ) -> replaceAllUsesWith ( * attrVal ) ; nodesToDestroy . emplace ( n ) ; } } } 
auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) { 
xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
"schema" , [ ] ( Node & n ) { std::stringstream ss ; if ( n . maybeSchema ( ) ) { ss << n . schema ( ) ; } else { ss << "(no schema)" ; }
std::unordered_map < std::string , std::string > copied_metadata = owner_ -> metadata ( ) ; if ( owner_ -> metadata ( ) . find ( "model_name" ) == owner_ -> metadata ( ) . end ( ) ) { copied_metadata [ "model_name" ] = owner_ -> name ( ) ; } if ( observer ) { observer -> onEnterRunMethod ( copied_metadata , function_ -> name ( ) ) ; 
PyObject * c10d_init ( PyObject * _unused , PyObject * noargs ) { C10_LOG_API_USAGE_ONCE ( "c10d.python.import" ) ; auto c10d_module = THPObjectPtr ( PyImport_ImportModule ( "torch.distributed" ) ) ; }
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
inline Variable valueToTensor ( c10::TensorOptions options , PyObject * value , const at::Device & device ) { if ( THPVariable_Check ( value ) ) { return THPVariable_Unpack ( value ) ; } 
const Var * metaVar = gpu_thread_vars_ [ gpu_thread_index ] ; body = Substitute ( Stmt::clone ( body ) , { { v -> var ( ) , metaVar } } ) ; 
constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 || lda >= std::max<int64_t> ( 1 L , m ) ; } 
listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return listenPort ; } 
c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { throw std::runtime_error ( "ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0" ) ; } 
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout && elapsed > timeout ) { TORCH_CHECK ( false , "Wait timeout" ) ; } 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
const auto size = tuple PyTuple_GET_SIZE ( obj ) : PyList_GET_SIZE ( obj ) ; if ( size == 0 ) { return true ; } 
{ auto stream = c10::cuda::getCurrentCUDAStream ( device . index ( ) ) ; C10_CUDA_CHECK ( cudaStreamSynchronize ( stream ) ) ; event_ = nullptr ; event_sync_required_ = false ; } else 
char * other_data = data [ 1 ] ; for ( const auto i : c10::irange ( dim_size ) ) { ( void ) i ; if ( c10::load<scalar_t> ( self_data ) != c10::load<scalar_t> ( other_data ) ) { result = false ; return ; } 
const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; if ( resize_output ( result , { total_nonzero , ndim } ) ) { result . as_strided_ ( { total_nonzero , ndim } , { 1 , total_nonzero } ) ; } if ( result . numel ( ) == 0 ) { return result ; 
std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim && B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } 
if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data || ( advance_data_ptr == 0 ) ) ; data += advance_data_ptr ; } return list . release ( ) ; 
in0_t , dtype , layout , device , pin_memory , memory_format ) ; } auto & out_t = p_node -> Output ( 0 ) . toTensor ( ) ; at::native::resize_ ( out_t , in0_t . sizes ( ) , c10::nullopt ) ; at::native::fill_out ( out_t , in1_s ) ; } ; } ) ; 
std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; for ( auto & da : device_allocator ) { auto snap = da -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } 
shdrList_ = ( Elf64_Shdr * ) ( fileData + ehdr_ -> e_shoff ) ; auto strtabSecNo = ehdr_ -> e_shstrndx ; MULTIPY_CHECK ( strtabSecNo >= 0 && strtabSecNo < numSections_ , "e_shstrndx out of range" ) ; 
TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( "Unexpected number of bound info entries in cacheAccesses in the fuser." ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad || info . kind == kMutate ; bool hasWrites = info . kind == kStore || info . kind == kMutate ; 
payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , "Missing the payload section" ) ; break ; } } 
std::vector < py::obj<Dim> > dims ; size_t size = s . size ( ) ; dims . reserve ( size ) ; for ( size_t i = 0 ; i < size ; ++ i ) { auto r = s [ i ] ; 
try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } 
llvm::BasicBlock * bb_ ; llvm::Value * value_ { nullptr } ; llvm::JITTargetAddress kernelAddress_ ; 
while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , "waiting for processes to " "connect has timed out" ) ; } else if ( res == -1 ) { 
if ( v . isTensor ( ) ) { v = IValue ( detach ( std::move ( v ) . toTensor ( ) ) ) ; } else if ( v . isTensorList ( ) ) { std::vector<at::Tensor> lst = v . toTensorVector ( ) ; for ( auto & tensor : lst ) { tensor = detach ( tensor ) ; } v = std::move ( lst ) ; } 
auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( false ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
Tensor atan2 ( const Tensor & self , const Tensor & other ) { Tensor result ; auto iter = TensorIterator::binary_float_op ( result , self , other ) ; atan2_stub ( iter . device_type ( ) , iter ) ; return iter . output ( ) ; } 
ss << "Wrapper[" ; if ( wrapped -> level ( ) . has_value ( ) ) { ss << "lvl=" << wrapped -> level ( ) . value ( ) << ", " ; } else { ss << "dead, " ; } 
work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { TORCH_CHECK ( false , "Stop_waiting response is expected" ) ; } } 
static inline Tensor unary_op_impl_with_complex_to_float ( const Tensor & self , OutImpl & out_impl ) { if ( self . is_complex ( ) ) { const auto float_type = c10::toRealValueType ( self . scalar_type ( ) ) ; Tensor result = at::empty_like ( self , self . options ( ) . dtype ( float_type ) ) ; return out_impl ( result , self ) ; } 
if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ ++ ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ ++ ; } else { add_next_edge ( autograd::Edge { } ) ; index_ ++ ; } } 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
auto iter = TensorIteratorConfig ( ) . add_output ( self ) . add_input ( src ) . resize_outputs ( false ) 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , self . options ( ) . dtype ( c10::kQUInt8 ) , scale , zero_point } ; 
auto bdims = batched -> bdims ( ) ; return bdims . back ( ) . level ( ) >= level ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { 
Tensor unbind_backward ( const variable_list & grads , int64_t dim ) { IntArrayRef sizes ; at::TensorOptions o ; for ( const auto & v : grads ) { if ( v . defined ( ) ) { sizes = v . sizes ( ) ; o = static_cast<Tensor> ( v ) . options ( ) ; 
T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; 
std::tuple < Tensor , Tensor > _symeig_helper_cpu ( const Tensor & self , bool eigenvectors , bool upper ) { auto infos = at::zeros ( { batchCount ( self ) } , self . options ( ) . dtype ( kInt ) ) ; auto self_sizes = self . sizes ( ) . vec ( ) ; self_sizes . pop_back ( ) ; 
dims = { arguments [ dim_arg_pos ] . toInt ( ) } ; } else if ( arguments [ dim_arg_pos ] . isNone ( ) ) { reduction_case = ReductionCase::DimArray ; if ( logical_dim == 0 ) { dims = { 0 } ; } else { auto all_dims = range ( 0 , self . dim ( ) - 1 ) ; dims = std::vector<int64_t> ( all_dims . begin ( ) , all_dims . end ( ) ) ; } } else { TORCH_INTERNAL_ASSERT ( false , "Unexpected dtype found at dims" ) ; 
int64_t output_zero_point ) { return apply_impl<true> ( input , output_scale , output_zero_point ) ; } 
void PyDefaultSavedVariableHooks::reset_hooks ( ) { std::lock_guard<std::mutex> lock ( mutex_ ) ; if ( Py_IsInitialized ( ) ) { py::gil_scoped_acquire gil ; Py_XDECREF ( pack_hook_ ) ; 
std::map < ParallelType , bool > ParallelTypeBitmap::getMap ( ) const { std::map < ParallelType , bool > map ; for ( const auto & pt_offset : pt_to_offset_ ) { map . emplace ( pt_offset . first , bitset_ [ pt_offset . second ] ) ; } return map ; } 
unsigned previous = y - 1 ; for ( size_t x = 1 ; x <= n ; ++ x ) { const auto old_row = row [ x ] ; row [ x ] = std::min ( previous + ( word1 [ y - 1 ] == word2 [ x - 1 ] 0 u : 1 u ) , std::min ( row [ x - 1 ] , row [ x ] ) + 1 ) ; 
if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( 
"schema" , [ ] ( Node & n ) { std::stringstream ss ; if ( n . maybeSchema ( ) ) { ss << n . schema ( ) ; } else { ss << "(no schema)" ; }
VaryingShape<size_t> stride_indices ; VaryingShape<int64_t> strides ; VaryingShape<int64_t> sizes ; if ( t . layout ( ) == at::kStrided && . is_nested ( ) ) { sizes = VaryingShape<int64_t> { t . sizes ( ) . vec ( ) } ; strides = VaryingShape<int64_t> { t . strides ( ) . vec ( ) } ; } 
 alias_info = std::move ( container ) ; } else if ( L . nextIf ( '?' ) ) { fake_value = c10::OptionalType::get ( fake_value ) ; real_value = c10::OptionalType::get ( real_value ) ; } else { break ; } 
int * ipiv , c10::complex<double> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnZgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuDoubleComplex * > ( dA ) , 
static inline void assertValidDevice ( int device ) { const auto device_num = caching_allocator . device_allocator . size ( ) ; TORCH_CHECK ( 0 <= device && device < device_num , "Invalid device argument." ) ; } 
int * ipiv , c10::complex<float> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnCgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuComplex * > ( dA ) , 
case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = "compressAllBuffers(l.root_stmt());n" ; randomization_helper::printHistory ( n_transform , message ) ; 
{ AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , "reflection_pad3d_cpu" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( 
 void print_init_message ( const char * message ) { write ( 1 , message , strlen ( message ) ) ; write ( 1 , "n" , 1 ) ; } bool object_exists ( const char * name ) { 
bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( "Not able to create/get the NCCL Communicator since " "the GPU devices are not known" ) ; } 
std::vector < py::obj<Dim> > dims ; size_t size = s . size ( ) ; dims . reserve ( size ) ; for ( size_t i = 0 ; i < size ; ++ i ) { auto r = s [ i ] ; 
auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { throw std::runtime_error ( "Stop_waiting response is expected" ) ; } } 
( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) && options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , 
KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( "KernelScope() must be constructed before calling this" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } 
line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) && str [ line_end ] != 'n' ) { + line_end ; } 
 alias_info = std::move ( container ) ; } else if ( L . nextIf ( '?' ) ) { fake_value = c10::OptionalType::get ( fake_value ) ; real_value = c10::OptionalType::get ( real_value ) ; } else { break ; } 
List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( src -> exception_ptr ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } 
ParsedArgs < 5 > parsed_args ; auto r = parser . parse ( args , kwargs , parsed_args ) ; if ( r . has_torch_function ( ) ) { return handle_torch_function ( r , args , kwargs , THPNNVariableFunctionsModule , "torch.nn" , "_parse_to" ) ; } auto parsed = parse_to_conversion ( r , false ) ; auto & device = std::get < 0 > ( parsed ) ; 
s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; AT_CUDA_CHECK ( cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } 
if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << "Got exception: " << errMsg ; TORCH_CHECK ( false , errMsg ) ; } } 
hash_key_t static_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
struct BucketKey { BucketKey ( c10::ScalarType type , c10::Device device ) : type ( type ) , device ( device ) { } const c10::ScalarType type ; const c10::Device device ; 
const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } 
std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; 
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce_coalesced ( std::vector<at::Tensor> & tensors , const AllreduceCoalescedOptions & opts ) { throw std::runtime_error ( "allreduce_coalesced is currently not supported with NCCL" ) ; } 
payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , "Missing the payload section" ) ; break ; } } 
if ( min && max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { maximum_stub ( device_type ( ) , * this ) ; } else if ( max ) { minimum_stub ( device_type ( ) , * this ) ; } } 
at::assert_no_internal_overlap ( result ) ; const Tensor * pnotSkippedTensor = [ ] ( const TensorList & tensors ) -> const Tensor * { for ( auto const & tensor : tensors ) { if ( should_skip ( tensor ) ) { continue ; 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , 
struct THCCachingHostAllocator final : public at::Allocator { at::DataPtr allocate ( size_t size ) const override { void * ptr ; THCudaCheck ( allocator . malloc ( & ptr , size ) ) ; return { ptr , ptr , & THCCachingHostDeleter , at::DeviceType::CPU } ; 
work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << "[INFO] ignore extra op schema str: " << * schemaStr << " in: " << _demangle ( _name ( src -> getFunction ( ) ) ) << ", because already found valid op schema str: " << * visitedOps -> begin ( ) << std::endl ; } 
void set_data ( Tensor & self , const Tensor & new_data ) { AT_ERROR ( "set_data is not implemented for Tensor" ) ; } 
vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; 
def __sizeof__ ( self ) : return ( super ( _StorageBase , self ) . __sizeof__ ( ) + self . element_size ( ) * self . size ( ) ) 
d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) 
for i in range ( 2 ) : assert current_frame is not None Optional [ FrameType ] current_frame = current_frame . f_back 
rank = symbolic_helper . _get_tensor_rank ( x1 ) assert rank is not None broadcasted_x1 = symbolic_helper . _unsqueeze_helper ( g , x1 , [ rank - 1 ] ) broadcasted_x2 = symbolic_helper . _unsqueeze_helper ( g , x2 , [ rank - 2 ] ) return pairwise_distance ( 
def forward ( self , * args , * * kwargs ) : return compiled_f ( dict ( self . orig_module . named_parameters ( ) ) , dict ( self . orig_module . named_buffers ( ) ) , * args , * * kwargs ) 
else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg , is_out = func . is_out_fn ( ) ) ] 
proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get 
with preserve_rng_state ( ) : Set input tensors that require grad to leaves flat_tensor_args = pytree . tree_map ( lambda x : x . detach ( ) . requires_grad_ ( x . requires_grad ) if isinstance ( x , Tensor ) else x , flat_tensor_args ) with torch . set_grad_enabled ( grad_state ) : out = flat_fn ( * flat_tensor_args ) 
paths . append ( p ) if not found_one : raise RuntimeError ( "Didn't find any test reports in s3, there is probably a bug!" ) return paths 
def forward ( self , input ) : return self . activation_post_process ( self . _forward ( input ) ) 
def op_name_from_native_function ( f : NativeFunction ) -> str : return f "{f.namespace}::{f.func.name}" 
if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) 
e1 = symbols [ n . args [ 0 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e1 , n . args [ 1 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter 
if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) 
composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict 
return sum / n register_decomposition ( aten . std . correction ) def std_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : return torch . sqrt ( torch . var ( x , dims , correction = correction , keepdim = keepdim ) ) 
buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) , False ) buf_value = buf . getvalue ( ) 
def hardtanh ( g , self , min_val , max_val ) : dtype = self . type ( ) . scalarType ( ) if dtype is None : dtype = 6 else : dtype = sym_help . scalar_type_to_onnx . index ( sym_help . cast_pytorch_to_onnx [ dtype ] ) 
pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) return _VF . stft ( input , n_fft , hop_length , win_length , window , normalized , onesided ) del torch . unique_dim 
E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol 
t_params = params for k in range ( 5 ) : t_f = net ( t_params , t_x ) t_loss = F . l1_loss ( t_f , t_y ) grads = torch . autograd . grad ( t_loss , t_params , create_graph = True ) 
def draw_joint_graph ( graph , joint_inputs , file_name = "full_graph.png" ) : draw_graph ( graph , file_name ) return default_partition ( graph , joint_inputs ) def create_compiled_function ( flat_fn , fw_compiler , bw_compiler , partition_fn ) : joint_forward_backward = create_joint_forward_backward ( flat_fn ) 
return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) 
index = self . _next_index ( ) may raise StopIteration data = self . _dataset_fetcher . fetch ( index ) may raise StopIteration if self . _pin_memory : data = _utils . pin_memory . pin_memory ( data , self . _pin_memory_device ) return data 
if prop . fset : setattr ( prop . fset , "_torchscript_modifier" , FunctionModifiers . UNUSED ) noqa : B010 return prop 
else : d_p = buf param . add_ ( d_p , alpha = - lr ) def _multi_tensor_sgd ( params : List [ Tensor ] , 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors , _ = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) 
def main ( args : List [ str ] ) -> None : parser = argparse . ArgumentParser ( description = "Generate unboxing source files" ) 
symbolic_helper . _set_opset_version ( opset_version ) symbolic_helper . _set_operator_export_type ( operator_export_type ) with exporter_context ( model , training , verbose ) : val_keep_init_as_ip = _decide_keep_init_as_input ( keep_initializers_as_inputs , operator_export_type , opset_version 
else : num_outs = 1 joint_inputs = ( flat_args , out ) with torch . enable_grad ( ) : fx_g = make_fx ( joint_forward_backward ) ( * joint_inputs ) fw_module , bw_module = partition_fn ( fx_g , joint_inputs ) 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
def _onnx_opset_unsupported_detailed ( op_name : str , current_opset : int , supported_opset : int , reason : str ) : raise RuntimeError ( f "Unsupported: ONNX export of {op_name} in " f "opset {current_opset}. {reason}. Please try opset version {supported_opset}." ) 
fns = [ getattr ( cls , name ) for name in cls . __dict__ if inspect . isroutine ( getattr ( cls , name ) ) ] captures = { } for fn in fns : 
arranged_img_CHW = make_grid ( make_np ( label_img ) , ncols = nrow ) arranged_augment_square_HWC = np . zeros ( ( arranged_img_CHW . shape [ 2 ] , arranged_img_CHW . shape [ 2 ] , 3 ) ) arranged_img_HWC = arranged_img_CHW . transpose ( 1 , 2 , 0 ) 
self . quant_min : int = quant_min self . quant_max : int = quant_max self . register_buffer ( "fake_quant_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . register_buffer ( "observer_enabled" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . is_symmetric_quant = _is_symmetric_quant ( self . activation_post_process . qscheme ) 
model = copy . deepcopy ( model ) model . eval ( ) prepare ( model , inplace = True ) run_fn ( model , * run_args ) convert ( model , mapping , inplace = True ) return model 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) 
fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) 
if node . op == ll_module if is_activation_post_process ( self . modules [ node . target ] ) : observer_module = self . modules [ node . target ] prev_node = node . args [ 0 ] if observer_module . dtype == torch . float16 : 
for overloads and raise an exception if there are more than one . qualified_op_name = : : { } ormat ( self . name , op_name ) op = torch . _C . _jit_get_operation ( qualified_op_name ) 
def log_abs_det_jacobian ( self , x , y ) : shape = x . shape scale = self . scale if isinstance ( scale , numbers . Real ) : result = torch . full_like ( x , math . log ( abs ( scale ) ) ) else : result = torch . abs ( scale ) . log ( ) 
def forward ( self , input ) : return F . relu ( F . linear ( input , self . weight_fake_quant ( self . weight ) , self . bias ) ) 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , steps = training_size , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) 
def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( "wb" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut 
step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad / batch_size del param . grad_sample 
def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None 
else : d_bias = grad_out else : d_bias = None return ( d_input , d_weight , d_bias ) 
if found_tensor : n . meta [ nsor_meta = meta n . meta [ pe = type ( result ) return result 
def __eq__ ( self , other ) : if isinstance ( other , _DataPipeType ) : return self . param == other . param return NotImplemented 
def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : 
self . _module_copies [ 0 ] = self . module for module_copy in self . _module_copies [ 1 : ] : for param , copy_param in zip ( self . module . parameters ( ) , parameters ( module_copy ) ) : copy_param . requires_grad = param . requires_grad 
for node in graph . nodes : if node . op == ll_function cnt [ node . target . __name__ ] += 1 print ( sorted ( cnt . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) ) def min_cut_rematerialization_partition ( 
if prop . fset : setattr ( prop . fset , "_torchscript_modifier" , FunctionModifiers . UNUSED ) noqa : B010 return prop 
if weight is None or _is_none ( weight ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , 
