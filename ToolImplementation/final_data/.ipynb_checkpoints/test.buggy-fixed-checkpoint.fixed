if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
const VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
if ( FLAGS_caffe2_log_level < google::GLOG_WARNING ) { FLAGS_logtostderr = 1 ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
void retain_grad ( Tensor & self ) { AT_ERROR ( "retain_grad is not implemented for Tensor" ) ; } 
if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { body_new = Substitute ( body , { { var_new , start_new } } ) ; body_new = body_new -> accept_mutator ( this ) ; return body_new ; } } 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) && ten . has_storage ( ) && en . is_nested ( ) ; } 
if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return at::sum ( new_values , 0 , false , new_values . scalar_type ( ) ) ; } } else { auto dimIndices = ( arange ( 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std >= 0 . 0 , "normal_ expects std >= 0.0, but found std=" , std ) ; return self ; } 
CudaIPCSentData::CudaIPCSentData ( const std::string & handle , int64_t offset , int64_t * counter_ptr , at::Device device ) 
scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos_data [ i ] = info ; if ( info != 0 ) { return ; } 
annotations_ . emplace_back ( "Python id" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( "Python parent id" , y_metadata_ . empty ( ) std::to_string ( py_metadata_ . at ( 0 ) . id_ ) : "null" ) ; annotations_ . emplace_back ( "Python thread" , std::to_string ( t . python_tid_ ) ) ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , "no support for _allgather_base in Gloo process group" ) ; } 
variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , holder . get ( ) ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; 
try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { TORCH_CHECK ( false , "RRef should contain a tensor for .backward()" ) ; } } 
const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( info . is_meta ( ) ) { return ; } if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { 
if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
. sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) . debugHandle ( ctx -> debug_handle ) ; if ( ctx -> shapes && tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } 
if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) && concrete_type -> isSubtypeOf ( DeviceObjType::get ( ) ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } 
c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; C10_CUDA_CHECK ( cudaMemGetInfo ( & device_free , & device_total ) ) ; return { device_free , device_total } ; } ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] > sizes [ b ] ) { return 1 ; } } 
while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { throw std::runtime_error ( "waiting for processes to " "connect has timed out" ) ; } else if ( res == -1 ) { 
if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; auto one = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 1 . 0 ) ) ; auto zero = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 0 . 0 ) ) ; ExprHandle y = one / ( one + exp ( zero - ExprHandle ( x ) ) ) ; return y . node ( ) ; } 
PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) ( void * ) DimList_subscript , 0 , } ; 
for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( || isCallSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; 
if ( beta == 0 ) { return at::native::l1_loss ( input , target , reduction ) ; } Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; 
std::vector<at::Tensor> & , int , int ) { TORCH_CHECK ( false , "ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0" ) ; } 
case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; weight = weight . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( 
if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } auto * batched = maybeGetBatchedImpl ( tensor ) ; if ( batched ) { auto requested_level = bdims . back ( ) . level ( ) ; auto batched_level = batched -> bdims ( ) . back ( ) . level ( ) ; TORCH_INTERNAL_ASSERT ( requested_level > batched_level ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } 
VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( const uint32_t binding , const VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } 
if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return true_new Stmt::clone ( true_new ) : nullptr ; } else { return false_new Stmt::clone ( false_new ) : nullptr ; } } 
"tried to send() a message of type " , requestMessage -> type ( ) , " but RPC is no longer running on this node." ) ; TORCH_CHECK ( false , err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; 
static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , const torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; 
std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , 
collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , "No device(s) specified" ) ; } 
if ( optimized_plan_ ) { GRAPH_DUMP ( "plan already optimized:" , graph ) ; return * optimized_plan_ ; } 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , "unsupported protocol" ) ; } return address ; } 
. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; return c10::str ( type_ -> toString ( ) , ordinal_ ) ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { 
VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) , buildErrorMessage ( "Expected to find '" + var -> name_hint ( ) + "' in intermediate vars in mem dep checker in the fuser." ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; 
for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; ++ i ) { types . push_back ( n -> output ( i ) -> type ( ) ) ; } 
if ( check_has_torch_function ( self ) ) { return handle_torch_function_getter ( ( THPVariable * ) self , "names" ) ; } size_t size = ( ( THPVariable * ) self ) -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = ( ( THPVariable * ) self ) -> cdata . names ( ) ; 
PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require || new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; 
auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) , buildErrorMessage ( "Expected ranks to match in compressBuffer in the fuser." ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; ++ i ) { if ( dims [ i ] ) { 
auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j -- ) { const auto v = input_ . index ( { "..." , Slice ( ) , j } ) ; const auto & v1 = v ; const auto & v2 = v ; auto tau_unsqueezed = tau . index ( { "..." , j } ) . unsqueeze ( -1 ) ; 
if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } cuda_ipc_global_entities . sync_events_used_ -- ; } } catch ( . . . ) { 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = json_str_escape ( c10::toString ( op_schema . value ( ) ) ) ; } writeJsonNode ( 
watchHandler ( socket ) ; } else { TORCH_CHECK ( false , "Unexpected query type" ) ; } } 
if ( * largest == 0 ) { size_t tmp_bytes ; C10_CUDA_CHECK ( cudaMemGetInfo ( largest , & tmp_bytes ) ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; 
int numRanks , rank ; if ( sP2POp ( opType ) ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } 
Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; auto result = _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; return result ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { 
def icdf ( self , value ) : return torch . tan ( math . pi * ( value - 0 . 5 ) ) * self . scale + self . loc 
logging . debug ( "partitioner_cache hit!" ) fused_graph_module = self . partitioner_cache [ graph_module ] else : partitioner = CapabilityBasedPartitioner ( graph_module , self . supported_ops , allows_single_node_partition = False ) fused_graph_module = partitioner . partition_and_fuse ( ) self . partitioner_cache [ graph_module ] = fused_graph_module 
with torch . enable_grad ( ) : fx_g = make_fx ( vjpfull ) ( fn , args , ( torch . ones_like ( out ) , ) ) fw_module , bw_module = partition_backwards ( fx_g ) print ( fw_module . code , bw_module . code ) garbage_hack = torch . randn ( ( ) ) fw_args = ( garbage_hack , ) + args 
then we don allow recomputation . if nsor_meta ot in node . meta : return False input_tensors_size = sum ( _maybe_size_of ( i ) for i in node . args if isinstance ( i , fx . Node ) ) output_size = _size_of ( node . meta [ nsor_meta return ( output_size * 4 < input_tensors_size ) 
hardshrink ( x ) = x if x > lambd = x if x < - lambd = 0 otherwise return refs . where ( refs . logical_and ( a >= - lambd , a <= lambd ) , 0 , a ) register_decomposition ( torch . ops . aten . softshrink ) 
modules : Dict [ str , types . ModuleType ] 
report ( "nCopying extension {}" . format ( ext . name ) ) relative_site_packages = sysconfig . get_path ( relib replace ( sysconfig . get_path ( ta . lstrip ( os . path . sep ) src = os . path . join ( "torch" , relative_site_packages , filename ) 
def unwrap_tensors ( x ) : if isinstance ( x , torch . Tensor ) : return _unwrap_for_grad ( x , level ) raise AssertionError ( ) return tree_map ( unwrap_tensors , inps ) 
data_value = data_buf . getvalue ( ) return data_value , serialized_storages , serialized_dtypes , importer . zip_reader if importer else None def _load_storages ( id , zip_reader , obj_bytes , serialized_storages ) : 
def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) else : if node . op != ll_function 
self . freeze_bn = freeze_bn if self . training else True self . bn = nn . BatchNorm2d ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) 
def bench ( func ) : begin = time . time ( ) for _ in range ( iters ) : out = func ( a ) . sin ( ) out . sum ( ) . backward ( ) a . grad = None print ( time . time ( ) - begin ) def bench_jax ( ) : 
value = [ Summary . Value ( tag = name , tensor = tensor_proto , metadata = smd , ) ] 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( 
( * A . shape [ : -1 ] , A . size ( -1 ) - D . size ( -1 ) ) , dtype = A . dtype , device = A . device , generator = gen ) ) U_ortho_t = U_ortho . transpose ( -2 , -1 ) . contiguous ( ) 
mem_sz = _size_of ( node . meta [ nsor_meta Heuristic to bias towards nodes closer to the backwards pass mem_sz = int ( mem_sz + node . dist_from_bw ) if is_materialized ( node ) : return mem_sz 
getattr ( x , "write_" + ext . lstrip ( "." ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def default_partition ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None 
torch . _C . _log_api_usage_once ( "quantization_api.quantize.quantize" ) if mapping is None : mapping = get_default_static_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) model . eval ( ) 
for dll in dlls : is_loaded = False if with_load_library_flags : res = kernel32 . LoadLibraryExW ( dll , None , 0x00001100 ) last_error = ctypes . get_last_error ( ) if res is None and last_error != 126 : err = ctypes . WinError ( last_error ) 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) 
q = fut . value ( ) [ 0 ] . div_ ( world_size ) torch . matmul ( p , q . t ( ) , out = matrix ) if state . use_error_feedback : state . error_dict [ input_tensor ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] 
for node in self . quantized_graph . nodes : if node . op == ll_module nd is_activation_post_process ( self . modules [ node . target ] ) : env [ node . name ] = env [ node . args [ 0 ] . name ] 
if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) 
padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , padding , stride , dilation ) ) input = unfold_func ( ) 
test_x = torch . arange ( -2 * math . pi , 2 * math . pi , step = 0 . 01 ) . unsqueeze ( 1 ) test_y = t_A * torch . sin ( test_x + t_b ) test_f = net ( t_params , test_x ) plt . plot ( test_x . data . numpy ( ) , test_y . data . numpy ( ) , label = n ( x ) plt . plot ( test_x . data . numpy ( ) , test_f . data . numpy ( ) , label = t ( x ) 
e2 = symbols [ n . args [ 1 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e2 , n . args [ 0 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter 
args = list ( node . args ) args [ 1 ] = [ 1 ] node . args = tuple ( args ) for node in fx_g . graph . nodes : new_kwargs = { } 
if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] 
def _handle_reduce_dim_none ( g , self , op_name ) : rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if len ( batch_sizes ) == 0 : raise ValueError ( ap : Expected at least one Tensor to vmap over if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped 
if len ( quant_uses ) == 1 : quantized . graph . erase_node ( node ) for arg in quant_args [ 1 : ] : if isinstance ( arg , Node ) : quantized . graph . erase_node ( arg ) return quantized 
) ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) 
guard_field = 0 : : cuda::OptionalCUDAGuard guard_ ; elif self . dispatch_key == DispatchKey . CompositeExplicitAutograd : 
_check_p2p_op_list ( p2p_op_list ) group = p2p_op_list [ 0 ] . group reqs = [ ] with _coalescing_manager ( group , reqs ) : for p2p_op in p2p_op_list : op = p2p_op . op tensor = p2p_op . tensor 
isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor and type ( x ) is not torch . nn . Parameter ) tree_map ( check_non_fake_tensor , args ) 
for name in curr_modules . keys ( ) : difference = curr_modules [ name ] . weight . sub ( prev_modules [ name ] . weight ) summed_norms += torch . norm ( difference ) return bool ( summed_norms < threshold ) 
paths . append ( p ) if not found_one : print ( "::warning title=s3 artifacts not found::" "Didn't find any test reports in s3, there might be a bug!" ) return paths 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 10 , sharded_keys = { "unboxed_ops" } , ) 
def inner ( * args , * * kwargs ) : a0 , a1 , * a_other = args if ( isinstance ( a0 , tuple ) and isinstance ( a1 , tuple ) ) or ( isinstance ( a0 , list ) and isinstance ( a1 , list ) ) : results = [ ] for el0 , el1 in zip ( a0 , a1 ) : new_args = ( el0 , el1 , * a_other ) 
z_cumprod = ( 1 - z ) . cumprod ( -1 ) y = pad ( z , [ 0 , 1 ] , value = 1 ) * pad ( z_cumprod , [ 1 , 0 ] , value = 1 ) return y 
self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) 
AGGRESSIVE_RECOMPUTATION = False def _maybe_size_of ( node ) : if nsor_meta n node . meta : return _size_of ( node . meta [ nsor_meta return 0 def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) 
def sign ( self ) : if isinstance ( self . scale , numbers . Real ) : return 1 if float ( self . scale ) > 0 else -1 if float ( self . scale ) < 0 else 0 return self . scale . sign ( ) 
else : observed . linear_Q . weight = nn . Parameter ( other . q_proj_weight ) observed . linear_K . weight = nn . Parameter ( other . k_proj_weight ) observed . linear_V . weight = nn . Parameter ( other . v_proj_weight ) if other . in_proj_bias is None : 
