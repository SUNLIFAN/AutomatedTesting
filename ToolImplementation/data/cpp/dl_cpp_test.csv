BUGGY_CODE,FIXED_CODE
if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; ,if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
const VImage * VulkanTensor::image ( c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } ,const VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } 
"work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } ","for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } "
if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } ,if ( FLAGS_caffe2_log_level < google::GLOG_WARNING ) { FLAGS_logtostderr = 1 ; } 
"work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"void retain_grad ( const Tensor & self ) { AT_ERROR ( ""retain_grad is not implemented for Tensor"" ) ; } ","void retain_grad ( Tensor & self ) { AT_ERROR ( ""retain_grad is not implemented for Tensor"" ) ; } "
"if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { return Substitute ( body_new , { { var_new , start_new } } ) ; } ","if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { body_new = Substitute ( body , { { var_new , start_new } } ) ; body_new = body_new -> accept_mutator ( this ) ; return body_new ; } } "
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } ,if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } 
bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) && ten . has_storage ( ) ; } ,bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) && ten . has_storage ( ) && en . is_nested ( ) ; } 
if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return new_values . sum ( 0 ) ; } } else { auto dimIndices = ( arange ( ,"if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return at::sum ( new_values , 0 , false , new_values . scalar_type ( ) ) ; } } else { auto dimIndices = ( arange ( "
"bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } ","bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } "
"Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std > 0 . 0 , ""normal_ expects std > 0.0, but found std="" , std ) ; return self ; } ","Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std >= 0 . 0 , ""normal_ expects std >= 0.0, but found std="" , std ) ; return self ; } "
"CudaIPCSentData::CudaIPCSentData ( std::string handle , int64_t offset , int64_t * counter_ptr , at::Device device ) ","CudaIPCSentData::CudaIPCSentData ( const std::string & handle , int64_t offset , int64_t * counter_ptr , at::Device device ) "
"scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos [ i ] = info ; if ( info != 0 ) { return ; } ","scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos_data [ i ] = info ; if ( info != 0 ) { return ; } "
"annotations_ . emplace_back ( ""Python id"" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( ""Python parent id"" , y_metadata_ . empty ( ) py_metadata_ . at ( 0 ) . name_ : ""null"" ) ; annotations_ . emplace_back ( ""Python thread"" , std::to_string ( t . python_tid_ ) ) ; } ","annotations_ . emplace_back ( ""Python id"" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( ""Python parent id"" , y_metadata_ . empty ( ) std::to_string ( py_metadata_ . at ( 0 ) . id_ ) : ""null"" ) ; annotations_ . emplace_back ( ""Python thread"" , std::to_string ( t . python_tid_ ) ) ; } "
"work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"at::Tensor & , at::Tensor & , const AllgatherOptions & ) { throw std::runtime_error ( ""no support for _allgather_base in Gloo process group"" ) ; } ","at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , ""no support for _allgather_base in Gloo process group"" ) ; } "
"variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , index ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; ","variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , holder . get ( ) ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; "
"try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { throw std::runtime_error ( ""RRef should contain a tensor for .backward()"" ) ; } } ","try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { TORCH_CHECK ( false , ""RRef should contain a tensor for .backward()"" ) ; } } "
"const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { ","const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( info . is_meta ( ) ) { return ; } if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { "
if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; ,if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
. sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) ; if ( ctx -> shapes && tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } ,. sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) . debugHandle ( ctx -> debug_handle ) ; if ( ctx -> shapes && tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } 
"if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) && DeviceObjType::get ( ) -> isSubtypeOf ( concrete_type ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } ","if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) && concrete_type -> isSubtypeOf ( DeviceObjType::get ( ) ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } "
"c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } ","c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; C10_CUDA_CHECK ( cudaMemGetInfo ( & device_free , & device_total ) ) ; return { device_free , device_total } ; } ) ; } "
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } ,const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] < sizes [ b ] || a > b ) { return 1 ; } } ,} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] > sizes [ b ] ) { return 1 ; } } 
"while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { ","while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { throw std::runtime_error ( ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { "
if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; ExprHandle y = ExprHandle ( 1 . 0 f ) / ( ExprHandle ( 1 . 0 f ) + exp ( ExprHandle ( -0 . 0 f ) - ExprHandle ( x ) ) ) ; return y . node ( ) ; } ,"if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; auto one = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 1 . 0 ) ) ; auto zero = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 0 . 0 ) ) ; ExprHandle y = one / ( one + exp ( zero - ExprHandle ( x ) ) ) ; return y . node ( ) ; } "
"PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) DimList_subscript , 0 , } ; ","PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) ( void * ) DimList_subscript , 0 , } ; "
for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( || allSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; ,for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( || isCallSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; 
"if ( beta <= 0 ) return at::native::l1_loss ( input , target , reduction ) ; Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; ","if ( beta == 0 ) { return at::native::l1_loss ( input , target , reduction ) ; } Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; "
"std::vector<at::Tensor> & , int , int ) { throw std::runtime_error ( ""ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0"" ) ; } ","std::vector<at::Tensor> & , int , int ) { TORCH_CHECK ( false , ""ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0"" ) ; } "
"case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( ","case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; weight = weight . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( "
"if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } ","if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } auto * batched = maybeGetBatchedImpl ( tensor ) ; if ( batched ) { auto requested_level = bdims . back ( ) . level ( ) ; auto batched_level = batched -> bdims ( ) . back ( ) . level ( ) ; TORCH_INTERNAL_ASSERT ( requested_level > batched_level ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } "
"VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( uint32_t binding , VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } ","VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( const uint32_t binding , const VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } "
"if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return Stmt::clone ( true_new ) ; } else { return Stmt::clone ( false_new ) ; } } ","if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return true_new Stmt::clone ( true_new ) : nullptr ; } else { return false_new Stmt::clone ( false_new ) : nullptr ; } } "
"""tried to send() a message of type "" , requestMessage -> type ( ) , "" but RPC is no longer running on this node."" ) ; throw std::runtime_error ( err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; ","""tried to send() a message of type "" , requestMessage -> type ( ) , "" but RPC is no longer running on this node."" ) ; TORCH_CHECK ( false , err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; "
"static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; ","static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , const torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; "
"std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ","std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , "
"collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { throw std::runtime_error ( ""No device(s) specified"" ) ; } ","collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , ""No device(s) specified"" ) ; } "
if ( optimized_plan_ ) { return * optimized_plan_ ; } ,"if ( optimized_plan_ ) { GRAPH_DUMP ( ""plan already optimized:"" , graph ) ; return * optimized_plan_ ; } "
"__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return address ; } ","__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return address ; } "
. getElementType ( ) ) ; py::list pyL ; for ( int jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; ,. getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; std::string str = type_ -> toString ( ) ; if ( has_index ( ) ) { str . append ( std::to_string ( ordinal_ ) ) ; } return str ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { ,"std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; return c10::str ( type_ -> toString ( ) , ordinal_ ) ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { "
VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; ,"VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) , buildErrorMessage ( ""Expected to find '"" + var -> name_hint ( ) + ""' in intermediate vars in mem dep checker in the fuser."" ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; "
for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; ++ i ) { types . push_back ( n -> output ( 0 ) -> type ( ) ) ; } ,for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; ++ i ) { types . push_back ( n -> output ( i ) -> type ( ) ) ; } 
"if ( check_has_torch_function ( ( PyObject * ) self ) ) { return handle_torch_function_getter ( self , ""names"" ) ; } size_t size = self -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = self -> cdata . names ( ) ; ","if ( check_has_torch_function ( self ) ) { return handle_torch_function_getter ( ( THPVariable * ) self , ""names"" ) ; } size_t size = ( ( THPVariable * ) self ) -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = ( ( THPVariable * ) self ) -> cdata . names ( ) ; "
"PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require && new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; ","PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require || new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; "
auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; ++ i ) { if ( dims [ i ] ) { ,"auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) , buildErrorMessage ( ""Expected ranks to match in compressBuffer in the fuser."" ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; ++ i ) { if ( dims [ i ] ) { "
"auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j -- ) { auto v = input_ . index ( { ""..."" , Slice ( ) , j } ) ; auto v1 = v , v2 = v ; auto tau_unsqueezed = tau . index ( { ""..."" , j } ) . unsqueeze ( -1 ) ; ","auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j -- ) { const auto v = input_ . index ( { ""..."" , Slice ( ) , j } ) ; const auto & v1 = v ; const auto & v2 = v ; auto tau_unsqueezed = tau . index ( { ""..."" , j } ) . unsqueeze ( -1 ) ; "
if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; cuda_ipc_global_entities . sync_events_used_ -- ; } } catch ( . . . ) { ,if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } cuda_ipc_global_entities . sync_events_used_ -- ; } } catch ( . . . ) { 
"makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } "
std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = c10::toString ( op_schema . value ( ) ) ; } writeJsonNode ( ,std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = json_str_escape ( c10::toString ( op_schema . value ( ) ) ) ; } writeJsonNode ( 
"watchHandler ( socket ) ; } else { throw std::runtime_error ( ""Unexpected query type"" ) ; } } ","watchHandler ( socket ) ; } else { TORCH_CHECK ( false , ""Unexpected query type"" ) ; } } "
"if ( * largest == 0 ) { size_t tmp_bytes ; cudaMemGetInfo ( largest , & tmp_bytes ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; ","if ( * largest == 0 ) { size_t tmp_bytes ; C10_CUDA_CHECK ( cudaMemGetInfo ( largest , & tmp_bytes ) ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; "
"int numRanks , rank ; if ( commType == NCCLCommType::COLL ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } ","int numRanks , rank ; if ( sP2POp ( opType ) ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } "
"Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; return _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { ","Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; auto result = _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; return result ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { "
