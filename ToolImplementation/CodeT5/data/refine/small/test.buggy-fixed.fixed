self . scale = torch . tensor ( [ 1 . 0 ] ) self . zero_point = torch . tensor ( [ 0 ] ) self . dtype = self . activation_post_process . dtype self . qscheme = self . activation_post_process . qscheme 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
for opt_tensor in optional_tensors : unwraps += unwrap_optional_tensor ( opt_tensor , cur_level_var ) unwrap_code = "n" . join ( unwraps ) unwrapped_arg_list = [ ] for arg in arg_names : 
func_args = _wrap_all_tensors_to_functional ( args , func_level ) func_kwargs = _wrap_all_tensors_to_functional ( kwargs , func_level ) flattened_unwrapped_args , _ = tree_flatten ( args ) flattened_wrapped_args , _ = tree_flatten ( func_args ) flattened_unwrapped_kwargs , _ = tree_flatten ( kwargs ) flattened_wrapped_kwargs , _ = tree_flatten ( func_kwargs ) func_outputs = func ( * func_args , * * func_kwargs ) outputs = _unwrap_all_tensors_from_functional ( func_outputs ) 
test_x = torch . arange ( -2 * math . pi , 2 * math . pi , step = 0 . 01 ) . unsqueeze ( 1 ) test_y = t_A * torch . sin ( test_x + t_b ) test_f = net ( t_params , test_x ) plt . plot ( test_x . data . numpy ( ) , test_y . data . numpy ( ) , label = n ( x ) plt . plot ( test_x . data . numpy ( ) , test_f . data . numpy ( ) , label = t ( x ) 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
def icdf ( self , value ) : return torch . tan ( math . pi * ( value - 0 . 5 ) ) * self . scale + self . loc 
ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if v is None : continue if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue 
raise AttributeError ( f "'_OpNamespace' '{self.name}' object has no attribute '{op_name}'" ) from e 
else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg , is_out = func . is_out_fn ( ) ) ] 
E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol 
if len ( quant_uses ) == 1 : quantized . graph . erase_node ( node ) for arg in quant_args [ 1 : ] : if isinstance ( arg , Node ) : quantized . graph . erase_node ( arg ) return quantized 
def safe_unflatten ( tensor , dim , shape ) : if len ( shape ) == 0 : assert tensor . shape [ dim ] == 1 return tensor . squeeze ( dim ) return tensor . unflatten ( dim , shape ) def jacfwd ( f , argnums = 0 ) : 
return sum / n register_decomposition ( aten . std . correction ) def std_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : return torch . sqrt ( torch . var ( x , dims , correction = correction , keepdim = keepdim ) ) 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backward = _backward 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
def size ( g , self , dim = None ) : if dim is None : return g . op ( "Shape" , self ) return sym_help . _size_helper ( g , self , dim ) 
) ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) 
else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) 
scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) 
LockType = _dummy_thread . LockType boolean_dispatched : akref . WeakKeyDictionary [ Callable , Dict [ str , Callable ] ] weakref . WeakKeyDictionary ( ) def createResolutionCallbackFromEnv ( lookup_base ) : 
for node in graph . nodes : if node . op == ll_function cnt [ node . target . __name__ ] += 1 print ( sorted ( cnt . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) ) def min_cut_rematerialization_partition ( 
torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) 
if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : 
training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , steps = training_size , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) 
continue storage_key = storage_metadata [ MetadataIndex ( fqn , storage_md . offsets ) ] target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] 
if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device 
has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if not maximize else - d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) 
assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : 
for i in range ( 2 ) : assert current_frame is not None Optional [ FrameType ] current_frame = current_frame . f_back 
if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) 
logging . debug ( "partitioner_cache hit!" ) fused_graph_module = self . partitioner_cache [ graph_module ] else : partitioner = CapabilityBasedPartitioner ( graph_module , self . supported_ops , allows_single_node_partition = False ) fused_graph_module = partitioner . partition_and_fuse ( ) self . partitioner_cache [ graph_module ] = fused_graph_module 
paths . append ( p ) if not found_one : raise RuntimeError ( "Didn't find any test reports in s3, there is probably a bug!" ) return paths 
modules : Dict [ str , types . ModuleType ] 
def issubtype ( self , other ) : if isinstance ( other . param , _GenericAlias ) : if getattr ( other . param , origin__ None ) is Generic : return True if isinstance ( other , _DataPipeType ) : return issubtype ( self . param , other . param ) 
auto maybe_layer = maybeCurrentDynamicLayer ( ) ; TORCH_INTERNAL_ASSERT ( maybe_layer . has_value ( ) ) ; int64_t cur_level = maybe_layer -> layerId ( ) ; { textwrap . indent ( bdims_all_none_case , "  " ) } { textwrap . indent ( unwraps , "  " ) } auto results = batch_rule ( { oin ( unwrapped_arg_list ) } ) ; { wrapped_returns } 
return_annotation = signature . return_annotation valid_return_annotation_type = return_annotation is not inspect . Parameter . empty and not isinstance ( return_annotation , str ) if valid_literal_annotation and valid_return_annotation_type : 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 10 , sharded_keys = { "unboxed_ops" } , ) 
getattr ( x , "write_" + ext . lstrip ( "." ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def default_partition ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None 
else : d_bias = grad_out else : d_bias = aten . new_empty ( input , ( 0 , ) ) return ( d_input , d_weight , d_bias ) register_decomposition ( aten . addmm ) 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension 
def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_symint_fn ( ) : name += "_symint" if func . is_out_fn ( ) : 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : 
do_write = True if os . path . exists ( fout_path ) : with open ( fout_path , 'r' , encoding = f -8 as fout_old : do_write = fout_old . read ( ) != output_source 
def get_type ( arg ) : if isinstance ( arg , fx . Node ) : return n . meta [ pe if pe n n . meta else None return type ( arg ) arg_types = map_aggregate ( n . args , get_type ) assert ( isinstance ( arg_types , tuple ) ) arg_types = tuple ( [ create_type_hint ( i ) for i in arg_types ] ) 
def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( "wb" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and preserve_rng_state and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . 
def sign ( self ) : if isinstance ( self . scale , numbers . Real ) : return 1 if float ( self . scale ) > 0 else -1 if float ( self . scale ) < 0 else 0 return self . scale . sign ( ) 
fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
def hardtanh ( g , self , min_val , max_val ) : dtype = self . type ( ) . scalarType ( ) if dtype is None : dtype = 6 else : dtype = sym_help . scalar_type_to_onnx . index ( sym_help . cast_pytorch_to_onnx [ dtype ] ) 
proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) , False ) buf_value = buf . getvalue ( ) 
self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) 
with torch . enable_grad ( ) : fx_g = make_fx ( vjpfull ) ( fn , args , ( torch . ones_like ( out ) , ) ) fw_module , bw_module = partition_backwards ( fx_g ) print ( fw_module . code , bw_module . code ) garbage_hack = torch . randn ( ( ) ) fw_args = ( garbage_hack , ) + args 
def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride . default ] def handle_symbolic_op ( func , args , kwargs ) : 
padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , padding , stride , dilation ) ) input = unfold_func ( ) 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) 
def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : 
isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor and type ( x ) is not torch . nn . Parameter ) tree_map ( check_non_fake_tensor , args ) 
torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
def _handle_reduce_dim_none ( g , self , op_name ) : rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : 
assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) if isinstance ( embed_dim , torch . Tensor ) : head_dim = embed_dim . div ( num_heads , rounding_mode = unc else : head_dim = embed_dim assert head_dim * num_heads == embed_dim , "embed_dim must be divisible by num_heads" scaling = float ( head_dim ) * * -0 . 5 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward 
elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( "c10::List<c10::optional<Tensor>>" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) 
def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
def op_name_from_native_function ( f : NativeFunction ) -> str : return f "{f.namespace}::{f.func.name}" 
if prop . fset : setattr ( prop . fset , "_torchscript_modifier" , FunctionModifiers . UNUSED ) noqa : B010 return prop 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) 
def draw_joint_graph ( graph , joint_inputs , file_name = "full_graph.png" ) : draw_graph ( graph , file_name ) return default_partition ( graph , joint_inputs ) def create_compiled_function ( flat_fn , fw_compiler , bw_compiler , partition_fn ) : joint_forward_backward = create_joint_forward_backward ( flat_fn ) 
return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
with preserve_rng_state ( ) : Set input tensors that require grad to leaves flat_tensor_args = pytree . tree_map ( lambda x : x . detach ( ) . requires_grad_ ( x . requires_grad ) if isinstance ( x , Tensor ) else x , flat_tensor_args ) with torch . set_grad_enabled ( grad_state ) : out = flat_fn ( * flat_tensor_args ) 
def get_output_device ( devices , op ) : The device propagation is a bit sketchy . aten::index ( CPU , CUDA ) = > CPU tensor aten::index ( CUDA , CPU ) = > CUDA tensor if op == aten . index : return devices [ 0 ] devices = list ( set ( devices ) ) if len ( devices ) == 1 : return devices [ 0 ] else : 
weights , weight_descriptors , _ = extract_weights ( model ) buffers , buf_descriptors , _ = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) 
def f ( * inps , out_tensors = None ) : inps = fx_model . graph . flatten_inps ( * inps ) if out_tensors is None : results = [ torch . empty ( shape , dtype = dtype ) for shape , dtype in outs [ 1 ] ] results = alloc_results else : results = out_tensors full_inps = module_stuff + list ( inps ) + results 
) ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) 
exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( 
if GLOBALS . export_onnx_opset_version < 13 : raise errors . SymbolicValueError ( "Opset version must be >= 13 for Squeeze with dynamic axes." , input ) axes_t = axes_i [ 0 ] axes_rank = _get_tensor_rank ( axes_t ) assert axes_rank is not None if axes_rank > 1 : raise errors . SymbolicValueError ( "For Squeeze axses as input, the axes rank must be one in ONNX spec." , input ) 
check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f "3D or 4D (batch mode) tensor expected for input, but got: {self}" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape 
) ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) 
if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) if custom_module_class_mapping is None : 
def forward ( self , * args , * * kwargs ) : return compiled_f ( dict ( self . orig_module . named_parameters ( ) ) , dict ( self . orig_module . named_buffers ( ) ) , * args , * * kwargs ) 
step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad / batch_size del param . grad_sample 
def bench ( func ) : begin = time . time ( ) for _ in range ( iters ) : out = func ( a ) . sin ( ) out . sum ( ) . backward ( ) a . grad = None print ( time . time ( ) - begin ) def bench_jax ( ) : 
tensor_read_requests = [ ] bytes_read_requests = [ ] storage_md = cast ( Dict [ MetadataIndex , str ] , metadata_from_storage . storage_data ) for fqn , obj in state_dict . items ( ) : md = metadata_from_storage . state_dict_metadata [ fqn ] if isinstance ( obj , ShardedTensor ) : 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
out_args : List [ Dict [ str , Any ] ] = [ ] for arg in f . func . arguments . flat_positional : if arg . default is not None : continue 
if right is Any or left == right : return True if isinstance ( right , _GenericAlias ) : if getattr ( right , origin__ None ) is Generic : return True if right == type ( None ) : return False 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
if self . use_cuda : torch . cuda . synchronize ( ) 
fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if len ( batch_sizes ) == 0 : raise ValueError ( ap : Expected at least one Tensor to vmap over if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped 
return CompiledFunction def compiled_function ( fn , fw_compiler , bw_compiler , partition_fn = default_partition ) : saved_fn = None def returned_function ( * args , * * kwargs ) : 
report ( "nCopying extension {}" . format ( ext . name ) ) relative_site_packages = sysconfig . get_path ( relib replace ( sysconfig . get_path ( ta . lstrip ( os . path . sep ) src = os . path . join ( "torch" , relative_site_packages , filename ) 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
_check_p2p_op_list ( p2p_op_list ) group = p2p_op_list [ 0 ] . group reqs = [ ] with _coalescing_manager ( group , reqs ) : for p2p_op in p2p_op_list : op = p2p_op . op tensor = p2p_op . tensor 
results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( 
( * A . shape [ : -1 ] , A . size ( -1 ) - D . size ( -1 ) ) , dtype = A . dtype , device = A . device , generator = gen ) ) U_ortho_t = U_ortho . transpose ( -2 , -1 ) . contiguous ( ) 
def inner ( * args , * * kwargs ) : a0 , a1 , * a_other = args if ( isinstance ( a0 , tuple ) and isinstance ( a1 , tuple ) ) or ( isinstance ( a0 , list ) and isinstance ( a1 , list ) ) : results = [ ] for el0 , el1 in zip ( a0 , a1 ) : new_args = ( el0 , el1 , * a_other ) 
for node in self . quantized_graph . nodes : if node . op == ll_module nd is_activation_post_process ( self . modules [ node . target ] ) : env [ node . name ] = env [ node . args [ 0 ] . name ] 
start_daemon = rank == 0 assert result . hostname is not None store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) 
proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( 
assert ( len ( flat_args ) == len ( flat_inps ) ) for idx , arg in enumerate ( flat_args ) : if isinstance ( flat_inps [ idx ] , torch . Tensor ) : flat_args [ idx ] = addPythonKey ( PythonTensor ( flat_inps [ idx ] , arg ) ) else : flat_args [ idx ] = flat_inps [ idx ] tree_args = pytree . tree_unflatten ( flat_args , args_spec ) 
count += 1 assert count >= prev_count , e number of converged eigenpairs as { } , got { } ) cannot decrease ormat ( prev_count , count ) 
fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = "StaticShapeHasher" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : 
if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device 
return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward 
def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) else : if node . op != ll_function 
def get_post_build_suffix ( self ) -> str : if self . no_build_suffix : return "" if self . gpu_arch_type == "cuda" : return f "+cu{self.gpu_arch_version.replace('.', '')}" return f "+{self.gpu_arch_type}{self.gpu_arch_version}" 
if mpositeImplicitAutograd n dispatchKeys and mpositeExplicitAutograd n dispatchKeys : raise RuntimeError ( "Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed." ) 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
def __str__ ( self ) -> str : all_arguments : List [ str ] = [ ] all_arguments . extend ( map ( str , self . flat_positional ) ) if self . flat_kwarg_only or self . out : all_arguments . append ( '*' ) all_arguments . extend ( map ( str , self . flat_kwarg_only ) ) all_arguments . extend ( map ( str , self . out ) ) return oin ( all_arguments ) 
raise AttributeError ( f "'_OpNamespace' '{self.name}' object has no attribute '{op_name}'" ) from e 
self . _module_copies = [ self . module ] self . modules_params = [ list ( parameters ( m ) ) for m in self . _module_copies ] self . modules_buffers = [ list ( m . buffers ( ) ) for m in self . _module_copies ] 
try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : _wrap_exception ( e ) } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result 
if weight is None or _is_none ( weight ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , 
guard_field = 0 : : cuda::OptionalCUDAGuard guard_ ; elif self . dispatch_key == DispatchKey . CompositeExplicitAutograd : 
return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) 
def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module 
ops = set ( [ i . target for i in joint_module . graph . nodes if i . op == ll_function print ( ops - recomputable_ops ) AGGRESSIVE_RECOMPUTATION = False for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + "_in" , "sink" , capacity = math . inf ) 
for node in self . quantized_graph . nodes : if node . op == ll_module nd is_activation_post_process ( self . modules [ node . target ] ) : env [ node . name ] = env [ node . args [ 0 ] . name ] 
arranged_img_CHW = make_grid ( make_np ( label_img ) , ncols = nrow ) arranged_augment_square_HWC = np . zeros ( ( arranged_img_CHW . shape [ 2 ] , arranged_img_CHW . shape [ 2 ] , 3 ) ) arranged_img_HWC = arranged_img_CHW . transpose ( 1 , 2 , 0 ) 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( 
isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor and type ( x ) is not torch . nn . Parameter ) tree_map ( check_non_fake_tensor , args ) 
if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) 
self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) 
def forward ( self , input ) : return self . activation_post_process ( self . _forward ( input ) ) 
r = torch . Tensor . _make_subclass ( cls , elem , elem . requires_grad ) r . proxy = proxy if elem . is_sparse : proxy . node . meta [ nsor_meta = { } else : proxy . node . meta [ nsor_meta = _extract_tensor_metadata ( r ) return r 
args = list ( node . args ) args [ 1 ] = [ 1 ] node . args = tuple ( args ) for node in fx_g . graph . nodes : new_kwargs = { } 
padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , padding , stride , dilation ) ) input = unfold_func ( ) 
AGGRESSIVE_RECOMPUTATION = False def _maybe_size_of ( node ) : if nsor_meta n node . meta : return _size_of ( node . meta [ nsor_meta return 0 def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) 
prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) 
def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) 
d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) 
if func == torch . ops . aten . is_contiguous . default : return True TODO : hack , we don currently support symbolic strides properly if func == torch . ops . aten . stride . default : return create_contiguous ( args [ 0 ] . shape ) TODO : An incomplete list 
All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) self . _pre_backward_hook_full_params_prefetched = False Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ "all_gather" ] ) 
def __init__ ( self , dtype = torch . float32 , custom_op_name = "" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) 
else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg , is_out = func . is_out_fn ( ) ) ] 
has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if not maximize else - d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension 
getattr ( x , "write_" + ext . lstrip ( "." ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def default_partition ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None 
return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) 
else : observed . linear_Q . weight = nn . Parameter ( other . q_proj_weight ) observed . linear_K . weight = nn . Parameter ( other . k_proj_weight ) observed . linear_V . weight = nn . Parameter ( other . v_proj_weight ) if other . in_proj_bias is None : 
if allow_list is None : allow_list = get_default_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) 
E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( 
default_decompositions = get_decompositions ( default_decompositions ) def print_compile ( fx_g , _ ) : 
for i in range ( len ( current_commits . commits ) ) : c = current_commits . commits [ i ] if categorized n str ( c ) : feature_item = get_commit_data_cache ( ) . get ( c . commit_hash ) features = features_to_dict ( feature_item ) category , topic = CommitList . categorize ( features ) current_commits [ i ] = dataclasses . replace ( c , category = category , topic = topic ) current_commits . write_result ( ) def get_hash_or_pr_url ( commit : Commit ) : 
tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , 
def __getitem__ ( self , idx ) : if isinstance ( idx , list ) : return self . dataset [ [ self . indices [ i ] for i in idx ] ] return self . dataset [ self . indices [ idx ] ] 
def _handle_reduce_dim_none ( g , self , op_name ) : rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : 
d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) 
q = fut . value ( ) [ 0 ] . div_ ( world_size ) torch . matmul ( p , q . t ( ) , out = matrix ) if state . use_error_feedback : state . error_dict [ input_tensor ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] 
if found_tensor : n . meta [ nsor_meta = meta n . meta [ pe = type ( result ) return result 
count += 1 assert count >= prev_count , e number of converged eigenpairs as { } , got { } ) cannot decrease ormat ( prev_count , count ) 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { "definitions" } , ) 
weights , weight_descriptors , _ = extract_weights ( model ) buffers , buf_descriptors , _ = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) 
modules : Dict [ str , types . ModuleType ] 
if weight is None or _is_none ( weight ) : if channel_size is None : raise errors . SymbolicValueError ( "Unsupported: ONNX export of batch_norm for unknown channel size." , input , ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , 
padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , padding , stride , dilation ) ) input = unfold_func ( ) 
def __sizeof__ ( self ) : return ( super ( _StorageBase , self ) . __sizeof__ ( ) + self . element_size ( ) * self . size ( ) ) 
else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) 
else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : 
