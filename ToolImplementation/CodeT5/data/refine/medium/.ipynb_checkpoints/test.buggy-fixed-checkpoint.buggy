if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
const VImage * VulkanTensor::image ( c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } 
work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
for ( auto i = size_t { } ; i < num_devices ; ++ i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( "Tensor list input to scatter/gather must match number of collective" " participants" ) ; } 
if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
void retain_grad ( const Tensor & self ) { AT_ERROR ( "retain_grad is not implemented for Tensor" ) ; } 
if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { return Substitute ( body_new , { { var_new , start_new } } ) ; } 
if ( s_output || is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } 
bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) && ten . has_storage ( ) ; } 
if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return new_values . sum ( 0 ) ; } } else { auto dimIndices = ( arange ( 
bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( "Invalid call to AsyncWork::isSuccess before work has completed" ) ; } 
Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std > 0 . 0 , "normal_ expects std > 0.0, but found std=" , std ) ; return self ; } 
CudaIPCSentData::CudaIPCSentData ( std::string handle , int64_t offset , int64_t * counter_ptr , at::Device device ) 
scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos [ i ] = info ; if ( info != 0 ) { return ; } 
annotations_ . emplace_back ( "Python id" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( "Python parent id" , y_metadata_ . empty ( ) py_metadata_ . at ( 0 ) . name_ : "null" ) ; annotations_ . emplace_back ( "Python thread" , std::to_string ( t . python_tid_ ) ) ; } 
work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( "Invalid backend" ) ; } enqueue ( work ) ; return work ; 
at::Tensor & , at::Tensor & , const AllgatherOptions & ) { throw std::runtime_error ( "no support for _allgather_base in Gloo process group" ) ; } 
variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , index ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; 
try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { throw std::runtime_error ( "RRef should contain a tensor for .backward()" ) ; } } 
const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { 
if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; 
. sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) ; if ( ctx -> shapes && tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } 
if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) && DeviceObjType::get ( ) -> isSubtypeOf ( concrete_type ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } 
c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } 
const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) && ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } 
} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] < sizes [ b ] || a > b ) { return 1 ; } } 
while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , "waiting for processes to " "connect has timed out" ) ; } else if ( res == -1 ) { 
if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; ExprHandle y = ExprHandle ( 1 . 0 f ) / ( ExprHandle ( 1 . 0 f ) + exp ( ExprHandle ( -0 . 0 f ) - ExprHandle ( x ) ) ) ; return y . node ( ) ; } 
PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) DimList_subscript , 0 , } ; 
for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( || allSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; 
if ( beta <= 0 ) return at::native::l1_loss ( input , target , reduction ) ; Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; 
std::vector<at::Tensor> & , int , int ) { throw std::runtime_error ( "ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0" ) ; } 
case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( 
if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } 
VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( uint32_t binding , VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } 
if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return Stmt::clone ( true_new ) ; } else { return Stmt::clone ( false_new ) ; } } 
"tried to send() a message of type " , requestMessage -> type ( ) , " but RPC is no longer running on this node." ) ; throw std::runtime_error ( err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; 
static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; 
std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , 
collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { throw std::runtime_error ( "No device(s) specified" ) ; } 
if ( optimized_plan_ ) { return * optimized_plan_ ; } 
__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( "unsupported protocol" ) ; } return address ; } 
. getElementType ( ) ) ; py::list pyL ; for ( int jdx = 0 ; jdx < l . size ( ) ; jdx ++ ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) && isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; 
std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; std::string str = type_ -> toString ( ) ; if ( has_index ( ) ) { str . append ( std::to_string ( ordinal_ ) ) ; } return str ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { 
VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; 
for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; ++ i ) { types . push_back ( n -> output ( 0 ) -> type ( ) ) ; } 
if ( check_has_torch_function ( ( PyObject * ) self ) ) { return handle_torch_function_getter ( self , "names" ) ; } size_t size = self -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = self -> cdata . names ( ) ; 
PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require && new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; 
auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; ++ i ) { if ( dims [ i ] ) { 
auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j -- ) { auto v = input_ . index ( { "..." , Slice ( ) , j } ) ; auto v1 = v , v2 = v ; auto tau_unsqueezed = tau . index ( { "..." , j } ) . unsqueeze ( -1 ) ; 
if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; cuda_ipc_global_entities . sync_events_used_ -- ; } } catch ( . . . ) { 
makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( "" , hostname ) ; if ( evice ) { throw std::runtime_error ( "makeDeviceForHostname(): unsupported gloo device" ) ; } return device ; } 
std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = c10::toString ( op_schema . value ( ) ) ; } writeJsonNode ( 
watchHandler ( socket ) ; } else { throw std::runtime_error ( "Unexpected query type" ) ; } } 
if ( * largest == 0 ) { size_t tmp_bytes ; cudaMemGetInfo ( largest , & tmp_bytes ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; 
int numRanks , rank ; if ( commType == NCCLCommType::COLL ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } 
Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; return _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { 
def icdf ( self , value ) : if self . _validate_args : self . _validate_sample ( value ) return torch . tan ( math . pi * ( value - 0 . 5 ) ) * self . scale + self . loc 
logging . debug ( "partitioner_cache hit!" ) fused_graph_module = self . partitioner_cache [ graph_module ] else : partitioner = CapabilityBasedPartitioner ( graph_module , self . supported_ops ) fused_graph_module = partitioner . partition_and_fuse ( ) self . partitioner_cache [ graph_module ] = fused_graph_module 
with torch . enable_grad ( ) : fx_g = make_fx ( vjpfull ) ( fn , args , ( torch . ones_like ( out ) , ) ) fw_module , bw_module = partition_backwards ( fx_g ) garbage_hack = torch . randn ( ( ) ) fw_args = ( garbage_hack , ) + args 
then we don allow recomputation . if nsor_meta ot in node . meta : return False input_tensors_size = sum ( _size_of ( i . meta [ nsor_meta for i in node . args if isinstance ( i , fx . Node ) ) output_size = _size_of ( node . meta [ nsor_meta return ( output_size * 4 < input_tensors_size ) 
hardshrink ( x ) = x if x > lambd = x if x < - lambd = 0 otherwise return refs . where ( abs ( a ) > abs ( lambd ) , a , 0 ) register_decomposition ( torch . ops . aten . softshrink ) 
torch . _C . _log_api_usage_once ( "torch.package.PackageImporter" ) modules : Dict [ str , types . ModuleType ] 
report ( "nCopying extension {}" . format ( ext . name ) ) relative_site_packages = ( sysconfig . get_path ( "purelib" ) . replace ( sysconfig . get_path ( "data" ) , "" ) . lstrip ( os . path . sep ) ) src = os . path . join ( "torch" , relative_site_packages , filename ) 
def unwrap_tensors ( x ) : if isinstance ( x , torch . Tensor ) : return _unwrap_for_grad ( x , level ) assert False return tree_map ( unwrap_tensors , inps ) 
data_value = data_buf . getvalue ( ) return ( data_value , serialized_storages , serialized_dtypes , importer . zip_reader if importer else None , ) def _load_storages ( id , zip_reader , obj_bytes , serialized_storages ) : 
def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd node . target in unrecomputable_ops ) else : if node . op != ll_function 
self . freeze_bn = freeze_bn if self . training else True self . bn = nn . BatchNorm2d ( out_channels , eps , momentum , True , True ) self . activation_post_process = self . qconfig . activation ( ) self . weight_fake_quant = self . qconfig . weight ( ) 
def bench ( func ) : begin = time . time ( ) for _ in range ( iters ) : out = func ( a ) out . sum ( ) . backward ( ) mod . zero_grad ( ) print ( time . time ( ) - begin ) def bench_jax ( ) : 
value = [ Summary . Value ( tag = name , tensor = tensor , metadata = smd , ) ] 
if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) : return handle_torch_function ( 
( * A . shape [ : -1 ] , A . size ( -1 ) - D . size ( -1 ) ) , dtype = A . dtype , device = A . device , generator = gen , ) ) U_ortho_t = U_ortho . transpose ( -2 , -1 ) . contiguous ( ) 
mem_sz = _size_of ( node . meta [ nsor_meta Heuristic to bias towards nodes closer to the backwards pass mem_sz = int ( mem_sz + node . dist_from_fw ) if is_materialized ( node ) : return mem_sz 
getattr ( x , "write_" + ext . lstrip ( "." ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def partition_backwards ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None 
torch . _C . _log_api_usage_once ( "quantization_api.quantize.quantize" ) if mapping is None : mapping = get_static_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) model . eval ( ) 
for dll in dlls : is_loaded = False if with_load_library_flags : res = kernel32 . LoadLibraryExW ( dll , 0 , 0x00001100 ) last_error = ctypes . get_last_error ( ) if res == 0 and last_error != 126 : err = ctypes . WinError ( last_error ) 
env_callable = lambda fn : { "definitions" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 5 , sharded_keys = { "definitions" } , ) 
q = fut . value ( ) [ 0 ] . div_ ( world_size ) torch . matmul ( p , q . t ( ) , out = matrix ) ret = input_tensor . resize_ ( total_length ) return [ ret ] 
for node in self . quantized_graph . nodes : if node . op == ll_module nd node . target . split ( '.' ) [ -1 ] . startswith ( tivation_post_process_ env [ node . name ] = env [ node . args [ 0 ] . name ] 
if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) 
padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , dilation , padding , stride ) ) input = unfold_func ( ) 
test_x = torch . arange ( -2 * math . pi , 2 * math . pi , step = 0 . 01 ) . unsqueeze ( 1 ) test_y = t_A * torch . sin ( test_x + t_b ) test_f = net ( test_x , t_params ) plt . plot ( test_x . data . numpy ( ) , test_y . data . numpy ( ) , label = n ( x ) plt . plot ( test_x . data . numpy ( ) , test_f . data . numpy ( ) , label = t ( x ) 
e2 = symbols [ n . args [ 1 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e2 , n . args [ 0 ] , op_add ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter 
args = list ( node . args ) args [ 1 ] = [ 1 ] node . args = tuple ( args ) elif node . target == torch . ops . aten . avg_pool2d_backward : Handle empty strides if node . args [ 3 ] == [ ] : args = list ( node . args ) args [ 3 ] = [ 1 , 1 ] node . args = tuple ( args ) for node in fx_g . graph . nodes : new_kwargs = { } 
if state . use_error_feedback : state . error_dict [ input_tensor ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] 
def _handle_reduce_dim_none ( g , self , op_name ) : dim_size = _get_tensor_dim_size ( self , 0 ) rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : 
flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped 
if len ( quant_uses ) == 1 : quantized . graph . erase_node ( node ) for arg in quant_args [ 1 : ] : quantized . graph . erase_node ( arg ) return quantized 
) ndim = symbolic_helper . _get_tensor_rank ( input ) perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) 
guard_field = 0 : : cuda::OptionalCUDAGuard guard_ ; elif self . dispatch_key == DispatchKey . DefaultBackend : 
_check_p2p_op_list ( p2p_op_list ) backend = get_backend ( p2p_op_list [ 0 ] . group ) reqs = [ ] with _batch_p2p_manager ( backend ) : for p2p_op in p2p_op_list : op = p2p_op . op tensor = p2p_op . tensor 
isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor ) tree_map ( check_non_fake_tensor , args ) 
for name in curr_modules . keys ( ) : difference = curr_modules [ name ] . weight . sub ( prev_modules [ name ] . weight ) summed_norms += torch . norm ( difference ) return summed_norms < threshold 
paths . append ( p ) if not found_one : raise RuntimeError ( "Didn't find any test reports in s3, there is probably a bug!" ) return paths 
env_callable = lambda fn : { "unboxed_ops" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { "unboxed_ops" } , ) 
def inner ( * args , * * kwargs ) : a0 , a1 , * a_other = args if isinstance ( a0 , tuple ) and isinstance ( a1 , tuple ) : results = [ ] for el0 , el1 in zip ( a0 , a1 ) : new_args = ( el0 , el1 , * a_other ) 
z_cumprod = ( 1 - z ) . cumprod ( -1 ) y = pad ( z , ( 0 , 1 ) , value = 1 ) * pad ( z_cumprod , ( 1 , 0 ) , value = 1 ) return y 
self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) self . zero_bias = torch . zeros ( out_channels ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) 
AGGRESSIVE_RECOMPUTATION = False def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) 
def sign ( self ) : if isinstance ( self . scale , numbers . Number ) : return 1 if self . scale > 0 else -1 if self . scale < 0 else 0 return self . scale . sign ( ) 
else : observed . linear_Q = other . q_proj_weight observed . linear_K = other . k_proj_weight observed . linear_V = other . v_proj_weight if other . in_proj_bias is None : 
